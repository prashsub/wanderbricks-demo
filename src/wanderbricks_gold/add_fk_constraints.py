# Databricks notebook source

"""
Wanderbricks Gold Layer - Add Foreign Key Constraints

Applies FK constraints from YAML definitions after all tables are created.

FK constraints must be applied AFTER all PKs exist to avoid errors.

Critical Rule: NEVER define FK constraints inline in CREATE TABLE.
Apply them via ALTER TABLE after all PKs exist.

Usage:
  databricks bundle run gold_setup_job (task 2)
"""

import yaml
from pathlib import Path
from pyspark.sql import SparkSession


def get_parameters():
    """Get job parameters from dbutils widgets."""
    catalog = dbutils.widgets.get("catalog")
    gold_schema = dbutils.widgets.get("gold_schema")
    domain = dbutils.widgets.get("domain")
    
    print(f"Catalog: {catalog}")
    print(f"Gold Schema: {gold_schema}")
    print(f"Domain: {domain}")
    
    return catalog, gold_schema, domain


def find_yaml_base() -> Path:
    """Find YAML directory."""
    possible_paths = [
        Path("../../gold_layer_design/yaml"),
        Path("gold_layer_design/yaml"),
    ]
    for path in possible_paths:
        if path.exists():
            return path
    raise FileNotFoundError("YAML directory not found")


def load_yaml(path: Path) -> dict:
    """Load and parse YAML file."""
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def apply_fk_constraints(spark: SparkSession, catalog: str, schema: str, config: dict):
    """
    Apply FK constraints from YAML config.
    
    Args:
        spark: SparkSession
        catalog: Unity Catalog name
        schema: Schema name
        config: Parsed YAML configuration
    """
    table_name = config['table_name']
    fks = config.get('foreign_keys', [])
    
    if not fks:
        return 0
    
    print(f"\nApplying FK constraints for {table_name}...")
    
    applied_count = 0
    for idx, fk in enumerate(fks):
        fk_cols = fk['columns']
        references = fk['references']
        
        # Extract parent table and columns from references
        # Format: "parent_table(column)"
        parent_table = references.split('(')[0]
        parent_col = references.split('(')[1].rstrip(')')
        
        fk_name = f"fk_{table_name}_{parent_table}"
        fk_cols_str = ", ".join(fk_cols)
        
        try:
            spark.sql(f"""
                ALTER TABLE {catalog}.{schema}.{table_name}
                ADD CONSTRAINT {fk_name}
                FOREIGN KEY ({fk_cols_str})
                REFERENCES {catalog}.{schema}.{parent_table}({parent_col})
                NOT ENFORCED
            """)
            print(f"  ✓ Added FK: {fk_cols_str} → {parent_table}({parent_col})")
            applied_count += 1
        except Exception as e:
            print(f"  ⚠️ Warning: Could not add FK constraint {fk_name}: {e}")
    
    return applied_count


def main():
    """Main entry point for FK constraint application."""
    from pyspark.sql import SparkSession
    
    catalog, gold_schema, domain = get_parameters()
    
    spark = SparkSession.builder.appName("Wanderbricks Gold FK Constraints").getOrCreate()
    
    print("=" * 80)
    print("WANDERBRICKS GOLD LAYER FOREIGN KEY CONSTRAINTS")
    print("=" * 80)
    
    try:
        yaml_base = find_yaml_base()
        print(f"✓ Found YAML directory: {yaml_base}")
        
        if domain.lower() == "all":
            domains = [d.name for d in yaml_base.iterdir() if d.is_dir()]
        else:
            domains = [domain]
        
        print(f"\nProcessing domains: {domains}")
        
        total_fks = 0
        for d in domains:
            domain_path = yaml_base / d
            if not domain_path.exists():
                continue
            
            yaml_files = sorted(domain_path.glob("*.yaml"))
            print(f"\n--- Domain: {d} ({len(yaml_files)} tables) ---")
            
            for yaml_file in yaml_files:
                config = load_yaml(yaml_file)
                fk_count = apply_fk_constraints(spark, catalog, gold_schema, config)
                total_fks += fk_count
        
        print("\n" + "=" * 80)
        print(f"✓ Applied {total_fks} FK constraints successfully!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\n❌ Error: {str(e)}")
        raise
    finally:
        spark.stop()


if __name__ == "__main__":
    main()

