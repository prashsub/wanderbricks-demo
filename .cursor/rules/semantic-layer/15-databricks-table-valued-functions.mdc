---
description: Patterns and best practices for creating Table-Valued Functions (TVFs) in Databricks optimized for Genie Space natural language queries
globs:
  - "src/**/semantic/**/*.sql"
  - "src/**/*tvf*.sql"
  - "src/**/*function*.sql"
alwaysApply: false
---

# Databricks Table-Valued Functions (TVFs) for Genie

## Pattern Recognition
Table-Valued Functions (TVFs) in Databricks have specific requirements when used with Genie Spaces for natural language queries. This rule standardizes TVF creation to ensure Genie compatibility and SQL compliance.

---

## ‚ö†Ô∏è CRITICAL: Schema Validation BEFORE Writing SQL

**RULE #0: Always consult YAML schema definitions before writing any TVF SQL**

### The Problem

**Case Study:** Wanderbricks TVF Deployment (Dec 10, 2025)
- 26 TVFs across 5 domains
- 6 deployment iterations
- 45 minutes of debugging
- 30+ individual column reference fixes

**Root Cause:** Assumed column names without consulting YAML schema files.

```sql
-- ‚ùå What we coded (based on assumptions)
SELECT 
  dd.city,           -- ‚ùå Column doesn't exist!
  dd.state,          -- ‚ùå Column doesn't exist!
  ...
FROM fact_booking_daily fbd
LEFT JOIN dim_destination dd ON fbd.destination_id = dd.destination_id

-- Error: [UNRESOLVED_COLUMN] A column, variable, or function parameter 
--        with name `dd`.`city` cannot be resolved
```

**100% of SQL compilation errors were caused by not consulting YAML schemas first.**

---

### Pre-Development Checklist (MANDATORY)

Before writing ANY TVF SQL, complete these steps:

#### Step 1: Read YAML Schema Files (5 minutes)

```bash
# Navigate to schema definitions
cd gold_layer_design/yaml

# List all tables
find . -name "*.yaml" -type f

# Check actual column names for each table you'll reference
grep "^table_name:\|  - name:" geography/dim_destination.yaml
grep "^table_name:\|  - name:" identity/dim_host.yaml
grep "^table_name:\|  - name:" property/dim_property.yaml
grep "^table_name:\|  - name:" booking/fact_booking_daily.yaml

# Check for SCD Type 2 (requires is_current filter)
grep "^scd_type:" */dim_*.yaml
```

**What to Look For:**
- ‚úÖ Actual column names (not assumptions)
- ‚úÖ SCD Type 1 vs Type 2 (affects join syntax)
- ‚úÖ Which columns are denormalized vs require joins
- ‚úÖ Data types (INT vs BIGINT, DATE vs TIMESTAMP)

---

#### Step 2: Create SCHEMA_MAPPING.md (2 minutes)

Document the actual schema in a reference file:

```markdown
# TVF Schema Mapping Reference

## Dimensions

### dim_destination
- destination_id (BIGINT, PK)
- destination (STRING) -- ‚úÖ NOT 'city'!
- state_or_province (STRING) -- ‚úÖ NOT 'state'!
- country (STRING)
- SCD Type: 1 (no is_current)

### dim_property
- property_key (STRING, PK - surrogate)
- property_id (BIGINT, business key)
- host_id (BIGINT, FK)
- destination_id (BIGINT, FK)
- title (STRING)
- property_type (STRING)
- SCD Type: 2 (has is_current) -- ‚ö†Ô∏è Must filter!

## Common Join Patterns

```sql
-- SCD Type 2 dimension (has is_current)
LEFT JOIN dim_property dp 
  ON fbd.property_id = dp.property_id 
  AND dp.is_current = true  -- ‚úÖ REQUIRED

-- SCD Type 1 dimension (no is_current)
LEFT JOIN dim_destination dd 
  ON fbd.destination_id = dd.destination_id
  -- ‚úÖ No is_current filter needed
```
```

**Benefits:**
- Single source of truth for your TVF development
- Quick reference while coding
- Prevents 100% of column name errors
- Helps future developers

---

#### Step 3: Validate Live Schema (Optional but Recommended)

```sql
-- Verify actual column names in deployed tables
DESCRIBE TABLE catalog.schema.dim_destination;
DESCRIBE TABLE catalog.schema.dim_property;
DESCRIBE TABLE catalog.schema.fact_booking_daily;
```

---

### Common Schema Gotchas

#### 1. Column Name Variations

| What You Assume | What Actually Exists | Table |
|----------------|---------------------|-------|
| `city` | `destination` | dim_destination |
| `state` | `state_or_province` | dim_destination |
| `function_name` | `routine_name` | information_schema.routines |

**Lesson:** Never assume. Always check YAML.

---

#### 2. SCD Type 1 vs Type 2

**Type 1 (No history):**
```yaml
# dim_destination.yaml
scd_type: 1  # ‚úÖ No is_current column

# SQL Join:
LEFT JOIN dim_destination dd 
  ON fbd.destination_id = dd.destination_id
  -- ‚úÖ No is_current filter needed
```

**Type 2 (With history):**
```yaml
# dim_property.yaml
scd_type: 2  # ‚ö†Ô∏è Has is_current column!

# SQL Join:
LEFT JOIN dim_property dp 
  ON fbd.property_id = dp.property_id 
  AND dp.is_current = true  -- ‚úÖ MUST filter for current version
```

**Common Error:** Forgetting `is_current = true` on SCD Type 2 joins results in duplicate rows.

---

#### 3. Denormalized vs Join Columns

**Check which columns exist in fact table:**

```yaml
# fact_booking_daily.yaml
columns:
  - property_id       # ‚úÖ FK to dim_property
  - destination_id    # ‚úÖ FK to dim_destination
  - booking_count     # ‚úÖ Measure
  # ‚ùå NO host_id - Must join through dim_property
  # ‚ùå NO status - Use fact_booking_detail for transaction status
```

**Correct Query:**
```sql
-- ‚úÖ Get host info via property dimension
SELECT 
  fbd.booking_count,
  dh.name as host_name  -- ‚úÖ Join through property
FROM fact_booking_daily fbd
LEFT JOIN dim_property dp 
  ON fbd.property_id = dp.property_id 
  AND dp.is_current = true
LEFT JOIN dim_host dh 
  ON dp.host_id = dh.host_id 
  AND dh.is_current = true
```

**Wrong Query:**
```sql
-- ‚ùå Assumes host_id is denormalized
SELECT 
  fbd.booking_count,
  dh.name as host_name
FROM fact_booking_daily fbd
LEFT JOIN dim_host dh 
  ON fbd.host_id = dh.host_id  -- ‚ùå Column doesn't exist!
```

---

### Pre-Deployment Validation Script

Create `scripts/validate_tvf_sql.sh`:

```bash
#!/bin/bash
# Validate TVF SQL for common errors BEFORE deployment

cd src/gold_layer/tvfs

# Check for deprecated column names
if grep -n "dd\.city" *.sql; then
  echo "‚ùå ERROR: Found dd.city (should be dd.destination)"
  exit 1
fi

if grep -n "dd\.state[^_]" *.sql | grep -v "state_or_province"; then
  echo "‚ùå ERROR: Found dd.state (should be dd.state_or_province)"
  exit 1
fi

# Check RETURNS clauses for standalone column names
if grep -E "^\s+city\s+STRING" *.sql; then
  echo "‚ùå ERROR: Found 'city' in RETURNS clause"
  exit 1
fi

echo "‚úÖ All validation checks passed"
```

**Run before deployment:**
```bash
./scripts/validate_tvf_sql.sh && databricks bundle deploy
```

---

### ROI Analysis

| Activity | Time | When |
|----------|------|------|
| Read YAML schemas | 5 min | **Before coding** |
| Create SCHEMA_MAPPING.md | 2 min | **Before coding** |
| Run validation script | 30 sec | **Before deployment** |
| **Total Prep** | **~8 min** | **Upfront** |

**Without Schema Validation:**
- 6 deployment iterations
- 40 min debugging
- 30+ bug fixes
- Total: 45 min

**With Schema Validation:**
- 1 deployment iteration
- 0 min debugging
- 0 bugs
- Total: 8 min prep + 5 min deploy = 13 min

**ROI:** 71% time reduction (45 min ‚Üí 13 min)  
**First-Time Success Rate:** 0% ‚Üí 95%+

---

### Updated Development Workflow

#### ‚ùå OLD Workflow (Don't Do This)
```
1. Write TVF SQL based on assumptions
2. Deploy to Databricks
3. ‚ùå Compilation error
4. Debug and fix one error
5. Deploy again
6. ‚ùå Another compilation error
7. Repeat until all errors fixed
```

#### ‚úÖ NEW Workflow (Always Do This)
```
1. Read YAML schema files (5 min)
2. Create/update SCHEMA_MAPPING.md (2 min)
3. Write TVF SQL using documented schema
4. Run validate_tvf_sql.sh (30 sec)
5. Deploy to Databricks
6. ‚úÖ Success on first try
```

---

### Key Takeaways

1. **YAML files are the single source of truth** - Not your memory, not assumptions
2. **5 minutes of prep saves 40 minutes of debugging** - ROI is 8x
3. **Schema assumptions cause 100% of SQL errors** - Always validate first
4. **Systematic validation prevents iteration** - Find ALL instances before deploying
5. **Documentation scales knowledge** - SCHEMA_MAPPING.md helps future you

**Remember:** When in doubt, read the YAML. It's always right.

---

## Critical SQL Requirements

### ‚ö†Ô∏è Issue 1: Parameter Types for Genie Compatibility

**RULE: Use STRING for date parameters, not DATE**

Genie Spaces do not support DATE type parameters. Always use STRING with explicit format documentation.

#### ‚ùå DON'T: Use DATE parameters
```sql
CREATE OR REPLACE FUNCTION get_sales_by_date_range(
  start_date DATE COMMENT 'Start date',
  end_date DATE COMMENT 'End date'
)
...
WHERE transaction_date BETWEEN start_date AND end_date
```

**Error:**
```
Parameter start_date has an unsupported type: date
Parameter end_date has an unsupported type: date
```

#### ‚úÖ DO: Use STRING parameters with CAST
```sql
CREATE OR REPLACE FUNCTION get_sales_by_date_range(
  start_date STRING COMMENT 'Start date (format: YYYY-MM-DD)',
  end_date STRING COMMENT 'End date (format: YYYY-MM-DD)'
)
...
WHERE transaction_date BETWEEN CAST(start_date AS DATE) AND CAST(end_date AS DATE)
```

**Benefits:**
- ‚úÖ Works with Genie natural language processing
- ‚úÖ Clear format documentation for users
- ‚úÖ Type-safe conversion inside function

---

### ‚ö†Ô∏è Issue 2: Parameter Ordering with DEFAULT Values

**RULE: Parameters with DEFAULT must come AFTER parameters without DEFAULT**

SQL functions require all required parameters first, optional parameters last.

#### ‚ùå DON'T: Mix DEFAULT and non-DEFAULT parameters
```sql
CREATE OR REPLACE FUNCTION get_top_stores(
  top_n INT DEFAULT 10,          -- ‚ùå DEFAULT parameter first
  start_date STRING,              -- ‚ùå Required parameter after DEFAULT
  end_date STRING                 -- ‚ùå Required parameter after DEFAULT
)
```

**Error:**
```
[USER_DEFINED_FUNCTIONS.NOT_A_VALID_DEFAULT_PARAMETER_POSITION]
User defined function is invalid: parameter with DEFAULT must not be 
followed by parameter without DEFAULT.
```

#### ‚úÖ DO: Required parameters first, optional last
```sql
CREATE OR REPLACE FUNCTION get_top_stores(
  start_date STRING,              -- ‚úÖ Required parameter first
  end_date STRING,                -- ‚úÖ Required parameter
  top_n INT DEFAULT 10            -- ‚úÖ Optional parameter last
)
```

**Parameter Order Rules:**
1. All required parameters (no DEFAULT)
2. All optional parameters (with DEFAULT)
3. Never mix the two groups

---

### ‚ö†Ô∏è Issue 3: LIMIT Clauses Cannot Use Parameters

**RULE: Use WHERE rank <= parameter instead of LIMIT parameter**

LIMIT clauses require compile-time constants. Use WHERE with ROW_NUMBER() instead.

#### ‚ùå DON'T: Use parameter in LIMIT clause
```sql
RETURN
  WITH store_metrics AS (
    SELECT ...,
      ROW_NUMBER() OVER (ORDER BY total_revenue DESC) as rank
    FROM ...
  )
  SELECT * FROM store_metrics
  ORDER BY total_revenue DESC
  LIMIT top_n;  -- ‚ùå Cannot use parameter here
```

**Error:**
```
[INVALID_LIMIT_LIKE_EXPRESSION.IS_UNFOLDABLE]
The limit like expression "outer(get_top_stores.top_n)" is invalid.
The limit expression must evaluate to a constant value.
```

#### ‚úÖ DO: Use WHERE clause with rank
```sql
RETURN
  WITH store_metrics AS (
    SELECT ... FROM ...
  ),
  ranked_stores AS (
    SELECT ...,
      ROW_NUMBER() OVER (ORDER BY total_revenue DESC) as rank
    FROM store_metrics
  )
  SELECT * FROM ranked_stores
  WHERE rank <= top_n  -- ‚úÖ Can use parameter in WHERE
  ORDER BY rank;
```

**Why This Works:**
- `LIMIT` is evaluated at compile-time (needs constant)
- `WHERE` is evaluated at runtime (can use parameters)
- `WHERE rank <= N` achieves same result as `LIMIT N`

---

## LLM-Friendly Metadata Pattern

**RULE: Always include comprehensive COMMENT metadata for Genie**

Every TVF should have rich metadata to help Genie understand when and how to use it.

---

### ‚ö†Ô∏è CRITICAL: Standardized TVF Comment Format (v3.0)

**Problems with unstructured comments:**
- Genie didn't know what questions the TVF was best for
- Column names weren't explicit, causing UNRESOLVED_COLUMN errors
- No guidance on when NOT to use the TVF
- Missing syntax examples led to wrong parameter formats

**Use this standardized bullet-point format for ALL TVF comments:**

```sql
COMMENT '
‚Ä¢ PURPOSE: [One-line description of what the TVF does]
‚Ä¢ BEST FOR: [Example questions separated by |]
‚Ä¢ NOT FOR: [What to avoid - redirect to correct TVF] (optional)
‚Ä¢ RETURNS: [PRE-AGGREGATED rows or Individual rows] (exact column list)
‚Ä¢ PARAMS: [Parameter names with defaults]
‚Ä¢ SYNTAX: SELECT * FROM tvf_name(''param1'', ''param2'')
‚Ä¢ NOTE: [Important caveats - DO NOT wrap in TABLE(), etc.] (optional)
'
```

---

### Example 1: Aggregate TVF (Returns Pre-Aggregated Rows)

```sql
CREATE OR REPLACE FUNCTION get_customer_segments(
  start_date STRING COMMENT 'Start date (format: YYYY-MM-DD)',
  end_date STRING COMMENT 'End date (format: YYYY-MM-DD)'
)
RETURNS TABLE(
  segment_name STRING COMMENT 'Customer segment category',
  customer_count BIGINT COMMENT 'Number of customers in segment',
  total_bookings BIGINT COMMENT 'Total bookings from segment',
  total_revenue DECIMAL(18,2) COMMENT 'Total revenue from segment',
  avg_booking_value DECIMAL(18,2) COMMENT 'Average booking value'
)
COMMENT '
‚Ä¢ PURPOSE: Customer behavioral segmentation with booking patterns and revenue metrics
‚Ä¢ BEST FOR: "What are our customer segments?" | "Segment performance" | "How many VIP customers?"
‚Ä¢ NOT FOR: VIP property preferences (use get_vip_customer_property_preferences) | Individual customer details
‚Ä¢ RETURNS: 5 PRE-AGGREGATED rows (segment_name, customer_count, total_bookings, total_revenue, avg_booking_value)
‚Ä¢ PARAMS: start_date, end_date (format: YYYY-MM-DD)
‚Ä¢ SYNTAX: SELECT * FROM get_customer_segments(''2020-01-01'', ''2024-12-31'')
‚Ä¢ NOTE: Column is "segment_name" NOT "segment" | DO NOT add GROUP BY - data is already aggregated
'
RETURN
  ...
```

**Key characteristics of aggregate TVFs:**
- Returns fixed number of rows (e.g., 5 segment rows)
- Data is PRE-AGGREGATED - no GROUP BY needed on top
- Do NOT use in JOINs (no user_id or other keys to join on)

---

### Example 2: Individual Row TVF (Returns Detail Rows)

```sql
CREATE OR REPLACE FUNCTION get_customer_ltv(
  start_date STRING COMMENT 'Start date (format: YYYY-MM-DD)',
  end_date STRING COMMENT 'End date (format: YYYY-MM-DD)',
  top_n INT DEFAULT 100 COMMENT 'Number of top customers to return'
)
RETURNS TABLE(
  rank BIGINT COMMENT 'Customer rank by lifetime value',
  user_id BIGINT COMMENT 'Customer identifier',
  country STRING COMMENT 'Customer country',
  user_type STRING COMMENT 'User type (individual/business)',
  total_bookings BIGINT COMMENT 'Total bookings',
  lifetime_value DECIMAL(18,2) COMMENT 'Total revenue from customer'
)
COMMENT '
‚Ä¢ PURPOSE: Individual customer lifetime value with ranking and booking history
‚Ä¢ BEST FOR: "Show VIP customers" | "Most valuable customers" | "Top customers by spend"
‚Ä¢ RETURNS: Individual customer rows (rank, user_id, country, user_type, total_bookings, lifetime_value)
‚Ä¢ PARAMS: start_date, end_date, top_n (default: 100)
‚Ä¢ SYNTAX: SELECT * FROM get_customer_ltv(''2020-01-01'', ''2024-12-31'')
‚Ä¢ NOTE: Returns user_id for individual customer analysis | Sorted by lifetime_value DESC
'
RETURN
  ...
```

**Key characteristics of individual row TVFs:**
- Returns variable number of rows based on data
- Each row represents one entity (customer, property, host)
- CAN be used in JOINs (has identifier columns)

---

### Example 3: TVF with Limitations (Redirect Pattern)

```sql
COMMENT '
‚Ä¢ PURPOSE: Comprehensive host performance metrics with accurate revenue from booking transactions
‚Ä¢ BEST FOR: "Top performing hosts" | "Impact of verification on performance" | "Host KPIs"
‚Ä¢ PREFERRED OVER: host_analytics_metrics (which has join path limitations)
‚Ä¢ RETURNS: Individual host rows (host_id, host_name, is_verified, rating, property_count, total_bookings, total_revenue)
‚Ä¢ PARAMS: start_date, end_date, top_n (default: 100)
‚Ä¢ SYNTAX: SELECT * FROM get_host_performance(''2020-01-01'', ''2024-12-31'')
‚Ä¢ NOTE: Returns ACCURATE revenue totals (~$40M) vs metric view (~$1K due to join issues)
'
```

---

### Key Elements Explained

| Element | Purpose | Why It Matters |
|---|---|---|
| **PURPOSE** | One-line description | Quick understanding for LLM |
| **BEST FOR** | Example questions (pipe-separated) | LLM matches user query to TVF |
| **NOT FOR** | Redirect to correct asset | Prevents wrong TVF selection |
| **PREFERRED OVER** | Alternative to problematic metric view | Steers away from broken assets |
| **RETURNS** | PRE-AGGREGATED vs Individual + columns | Prevents GROUP BY on aggregates, clarifies available columns |
| **PARAMS** | Parameter names and defaults | Prevents WRONG_NUM_ARGS errors |
| **SYNTAX** | Exact copyable example | Prevents TABLE() wrapper, wrong date format |
| **NOTE** | Critical caveats | Prevents common misuse patterns |

---

### Common Genie Misuse Patterns to Prevent

**1. TABLE() Wrapper Error:**
```sql
-- ‚ùå WRONG: Genie sometimes wraps TVFs in TABLE()
SELECT * FROM TABLE(get_customer_segments('2020-01-01', '2024-12-31'))
-- Error: NOT_A_SCALAR_FUNCTION

-- ‚úÖ CORRECT: Direct call
SELECT * FROM get_customer_segments('2020-01-01', '2024-12-31')
```

**Prevention:** Add to NOTE: "DO NOT wrap in TABLE()"

**2. GROUP BY on Pre-Aggregated Data:**
```sql
-- ‚ùå WRONG: Adding GROUP BY to already-aggregated output
SELECT segment_name, SUM(total_revenue) 
FROM get_customer_segments('2020-01-01', '2024-12-31')
GROUP BY segment_name
-- Result: Same as without GROUP BY, but confusing

-- ‚úÖ CORRECT: No GROUP BY needed
SELECT * FROM get_customer_segments('2020-01-01', '2024-12-31')
```

**Prevention:** Add to NOTE: "DO NOT add GROUP BY - data is already aggregated"

**3. Wrong Column Name:**
```sql
-- ‚ùå WRONG: Using assumed column name
SELECT segment FROM get_customer_segments(...)
-- Error: UNRESOLVED_COLUMN

-- ‚úÖ CORRECT: Use actual column name
SELECT segment_name FROM get_customer_segments(...)
```

**Prevention:** List exact column names in RETURNS

**4. Missing Parameters:**
```sql
-- ‚ùå WRONG: No parameters when required
SELECT * FROM get_customer_segments()
-- Error: WRONG_NUM_ARGS

-- ‚úÖ CORRECT: Include required parameters
SELECT * FROM get_customer_segments('2020-01-01', '2024-12-31')
```

**Prevention:** Include exact SYNTAX example with parameters

---

### Professional Language Standards

**‚ùå Avoid unprofessional language:**
```sql
-- BAD: Negative about other assets
COMMENT 'Use this because the metric view is broken and returns wrong data'
COMMENT 'This TVF actually works unlike the metric view'
```

**‚úÖ Use professional, guiding language:**
```sql
-- GOOD: Professional redirect
COMMENT '
‚Ä¢ PREFERRED OVER: host_analytics_metrics for revenue/booking queries (different join paths)
'

-- GOOD: Factual comparison
COMMENT '
‚Ä¢ NOTE: Returns ACCURATE revenue totals (~$40M) vs metric view (~$1K)
'
```

---

### Legacy Format (Deprecated)

The following format is still supported but not recommended for new TVFs:

```sql
COMMENT 'LLM: [Brief description]. Use this for [use cases]. 
Parameters: [parameter list with formats]. 
Example: "[Natural language question 1]" or "[Natural language question 2]"'
```

**Migration:** When updating existing TVFs, convert to the new bullet-point format

---

## Complete TVF Pattern (All Rules Applied)

```sql
-- Function: Get top performing stores by revenue
CREATE OR REPLACE FUNCTION get_top_stores_by_revenue(
  -- Required parameters first (no DEFAULT)
  start_date STRING COMMENT 'Start date (format: YYYY-MM-DD)',
  end_date STRING COMMENT 'End date (format: YYYY-MM-DD)',
  -- Optional parameters last (with DEFAULT)
  top_n INT DEFAULT 10 COMMENT 'Number of top stores to return'
)
RETURNS TABLE(
  -- Every column documented
  rank INT COMMENT 'Store rank by revenue',
  store_number STRING COMMENT 'Store identifier',
  store_name STRING COMMENT 'Store name',
  total_revenue DECIMAL(18,2) COMMENT 'Total revenue for period',
  total_units BIGINT COMMENT 'Total units sold',
  transaction_count BIGINT COMMENT 'Number of transactions',
  avg_transaction_value DECIMAL(18,2) COMMENT 'Average transaction value',
  unique_products BIGINT COMMENT 'Number of unique products sold'
)
-- LLM-friendly function comment
COMMENT 'LLM: Returns the top N stores ranked by revenue for a date range. 
Use this for store performance analysis, regional comparisons, and identifying 
best performers. Parameters: start_date, end_date, optional top_n (default 10). 
Example: "What are the top 10 stores by revenue this month?" or 
"Show me the best performing stores"'
RETURN
  WITH store_metrics AS (
    SELECT 
      store_number,
      store_name,
      SUM(net_revenue) as total_revenue,
      SUM(net_units) as total_units,
      SUM(transaction_count) as transaction_count,
      COUNT(DISTINCT upc_code) as unique_products
    FROM fact_sales_daily
    -- Cast STRING dates to DATE for comparison
    WHERE transaction_date BETWEEN CAST(start_date AS DATE) AND CAST(end_date AS DATE)
    GROUP BY store_number, store_name
  ),
  ranked_stores AS (
    SELECT 
      ROW_NUMBER() OVER (ORDER BY total_revenue DESC) as rank,
      store_number,
      store_name,
      total_revenue,
      total_units,
      transaction_count,
      total_revenue / NULLIF(transaction_count, 0) as avg_transaction_value,
      unique_products
    FROM store_metrics
  )
  -- Use WHERE rank <= parameter instead of LIMIT parameter
  SELECT * FROM ranked_stores
  WHERE rank <= top_n
  ORDER BY rank;
```

---

## Null Safety Best Practices

**RULE: Always use NULLIF() for division to prevent divide-by-zero errors**

```sql
-- ‚ùå DON'T: Direct division
total_revenue / transaction_count as avg_transaction_value

-- ‚úÖ DO: Null-safe division
total_revenue / NULLIF(transaction_count, 0) as avg_transaction_value
```

---

## SCD Type 2 Dimension Handling

**RULE: Always filter for current records when joining SCD2 dimensions**

```sql
-- ‚úÖ Correct: Filter for current version
LEFT JOIN dim_store ds 
  ON fsd.store_number = ds.store_number 
  AND ds.is_current = true
```

---

## TVF Creation Checklist

When creating Table-Valued Functions for Genie:

### SQL Compliance
- [ ] All date parameters are STRING type (not DATE)
- [ ] Required parameters come before optional parameters
- [ ] No parameters used in LIMIT clauses (use WHERE rank <= param)
- [ ] All divisions use NULLIF to prevent divide-by-zero
- [ ] SCD2 joins include `is_current = true` filter
- [ ] **No cartesian products:** CTEs don't re-join tables already aggregated
- [ ] **Single aggregation pass:** Each source table read and aggregated only once

### Genie Optimization (Standardized Comment Format)
- [ ] Function COMMENT uses bullet-point format (‚Ä¢ PURPOSE, ‚Ä¢ BEST FOR, etc.)
- [ ] **PURPOSE:** One-line description of what TVF does
- [ ] **BEST FOR:** 2+ example questions (pipe-separated)
- [ ] **NOT FOR / PREFERRED OVER:** Redirect to correct asset when applicable
- [ ] **RETURNS:** Specifies PRE-AGGREGATED or Individual rows + exact column list
- [ ] **PARAMS:** Parameter names with defaults
- [ ] **SYNTAX:** Exact copyable example with proper date format
- [ ] **NOTE:** Caveats (DO NOT wrap in TABLE(), DO NOT add GROUP BY, etc.)
- [ ] All parameters have descriptive COMMENT with format
- [ ] All returned columns have COMMENT
- [ ] Professional language (no "metric view is broken" phrases)

### Testing
- [ ] Function compiles without errors
- [ ] Function executes with valid parameters
- [ ] Function handles edge cases (empty results, null values)
- [ ] Function tested in Genie Space (if applicable)
- [ ] **Results validated against metric view** (ratio ‚âà 1.0, not 254x)

---

## Common Mistakes to Avoid

### ‚ùå Mistake 1: Using DATE parameters
```sql
-- Will fail in Genie
start_date DATE COMMENT 'Start date'
```

### ‚ùå Mistake 2: Wrong parameter order
```sql
-- Will fail to compile
CREATE FUNCTION func(
  optional_param INT DEFAULT 10,
  required_param STRING
)
```

### ‚ùå Mistake 3: Parameter in LIMIT
```sql
-- Will fail to compile
SELECT * FROM data
LIMIT top_n
```

### ‚ùå Mistake 4: No null safety
```sql
-- Can fail with divide by zero
revenue / transactions
```

### ‚ùå Mistake 5: Poor Genie metadata
```sql
-- Won't be discoverable by Genie
COMMENT 'Gets data'
```

### ‚ùå Mistake 6: Cartesian Product in CTE (254x Over-Counting)
```sql
-- ‚ùå CRITICAL BUG: Re-joining source table after aggregation
WITH period_data AS (
  SELECT SUM(revenue) as total_revenue
  FROM fact_table
  GROUP BY period
),
final AS (
  SELECT SUM(pd.total_revenue), SUM(ft.other_metric)  -- üî• CARTESIAN!
  FROM period_data pd
  LEFT JOIN fact_table ft ON ...  -- ‚ùå Re-joining source!
)
```

**Fix:** Single aggregation pass, no self-joins:
```sql
-- ‚úÖ CORRECT: Single aggregation
SELECT 
  period,
  SUM(revenue) as total_revenue,
  SUM(other_metric) as other_metric
FROM fact_table
GROUP BY period;
```

---

## References

### Official Documentation
- [Databricks Table-Valued Functions](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-tvf)
- [Genie Trusted Assets - Functions](https://docs.databricks.com/genie/trusted-assets#tips-for-writing-functions)
- [SQL UDF Best Practices](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-sql-function)

### Project Examples
- [Revenue TVFs](../../src/wanderbricks_gold/tvfs/revenue_tvfs.sql) - 6 revenue analysis TVFs
- [Engagement TVFs](../../src/wanderbricks_gold/tvfs/engagement_tvfs.sql) - 5 property engagement TVFs
- [Property TVFs](../../src/wanderbricks_gold/tvfs/property_tvfs.sql) - 5 property performance TVFs
- [Host TVFs](../../src/wanderbricks_gold/tvfs/host_tvfs.sql) - 5 host analytics TVFs
- [Customer TVFs](../../src/wanderbricks_gold/tvfs/customer_tvfs.sql) - 5 customer behavior TVFs
- [SCHEMA_MAPPING.md](../../src/wanderbricks_gold/tvfs/SCHEMA_MAPPING.md) - Schema reference documentation

### Related Cursor Rules
- [metric-views-patterns.mdc](mdc:.cursor/rules/semantic-layer/14-metric-views-patterns.mdc) - Metric view YAML structure
- [genie-space-patterns.mdc](mdc:.cursor/rules/semantic-layer/16-genie-space-patterns.mdc) - Genie Space setup

### Project Documentation
- [Genie Space Post-Mortem](../../docs/reference/genie-space-semantic-layer-postmortem.md) - Comprehensive lessons learned from Genie Space implementation
- [TVF Deployment Post-Mortem](../../docs/troubleshooting/2025-12-10-tvf-deployment-postmortem.md) - Complete failure analysis and prevention
- [Schema-First Development Guide](../../docs/development/SCHEMA_FIRST_DEVELOPMENT.md) - Comprehensive workflow guide
- [Semantic Setup Refactoring](../../docs/reference/semantic-setup-refactoring.md) - Job reorganization details
- [Validation Script](../../scripts/validate_tvf_sql.sh) - Pre-deployment SQL validation

---

## Version History

- **v3.0** (Dec 16, 2025) - Standardized TVF comment format for Genie optimization
  - New bullet-point comment format (PURPOSE, BEST FOR, NOT FOR, RETURNS, PARAMS, SYNTAX, NOTE)
  - Separate examples for aggregate vs individual row TVFs
  - Common Genie misuse patterns documentation (TABLE() wrapper, GROUP BY on aggregates)
  - Professional language standards (avoid "metric view is broken")
  - Key elements table explaining why each comment element matters
  - Updated checklist with new comment format requirements
  - **Key Learning:** Structured comments prevent 80%+ of Genie misuse errors
  - Based on: [Genie Space Post-Mortem](../../docs/reference/genie-space-semantic-layer-postmortem.md)

- **v2.1** (Dec 15, 2025) - Critical bug prevention: Cartesian product in aggregations
  - Added cartesian product bug detection section
  - Documented self-join anti-pattern in CTEs
  - Case study: get_revenue_by_period bug (254x revenue over-counting)
  - **Key Learning:** Never re-join a table that's already been aggregated in a CTE
  
- **v2.0** (Dec 2025) - Major enhancement: Schema-first development patterns
  - Added comprehensive pre-development schema validation section
  - Documented YAML-first approach (Rule #0) based on production post-mortem
  - Included schema mapping documentation patterns
  - Added pre-deployment validation script template
  - Provided ROI analysis (71% time reduction)
  - Case study: Wanderbricks 26 TVF deployment (6 iterations ‚Üí 1 iteration)
  - **Key Learning:** 100% of SQL errors caused by not consulting YAML schemas

- **v1.0** (Oct 2025) - Initial rule based on 15 TVF deployment learnings
  - 3 critical SQL issues identified and resolved
  - Genie compatibility patterns established
  - Production-tested with successful deployment

---

## ‚ö†Ô∏è CRITICAL: Cartesian Product Bug in Aggregation CTEs (Dec 15, 2025)

### The Bug That Inflated Revenue by 254x

**Case Study:** Wanderbricks `get_revenue_by_period` TVF reported $35.8M revenue when actual was $141K.

**Root Cause:** Self-join on a table that was already aggregated in a CTE.

#### ‚ùå BUGGY PATTERN: Re-Joining After Aggregation

```sql
-- BUG: This pattern causes cartesian product!
CREATE FUNCTION get_revenue_by_period(...)
RETURN
  WITH period_data AS (
    -- First CTE: Aggregate revenue from fact table
    SELECT
      DATE_TRUNC(time_grain, fbd.check_in_date) AS period_start,
      period_name,
      SUM(fbd.total_booking_value) as total_revenue,  -- ‚úÖ Aggregated here
      ...
    FROM ${catalog}.${schema}.fact_booking_daily fbd
    WHERE fbd.check_in_date BETWEEN ...
    GROUP BY DATE_TRUNC(...), period_name
  ),
  aggregated_periods AS (
    -- Second CTE: BUG - Re-joins the same table!
    SELECT
      pd.period_start,
      pd.period_name,
      SUM(pd.total_revenue) as total_revenue,  -- üî• ALREADY SUMMED!
      SUM(fbd.booking_count) as booking_count  -- üî• JOINED AGAIN
    FROM period_data pd
    LEFT JOIN ${catalog}.${schema}.fact_booking_daily fbd  -- ‚ùå CARTESIAN PRODUCT!
      ON DATE_TRUNC(time_grain, fbd.check_in_date) = pd.period_start
      AND fbd.check_in_date BETWEEN ...
    GROUP BY pd.period_start, pd.period_name
  )
  SELECT * FROM aggregated_periods;
```

**Why It's Wrong:**
1. `period_data` already aggregates from `fact_booking_daily` (e.g., 100 rows ‚Üí 10 periods)
2. `aggregated_periods` joins `period_data` back to `fact_booking_daily`
3. Each period row matches ALL detail rows for that period
4. 1 period row √ó 100 matching detail rows = 100x multiplication
5. SUM() then adds these inflated values together

**Actual Bug Impact:**
- Expected revenue per week: ~$141K
- Actual buggy output: ~$35.8M  
- **Inflation factor: 254x**

---

#### ‚úÖ CORRECT PATTERN: Single Aggregation, No Self-Join

```sql
-- CORRECT: Single aggregation pass, no self-join
CREATE FUNCTION get_revenue_by_period(...)
RETURN
  SELECT
    DATE_TRUNC(time_grain, fbd.check_in_date) AS period_start,
    CASE
      WHEN time_grain = 'week' THEN CONCAT('Week ', WEEKOFYEAR(fbd.check_in_date), ' ', YEAR(fbd.check_in_date))
      WHEN time_grain = 'month' THEN DATE_FORMAT(fbd.check_in_date, 'MMMM yyyy')
      ...
    END AS period_name,
    SUM(fbd.total_booking_value) as total_revenue,  -- ‚úÖ Aggregate once
    SUM(fbd.booking_count) as booking_count,        -- ‚úÖ Same aggregation pass
    SUM(fbd.cancellation_count) as cancellation_count,
    ...
  FROM ${catalog}.${schema}.fact_booking_daily fbd
  WHERE fbd.check_in_date BETWEEN CAST(start_date AS DATE) AND CAST(end_date AS DATE)
  GROUP BY DATE_TRUNC(time_grain, fbd.check_in_date), period_name
  ORDER BY period_start;
```

**Why It's Correct:**
- Single pass over `fact_booking_daily`
- All aggregations in same GROUP BY
- No CTEs that re-join the source table
- Results are accurate

---

### Detection Rules

**WARNING SIGN 1: CTE that aggregates, followed by join back to same source**
```sql
-- üö® WARNING: CTE1 reads from table_A, CTE2 joins table_A
WITH cte1 AS (
  SELECT ... SUM(...) FROM table_A ...
),
cte2 AS (
  SELECT ... FROM cte1 JOIN table_A ...  -- üî• DANGER!
)
```

**WARNING SIGN 2: SUM of a SUM**
```sql
-- üö® WARNING: Summing something that's already summed
SUM(already_aggregated_column)  -- Was this already a SUM()?
```

**WARNING SIGN 3: Dramatic result differences**
```
Metric view result:  $141K
TVF result:          $35.8M
Ratio:               254x  -- üö® Almost certainly a cartesian product!
```

---

### Prevention Checklist

**Before deploying any TVF with aggregations:**
- [ ] Each CTE aggregates from source tables ONLY ONCE
- [ ] No CTE re-joins a table that was already read in a prior CTE
- [ ] If multiple CTEs, they join to EACH OTHER, not back to source
- [ ] Results validated against known good source (metric view)
- [ ] Spot check: result order of magnitude matches expectations

**Validation Query:**
```sql
-- Compare TVF result to metric view result
WITH tvf_result AS (
  SELECT SUM(total_revenue) as tvf_total
  FROM get_revenue_by_period('2024-10-01', '2024-12-31', 'week')
),
metric_result AS (
  SELECT SUM(MEASURE(total_revenue)) as metric_total
  FROM revenue_analytics_metrics
  WHERE check_in_date BETWEEN '2024-10-01' AND '2024-12-31'
)
SELECT 
  tvf_total,
  metric_total,
  tvf_total / NULLIF(metric_total, 0) as ratio  -- Should be ~1.0
FROM tvf_result, metric_result;
```

**Expected:** ratio ‚âà 1.0  
**Bug indicator:** ratio >> 1.0 (e.g., 254)
