---
description: Delta MERGE deduplication pattern for Gold layer to prevent duplicate source key errors
---
# Gold Layer Delta MERGE Deduplication Pattern

## Pattern Recognition
Gold layer merge operations read from Silver and write to Gold using Delta MERGE. This rule standardizes the deduplication pattern required to prevent `[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE]` errors.

## Core Principle: Always Deduplicate Before MERGE

**Delta MERGE operations REQUIRE unique source keys.** Multiple source rows matching the same target row causes ambiguity and fails the merge.

### Why Deduplication is Needed

Silver tables often contain multiple versions of the same entity due to:
- ✅ Incremental DLT streaming ingestion  
- ✅ Change Data Capture (CDC) patterns
- ✅ Historical SCD Type 2 tracking
- ✅ Test data generation creating duplicates
- ✅ Multiple batch loads

**Without deduplication:** MERGE fails with error:
```
[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same
target row in the Delta table in possibly conflicting ways.
```

## Standard Deduplication Pattern

### ✅ CORRECT: Deduplicate Before MERGE

```python
def merge_dim_store(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge dim_store from Silver to Gold (SCD Type 2)."""
    print("Merging dim_store...")
    
    silver_table = f"{catalog}.{silver_schema}.silver_store_dim"
    gold_table = f"{catalog}.{gold_schema}.dim_store"
    
    # CRITICAL: Deduplicate Silver source before merging
    silver_raw = spark.table(silver_table)
    original_count = silver_raw.count()
    
    silver_df = (
        silver_raw
        .orderBy(col("processed_timestamp").desc())  # Order by timestamp first (latest first)
        .dropDuplicates(["store_number"])  # Keep only first occurrence per business key
    )
    
    dedupe_count = silver_df.count()
    print(f"  Deduplicated: {original_count} → {dedupe_count} records ({original_count - dedupe_count} duplicates removed)")
    
    # Prepare Gold updates
    updates_df = (
        silver_df
        .withColumn("store_key", md5(concat_ws("||", col("store_id"), col("processed_timestamp"))))
        # ... other transformations
    )
    
    # MERGE into Gold
    delta_gold = DeltaTable.forName(spark, gold_table)
    
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        "target.store_number = source.store_number AND target.is_current = true"
    ).whenMatchedUpdate(set={
        "record_updated_timestamp": "source.record_updated_timestamp"
    }).whenNotMatchedInsertAll(
    ).execute()
    
    record_count = updates_df.count()
    print(f"✓ Merged {record_count} records into dim_store")
```

### ❌ WRONG: No Deduplication

```python
def merge_dim_store(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """WRONG - Will fail if Silver has duplicates."""
    silver_table = f"{catalog}.{silver_schema}.silver_store_dim"
    gold_table = f"{catalog}.{gold_schema}.dim_store"
    
    # ❌ BAD: No deduplication - reads ALL Silver records including duplicates
    silver_df = spark.table(silver_table)
    
    # Prepare updates
    updates_df = prepare_transformations(silver_df)
    
    # ❌ MERGE will fail if multiple rows have same store_number
    delta_gold = DeltaTable.forName(spark, gold_table)
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        "target.store_number = source.store_number"
    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
```

## Key Requirements

### 1. Order by Timestamp Descending

**Always order by `processed_timestamp DESC`** before deduplication to keep the LATEST record:

```python
.orderBy(col("processed_timestamp").desc())  # Latest first
```

**Why important:**
- Ensures most recent data is kept
- Older versions are dropped
- Consistent with SCD Type 2 patterns

### 2. Match Deduplication Key to Merge Key

**CRITICAL:** The column used in `.dropDuplicates()` MUST match the column in the MERGE condition.

#### ✅ CORRECT: Keys Match

```python
# Deduplicate on product_code
silver_df = (
    silver_raw
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["product_code"])
)

# Merge on product_code (SAME KEY)
delta_gold.alias("target").merge(
    updates_df.alias("source"),
    "target.product_code = source.product_code"
).execute()
```

#### ❌ WRONG: Keys Don't Match

```python
# Deduplicate on upc_code
silver_df = (
    silver_raw
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["upc_code"])  # ❌ WRONG KEY
)

# Merge on product_code (DIFFERENT KEY!)
delta_gold.alias("target").merge(
    updates_df.alias("source"),
    "target.product_code = source.product_code"  # Still has duplicates!
).execute()
```

**Result:** MERGE still fails because multiple `upc_code` records can have the same `product_code`.

### 3. Use dropDuplicates() Not Window Functions

**Preferred approach:** `.dropDuplicates()` with ordering

```python
# ✅ SIMPLE and RELIABLE
silver_df = (
    spark.table(silver_table)
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["store_number"])
)
```

**Avoid:** Window functions with `row_number()`

```python
# ❌ MORE COMPLEX, no benefit
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window_spec = Window.partitionBy("store_number").orderBy(col("processed_timestamp").desc())
silver_df = (
    spark.table(silver_table)
    .withColumn("row_num", row_number().over(window_spec))
    .filter(col("row_num") == 1)
    .drop("row_num")
)
```

**Why `.dropDuplicates()` is better:**
- ✅ Simpler and more readable
- ✅ Less code to maintain
- ✅ No temporary columns needed
- ✅ More performant for large datasets

### 4. Add Debug Logging

**Always log deduplication metrics** for visibility:

```python
original_count = silver_raw.count()
dedupe_count = silver_df.count()
print(f"  Deduplicated: {original_count} → {dedupe_count} records ({original_count - dedupe_count} duplicates removed)")
```

**Benefits:**
- Proves deduplication is working
- Shows magnitude of duplicate problem
- Helps troubleshoot merge failures
- Provides data quality insights

## Complete Pattern Template

### Generic Reusable Function

```python
def merge_dimension_with_deduplication(
    spark: SparkSession,
    catalog: str,
    silver_schema: str,
    gold_schema: str,
    silver_table_name: str,
    gold_table_name: str,
    business_key: str,
    transformation_func: callable
):
    """
    Generic pattern for merging dimensions from Silver to Gold with deduplication.
    
    Args:
        silver_table_name: Name of Silver table (e.g., "silver_store_dim")
        gold_table_name: Name of Gold table (e.g., "dim_store")
        business_key: Column to deduplicate and merge on (e.g., "store_number")
        transformation_func: Function to transform Silver to Gold schema
    """
    print(f"Merging {gold_table_name}...")
    
    silver_table = f"{catalog}.{silver_schema}.{silver_table_name}"
    gold_table = f"{catalog}.{gold_schema}.{gold_table_name}"
    
    # Step 1: Read and deduplicate Silver source
    silver_raw = spark.table(silver_table)
    original_count = silver_raw.count()
    
    silver_df = (
        silver_raw
        .orderBy(col("processed_timestamp").desc())  # Latest first
        .dropDuplicates([business_key])  # Keep first (latest) per business key
    )
    
    dedupe_count = silver_df.count()
    print(f"  Deduplicated: {original_count} → {dedupe_count} records ({original_count - dedupe_count} duplicates removed)")
    
    # Step 2: Apply table-specific transformations
    updates_df = transformation_func(silver_df)
    
    # Step 3: MERGE into Gold
    delta_gold = DeltaTable.forName(spark, gold_table)
    
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        f"target.{business_key} = source.{business_key}"
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()
    
    record_count = updates_df.count()
    print(f"✓ Merged {record_count} records into {gold_table_name}")
    
    return record_count
```

### Usage Example

```python
def transform_store_to_gold(silver_df):
    """Transform silver_store_dim to dim_store schema."""
    return (
        silver_df
        .withColumn("store_key", md5(concat_ws("||", col("store_id"), col("processed_timestamp"))))
        .withColumn("effective_from", col("processed_timestamp"))
        .withColumn("effective_to", lit(None).cast("timestamp"))
        .withColumn("is_current", lit(True))
        # ... more transformations
        .select("store_key", "store_number", "store_name", ...)
    )

# Use the generic function
merge_dimension_with_deduplication(
    spark=spark,
    catalog="user_catalog",
    silver_schema="dev_user_company_silver",
    gold_schema="dev_user_company_gold",
    silver_table_name="silver_store_dim",
    gold_table_name="dim_store",
    business_key="store_number",
    transformation_func=transform_store_to_gold
)
```

## Common Scenarios

### Scenario 1: Simple SCD Type 1 (Overwrite)

```python
def merge_dim_product(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge dim_product (SCD Type 1 - overwrites)."""
    silver_table = f"{catalog}.{silver_schema}.silver_product_dim"
    gold_table = f"{catalog}.{gold_schema}.dim_product"
    
    # Deduplicate on product_code (business key)
    silver_raw = spark.table(silver_table)
    silver_df = (
        silver_raw
        .orderBy(col("processed_timestamp").desc())
        .dropDuplicates(["product_code"])
    )
    
    updates_df = prepare_product_updates(silver_df)
    
    delta_gold = DeltaTable.forName(spark, gold_table)
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        "target.product_code = source.product_code"
    ).whenMatchedUpdateAll(  # SCD Type 1: Update all fields
    ).whenNotMatchedInsertAll(
    ).execute()
```

### Scenario 2: SCD Type 2 (Historical Tracking)

```python
def merge_dim_store(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge dim_store (SCD Type 2 - historical tracking)."""
    silver_table = f"{catalog}.{silver_schema}.silver_store_dim"
    gold_table = f"{catalog}.{gold_schema}.dim_store"
    
    # Deduplicate on store_number (business key)
    silver_raw = spark.table(silver_table)
    silver_df = (
        silver_raw
        .orderBy(col("processed_timestamp").desc())
        .dropDuplicates(["store_number"])
    )
    
    updates_df = prepare_store_updates_scd2(silver_df)
    
    delta_gold = DeltaTable.forName(spark, gold_table)
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        "target.store_number = source.store_number AND target.is_current = true"
    ).whenMatchedUpdate(set={
        "record_updated_timestamp": "source.record_updated_timestamp"  # Only update timestamp
    }).whenNotMatchedInsertAll(
    ).execute()
```

### Scenario 3: Fact Table with Date Dimension

```python
def merge_fact_sales_daily(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge fact_sales_daily from Silver to Gold."""
    silver_table = f"{catalog}.{silver_schema}.silver_transactions"
    gold_table = f"{catalog}.{gold_schema}.fact_sales_daily"
    
    transactions = spark.table(silver_table)
    
    # Aggregate daily sales (this inherently deduplicates by grouping)
    daily_sales = (
        transactions
        .groupBy("store_number", "upc_code", "transaction_date")
        .agg(
            spark_sum(col("final_sales_price")).alias("net_revenue"),
            spark_sum(col("quantity_sold")).alias("net_units"),
            count("*").alias("transaction_count")
        )
    )
    
    # Note: Aggregation handles deduplication, but MERGE key must be composite
    delta_gold = DeltaTable.forName(spark, gold_table)
    delta_gold.alias("target").merge(
        daily_sales.alias("source"),
        """target.store_number = source.store_number 
           AND target.upc_code = source.upc_code 
           AND target.transaction_date = source.transaction_date"""
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()
```

## Troubleshooting Checklist

When MERGE fails with duplicate key error:

### Step 1: Verify Duplicates Exist
```sql
-- Check for duplicate business keys in Silver
SELECT business_key, COUNT(*) as count
FROM {catalog}.{silver_schema}.silver_table
GROUP BY business_key
HAVING COUNT(*) > 1
ORDER BY count DESC
LIMIT 10
```

### Step 2: Add Deduplication
```python
silver_df = (
    spark.table(silver_table)
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates([business_key])
)
```

### Step 3: Verify Keys Match
```python
# Deduplication key
.dropDuplicates(["product_code"])

# Merge condition key (MUST MATCH)
"target.product_code = source.product_code"
```

### Step 4: Add Debug Logging
```python
original_count = silver_raw.count()
dedupe_count = silver_df.count()
print(f"  Deduplicated: {original_count} → {dedupe_count} records")
```

### Step 5: Check for Empty Tables
```python
if dedupe_count == 0:
    print(f"⚠️ Warning: No records in {silver_table}")
    return 0
```

## Validation Checklist

Before deploying Gold MERGE operations:

- [ ] Source data is deduplicated with `.dropDuplicates()`
- [ ] Deduplication key matches MERGE condition key
- [ ] Ordered by `processed_timestamp DESC`
- [ ] Debug logging shows deduplication metrics
- [ ] Error handling with try/except
- [ ] Success/failure messages printed
- [ ] Tested with actual Silver data containing duplicates
- [ ] MERGE condition uses correct business key

## Common Mistakes to Avoid

### ❌ Mistake 1: No Deduplication
```python
# BAD
silver_df = spark.table(silver_table)
```

### ❌ Mistake 2: Wrong Deduplication Key
```python
# BAD - Deduplicate on upc_code but merge on product_code
.dropDuplicates(["upc_code"])
# ... later ...
"target.product_code = source.product_code"
```

### ❌ Mistake 3: No Ordering
```python
# BAD - Which duplicate to keep is non-deterministic
.dropDuplicates(["store_number"])  # No orderBy!
```

### ❌ Mistake 4: Using Window Functions Unnecessarily
```python
# BAD - Overly complex
window_spec = Window.partitionBy("store_number").orderBy(col("processed_timestamp").desc())
silver_df = (
    spark.table(silver_table)
    .withColumn("row_num", row_number().over(window_spec))
    .filter(col("row_num") == 1)
    .drop("row_num")
)
```

### ✅ CORRECT: Simple and Reliable
```python
# GOOD
silver_df = (
    spark.table(silver_table)
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["store_number"])
)
```

## Performance Considerations

### 1. Deduplication Before Transformations
Deduplicate BEFORE applying transformations to reduce compute:

```python
# ✅ GOOD - Deduplicate first, then transform
silver_df = (
    spark.table(silver_table)
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["store_number"])  # Reduces data size
)
updates_df = apply_transformations(silver_df)  # Works on smaller dataset

# ❌ BAD - Transform first, then deduplicate
silver_df = apply_transformations(spark.table(silver_table))  # Transforms all duplicates
deduplicated_df = silver_df.dropDuplicates(["store_number"])  # Wastes compute
```

### 2. Partition Pruning
If Silver tables are partitioned, leverage partition pruning:

```python
silver_df = (
    spark.table(silver_table)
    .filter(col("transaction_date") >= "2025-01-01")  # Partition pruning
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["store_number"])
)
```

### 3. Cache if Reusing
If using the deduplicated DataFrame multiple times, cache it:

```python
silver_df = (
    spark.table(silver_table)
    .orderBy(col("processed_timestamp").desc())
    .dropDuplicates(["store_number"])
    .cache()  # Cache for reuse
)
```

## References

### Official Documentation
- [Delta Lake MERGE](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge)
- [PySpark dropDuplicates](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dropDuplicates.html)

### Project Documentation
- [Gold Layer README](../docs/gold/GOLD_LAYER_README.md)
- [Gold Delta MERGE Pattern](../docs/gold/GOLD_DELTA_MERGE_PATTERN.md)
- [Troubleshooting Guide](GOLD_MERGE_TROUBLESHOOTING_SUMMARY.md)

### Session Learning
- [Session Summary 2025-10-19](../SESSION_SUMMARY_2025-10-19.md)

---

## Key Takeaway

> **Delta MERGE operations require unique source keys. ALWAYS deduplicate Silver data before merging to Gold using `.orderBy(col("processed_timestamp").desc()).dropDuplicates([business_key])` to prevent `[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE]` errors.**

This pattern is mandatory for all Gold layer MERGE operations.
