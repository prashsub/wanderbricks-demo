---
description: Schema management patterns for Databricks Asset Bundles with Unity Catalog, including development mode prefix handling and config-driven approach
globs: ["resources/*.yml", "databricks.yml", "src/**/*.py"]
alwaysApply: true
---

# Schema Management Patterns for Databricks Asset Bundles

## Core Principle: Config-Driven Schema Management

**ALWAYS use `resources/schemas.yml` to define schemas as Databricks resources.**

### Why Config-Driven?
- ✅ Centralized schema definitions
- ✅ Consistent metadata and properties
- ✅ Version-controlled governance settings
- ✅ Declarative infrastructure-as-code
- ✅ Automatic lifecycle management

## Development Mode Prefix Behavior

### Understanding the Prefix
In Databricks Asset Bundles, **development mode automatically adds a prefix** to resource names:

```
Pattern: dev_{username}_{original_name}
Example: company_silver → dev_user_company_silver
```

### Critical Rule: Schema Variable Overrides

**In `databricks.yml`, ALWAYS override schema variables in the dev target to match the prefixed schema names:**

```yaml
variables:
  bronze_schema:
    description: Schema name for Bronze layer
    default: company_bronze
  silver_schema:
    description: Schema name for Silver layer
    default: company_silver
  gold_schema:
    description: Schema name for Gold layer
    default: company_gold

targets:
  dev:
    mode: development
    default: true
    variables:
      catalog: prashanth_subrahmanyam_catalog
      # Override schema names to match dev mode prefixes
      bronze_schema: dev_user_company_bronze
      silver_schema: dev_user_company_silver
      gold_schema: dev_user_company_gold
      
  prod:
    mode: production
    variables:
      catalog: prashanth_subrahmanyam_catalog
      # Production uses standard names (no prefix)
```

### Why This Matters

1. **Schema Resources** (from `resources/schemas.yml`):
   - Dev mode: Creates `dev_{username}_company_silver`
   - Prod mode: Creates `company_silver`

2. **DLT Pipelines** (Silver layer):
   - Configured with `schema: ${var.silver_schema}`
   - DLT automatically adds prefix in dev mode
   - Must match the schema resource name

3. **Python Jobs** (Bronze, Gold):
   - Use `${var.bronze_schema}`, `${var.gold_schema}` parameters
   - Create tables directly via SQL
   - Must reference the actual prefixed schema names

## Resources Schema Pattern

**File: `resources/schemas.yml`**

```yaml
resources:
  schemas:
    bronze_schema:
      name: ${var.bronze_schema}
      catalog_name: ${var.catalog}
      comment: "Bronze layer - raw ingestion..."
      properties:
        delta.autoOptimize.optimizeWrite: "true"
        delta.autoOptimize.autoCompact: "true"
        databricks.pipelines.predictiveOptimizations.enabled: "true"
        layer: "bronze"
        # ... governance metadata
    
    silver_schema:
      name: ${var.silver_schema}
      catalog_name: ${var.catalog}
      # ... similar structure
    
    gold_schema:
      name: ${var.gold_schema}
      catalog_name: ${var.catalog}
      # ... similar structure
```

## Schema Creation in Python Scripts

### Bronze and Gold Setup Scripts

**ALWAYS use CREATE OR REPLACE TABLE for idempotent schema management:**

```python
# At the start of setup script
def create_catalog_and_schema(spark: SparkSession, catalog: str, schema: str):
    """Ensures the Unity Catalog schema exists."""
    print(f"Ensuring catalog '{catalog}' and schema '{schema}' exist...")
    spark.sql(f"CREATE CATALOG IF NOT EXISTS {catalog}")
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}")
    print(f"✓ Schema {catalog}.{schema} ready")

# For table creation
spark.sql(f"""
    CREATE OR REPLACE TABLE {catalog}.{schema}.table_name (
        -- columns
    )
    USING DELTA
    CLUSTER BY AUTO
    TBLPROPERTIES (...)
""")
```

### Why CREATE OR REPLACE?
- ✅ Allows schema evolution without manual drops
- ✅ Idempotent deployments
- ✅ Faster iteration during development
- ✅ Prevents "table already exists" errors

## DLT Pipeline Schema Configuration

**File: `resources/silver_dlt_pipeline.yml`**

```yaml
resources:
  pipelines:
    silver_dlt_pipeline:
      name: "[${bundle.target}] Silver Layer Pipeline"
      
      # Catalog for serverless (required)
      catalog: ${var.catalog}
      
      # Schema where DLT will create tables
      # In dev mode, DLT adds prefix automatically
      schema: ${var.silver_schema}
      
      # Pass configuration to notebooks
      configuration:
        catalog: ${var.catalog}
        bronze_schema: ${var.bronze_schema}
        silver_schema: ${var.silver_schema}
```

## Common Pitfalls

### ❌ DON'T: Hardcode schema names
```python
# BAD
silver_table = f"{catalog}.company_silver.silver_store_dim"
```

### ✅ DO: Use variables
```python
# GOOD
silver_table = f"{catalog}.{silver_schema}.silver_store_dim"
```

### ❌ DON'T: Create schemas manually in scripts without considering prefixes
```python
# BAD - will create company_bronze in dev, not dev_user_company_bronze
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog}.company_bronze")
```

### ✅ DO: Use the schema variable passed from the bundle
```python
# GOOD - respects the prefixed schema name
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog}.{bronze_schema}")
```

## Schema Property Standards

**ALWAYS include these properties in `resources/schemas.yml`:**

```yaml
properties:
  # Performance
  delta.autoOptimize.optimizeWrite: "true"
  delta.autoOptimize.autoCompact: "true"
  databricks.pipelines.predictiveOptimizations.enabled: "true"
  
  # Governance
  layer: "<bronze|silver|gold>"
  source_system: "<source_name>"
  data_classification: "<confidential|internal|public>"
  business_owner: "<Team Name>"
  technical_owner: "Data Engineering"
  
  # Lifecycle
  retention_period: "<duration>"
  backup_enabled: "true"
```

## Enabling Predictive Optimization at Schema Level

**Predictive optimization should be enabled at the SCHEMA or CATALOG level, not per-table.**

### ✅ CORRECT: Using Dedicated DDL Commands

```python
# Enable for a specific schema (RECOMMENDED for layer-specific control)
spark.sql(f"ALTER SCHEMA {catalog}.{schema} ENABLE PREDICTIVE OPTIMIZATION")

# OR enable for entire catalog (simpler but less granular)
spark.sql(f"ALTER CATALOG {catalog} ENABLE PREDICTIVE OPTIMIZATION")

# To disable (if needed)
spark.sql(f"ALTER SCHEMA {catalog}.{schema} DISABLE PREDICTIVE OPTIMIZATION")

# To inherit from parent (catalog level)
spark.sql(f"ALTER SCHEMA {catalog}.{schema} INHERIT PREDICTIVE OPTIMIZATION")
```

### ❌ WRONG: Using Table Property Syntax

```python
# ❌ WRONG - This syntax doesn't work for schemas
spark.sql(f"""
    ALTER SCHEMA {catalog}.{schema} SET TBLPROPERTIES (
        'databricks.pipelines.predictiveOptimizations.enabled' = 'true'
    )
""")
# Error: PARSE_SYNTAX_ERROR - Syntax error at or near 'TBLPROPERTIES'
```

### Why This Matters

**Schema-level enablement is the recommended pattern because:**
- ✅ Single command enables for all tables in the schema
- ✅ More granular than catalog-level (allows per-layer control)
- ✅ Less tedious than per-table enablement (30+ tables)
- ✅ Consistent governance across all tables in the layer

**Typical deployment pattern:**
```python
def enable_predictive_optimization(spark: SparkSession, catalog: str):
    """Enable predictive optimization for all medallion schemas."""
    schemas = ['bronze_schema', 'silver_schema', 'gold_schema']
    
    for schema_name in schemas:
        try:
            spark.sql(f"ALTER SCHEMA {catalog}.{schema_name} ENABLE PREDICTIVE OPTIMIZATION")
            print(f"✓ Enabled predictive optimization for {catalog}.{schema_name}")
        except Exception as e:
            print(f"⚠ Could not enable for {schema_name}: {e}")
```

### Reference
- [Predictive Optimization for Catalog/Schema](https://docs.databricks.com/aws/en/optimizations/predictive-optimization#enable-or-disable-predictive-optimization-for-a-catalog-or-schema)

## Validation Checklist

When setting up schemas:
- [ ] `resources/schemas.yml` defines all schemas
- [ ] Each schema has comprehensive properties
- [ ] `databricks.yml` has dev target overrides with prefixes
- [ ] All Python scripts use schema variables (not hardcoded)
- [ ] DLT pipelines use `catalog:` and `schema:` fields
- [ ] Bronze/Gold scripts use `CREATE OR REPLACE TABLE`
- [ ] Schema creation happens before table creation

## Troubleshooting

### Issue: "Schema does not exist" in dev
**Solution:** Check that dev target overrides match the actual prefixed schema names.

### Issue: Double-prefixed schemas (dev_user_dev_user_schema)
**Solution:** DLT adds prefix automatically. Don't include the prefix in the DLT `schema:` field.

### Issue: Schema mismatch between Bronze and Silver
**Solution:** All jobs must use the same schema variable values from `databricks.yml`.

## References
- [Databricks Asset Bundles](https://docs.databricks.com/dev-tools/bundles/)
- [Unity Catalog Schemas](https://docs.databricks.com/data-governance/unity-catalog/create-schemas.html)
- [Schema Resources](https://docs.databricks.com/dev-tools/bundles/resources.html#schema)
