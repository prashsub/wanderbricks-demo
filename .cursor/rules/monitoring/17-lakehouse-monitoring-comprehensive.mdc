---
description: Comprehensive guide for Databricks Lakehouse Monitoring - setup, custom metrics, querying, and best practices
globs: **/*monitoring*.py
alwaysApply: false
---

# Lakehouse Monitoring: Complete Guide for Gold Layer

## üìã Table of Contents

1. [Core Principles](#core-principles)
2. [Setup & Configuration](#setup--configuration)
3. [Custom Metrics Design](#custom-metrics-design)
4. [Querying Metrics](#querying-metrics)
5. [Complete Examples](#complete-examples)
6. [Troubleshooting](#troubleshooting)
7. [References](#references)

---

## Core Principles

### Principle 1: Graceful Degradation
Monitor setup should handle SDK version differences, missing tables, and existing monitors gracefully.

### Principle 2: Business-First Metrics
Every custom metric must answer: **"What business decision would change based on this metric?"**

### Principle 3: Table-Level Business KPIs
**‚ö†Ô∏è CRITICAL:** For table-level business KPIs that reference each other, ALWAYS use `input_columns=[":table"]` for ALL metric types (AGGREGATE, DERIVED, DRIFT).

**Why This Matters:**
- DERIVED metrics can ONLY reference metrics in the same `column_name` row
- DRIFT metrics can ONLY compare metrics in the same `column_name` row  
- Mixing `input_columns` values breaks cross-references ‚Üí NULL values

**Decision Tree:**
```
Is this a table-level business KPI?
‚îú‚îÄ YES ‚Üí Use input_columns=[":table"]
‚îÇ        - Will be used in DERIVED metrics
‚îÇ        - Will be compared in DRIFT metrics
‚îÇ        - Represents overall business state
‚îÇ        ‚Üí ALL RELATED METRICS MUST USE [":table"]
‚îÇ
‚îî‚îÄ NO ‚Üí Is it column-specific profiling?
         ‚îî‚îÄ YES ‚Üí Use input_columns=["column_name"]
                  - Tracks column-specific statistics
                  - Won't be referenced by other metrics
                  - Pure data quality monitoring
```

### Principle 4: Where Custom Metrics Appear

**‚ö†Ô∏è CRITICAL:** Custom metrics appear as **NEW COLUMNS** in monitoring output tables:
- **AGGREGATE + DERIVED metrics** ‚Üí `{table}_profile_metrics` table (as new columns)
- **DRIFT metrics** ‚Üí `{table}_drift_metrics` table (as new columns)
- **There is NO separate `custom_metrics` table!**

---

## Setup & Configuration

### Import with Graceful Fallback

```python
# ‚úÖ CORRECT: Try-except for optional monitoring classes
try:
    from databricks.sdk.service.catalog import (
        MonitorTimeSeries, 
        MonitorSnapshot, 
        MonitorMetric, 
        MonitorMetricType, 
        MonitorCronSchedule
    )
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    print("‚ö†Ô∏è  Lakehouse Monitoring classes not available in this SDK version")

# Check before use
if not MONITORING_AVAILABLE:
    print("‚ö†Ô∏è  Skipping monitoring setup - SDK version incompatible")
    return
```

### Exception Handling

```python
from databricks.sdk.errors import ResourceAlreadyExists, ResourceDoesNotExist

try:
    monitor = workspace_client.quality_monitors.create(
        table_name=f"{catalog}.{schema}.{table}",
        # ... configuration
    )
    print(f"‚úì Monitor created for {table}")
except ResourceAlreadyExists:
    print(f"‚ö†Ô∏è  Monitor for {table} already exists - skipping")
except ResourceDoesNotExist:
    print(f"‚ö†Ô∏è  Table {table} does not exist - skipping monitor creation")
except Exception as e:
    print(f"‚ö†Ô∏è  Failed to create monitor for {table}: {e}")
```

### Monitor Mode Configuration

**Critical Rule: Always Specify ONE of: `snapshot`, `time_series`, or `inference_log`**

```python
# ‚ùå WRONG: No mode specified
monitor = workspace_client.quality_monitors.create(
    table_name=table_name,
    # ‚ùå ERROR: Must specify snapshot, time_series, or inference_log
)

# ‚úÖ CORRECT: Snapshot mode
monitor = workspace_client.quality_monitors.create(
    table_name=table_name,
    snapshot=MonitorSnapshot(),  # ‚úÖ For non-temporal data
    custom_metrics=[...],
)

# ‚úÖ CORRECT: Time series mode
monitor = workspace_client.quality_monitors.create(
    table_name=table_name,
    time_series=MonitorTimeSeries(
        timestamp_col="transaction_date",
        granularities=["1 day"]
    ),
    custom_metrics=[...],
)
```

**When to Use Each Mode:**

| Mode | Use Case | Example |
|------|----------|---------|
| `snapshot` | Daily snapshots without time dimension | `fact_inventory_snapshot` |
| `time_series` | Temporal data with timestamp column | `fact_sales_daily` |
| `inference_log` | ML model inference monitoring | Model prediction tables |

### Complete Monitor Creation Template

```python
def create_table_monitor(
    workspace_client: WorkspaceClient, 
    catalog: str, 
    schema: str,
    table: str,
    monitor_type: str = "time_series"
):
    """
    Create Lakehouse monitor with comprehensive error handling.
    
    Args:
        monitor_type: "snapshot" or "time_series"
    """
    table_name = f"{catalog}.{schema}.{table}"
    
    print(f"Creating {monitor_type} monitor for {table_name}...")
    
    try:
        # Configure monitor based on type
        if monitor_type == "snapshot":
            config = {"snapshot": MonitorSnapshot()}
        elif monitor_type == "time_series":
            config = {
                "time_series": MonitorTimeSeries(
                    timestamp_col="transaction_date",
                    granularities=["1 day"]
                )
            }
        
        monitor = workspace_client.quality_monitors.create(
            table_name=table_name,
            assets_dir=f"/Workspace/Shared/lakehouse_monitoring/{catalog}/{schema}",
            output_schema_name=f"{catalog}.{schema}_monitoring",
            **config,
            custom_metrics=[
                # See Custom Metrics Design section below
            ],
            slicing_exprs=["store_number", "upc_code"],  # Dimensional analysis
            schedule=MonitorCronSchedule(
                quartz_cron_expression="0 0 2 * * ?",  # Daily at 2 AM
                timezone_id="America/New_York"
            )
        )
        
        print(f"‚úì Monitor created for {table_name}")
        if hasattr(monitor, 'table_name'):
            print(f"  Table: {monitor.table_name}")
        if hasattr(monitor, 'dashboard_id'):
            print(f"  Dashboard: {monitor.dashboard_id}")
        
        return monitor
        
    except ResourceAlreadyExists:
        print(f"‚ö†Ô∏è  Monitor for {table} already exists - skipping")
        return None
        
    except ResourceDoesNotExist:
        print(f"‚ö†Ô∏è  Table {table} does not exist - skipping monitor creation")
        return None
        
    except Exception as e:
        print(f"‚ùå Failed to create monitor for {table}: {str(e)}")
        raise
```

### Async Operations Pattern

**Critical Rule: Wait for Table Creation**

Lakehouse Monitoring creates output tables asynchronously:
1. Monitor API call returns immediately (~30 seconds)
2. Initial profiling runs in background (~15 minutes)
3. Tables don't exist until profiling completes

```python
def wait_with_progress(minutes: int = 15):
    """Wait with progress updates."""
    wait_seconds = minutes * 60
    for elapsed in range(0, wait_seconds, 60):
        progress_pct = (elapsed / wait_seconds) * 100
        remaining = wait_seconds - elapsed
        print(f"‚è±Ô∏è  Progress: {progress_pct:.1f}% | Remaining: {remaining//60}m")
        time.sleep(60)
    print(f"‚úì Wait completed - tables should be ready")

# Use in workflow
create_sales_monitor(workspace_client, catalog, schema)
wait_with_progress(minutes=15)  # ‚úÖ Wait for async table creation
document_monitoring_tables(spark, catalog, schema)  # ‚úÖ Now tables exist
```

### Complete Monitor Cleanup Pattern

**Critical Rule: Delete Monitor AND Output Tables**

Problem: Deleting a monitor doesn't delete its output tables, causing schema conflicts.

```python
def delete_monitor_if_exists(workspace_client: WorkspaceClient, table_name: str, spark=None):
    """Complete cleanup: monitor definition + output tables."""
    from pyspark.sql import SparkSession
    
    if spark is None:
        spark = SparkSession.getActiveSession()
    
    try:
        # 1. Check if monitor exists
        workspace_client.quality_monitors.get(table_name=table_name)
        
        # 2. Delete monitor definition
        print(f"  Deleting existing monitor for {table_name}...")
        workspace_client.quality_monitors.delete(table_name=table_name)
        print(f"  ‚úì Existing monitor deleted")
        
        # 3. Parse table name to construct monitoring table names
        catalog, schema, table = table_name.split(".")
        monitoring_schema = f"{schema}_monitoring"
        
        # 4. Drop profile_metrics table
        profile_table = f"{catalog}.{monitoring_schema}.{table}_profile_metrics"
        print(f"  Dropping profile metrics table: {profile_table}...")
        spark.sql(f"DROP TABLE IF EXISTS {profile_table}")
        print(f"  ‚úì Profile metrics table dropped")
        
        # 5. Drop drift_metrics table
        drift_table = f"{catalog}.{monitoring_schema}.{table}_drift_metrics"
        print(f"  Dropping drift metrics table: {drift_table}...")
        spark.sql(f"DROP TABLE IF EXISTS {drift_table}")
        print(f"  ‚úì Drift metrics table dropped")
        
        return True
        
    except ResourceDoesNotExist:
        return False  # Silent success - nothing to delete
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error checking/deleting monitor: {str(e)}")
        return False

# Use before creating new monitor
delete_monitor_if_exists(workspace_client, sales_table, spark)
create_sales_monitor(workspace_client, catalog, schema)
```

### Monitor Update Pattern

**‚ö†Ô∏è CRITICAL:** `quality_monitors.update()` is a **REPLACEMENT operation, not a MERGE**. If you omit fields (especially `custom_metrics`), they are **DELETED**.

**‚úÖ CORRECT: Import from Pure Python Configuration File**

```python
# monitor_configs.py (pure Python file - NOT a notebook)
"""Centralized Monitor Configuration for Lakehouse Monitoring"""

from databricks.sdk.service.catalog import (
    MonitorTimeSeries, MonitorSnapshot, MonitorMetric, 
    MonitorMetricType, MonitorCronSchedule
)
import pyspark.sql.types as T

def get_all_monitor_configs(catalog: str, schema: str):
    """Get all monitor configurations with FULL custom metrics"""
    return [
        {
            "table_name": f"{catalog}.{schema}.fact_sales_daily",
            "custom_metrics": [... 99 metrics ...],  # ‚úÖ MUST include all metrics
            "slicing_exprs": ["store_number"],
        }
    ]
```

```python
# update_monitors.py (notebook)
from monitor_configs import get_all_monitor_configs
from databricks.sdk import WorkspaceClient

def main():
    catalog, schema = get_parameters()
    workspace_client = WorkspaceClient()
    
    # ‚úÖ Get FULL configuration including all custom metrics
    monitor_configs = get_all_monitor_configs(catalog, schema)
    
    # Verify custom metrics are included
    for config in monitor_configs:
        custom_metrics = config.get('custom_metrics', [])
        print(f"Updating with {len(custom_metrics)} custom metrics")
    
    # Update with full configuration
    for config in monitor_configs:
        workspace_client.quality_monitors.update(**config)
```

**Update Behavior:**

| What You Pass | What Happens |
|---------------|--------------|
| `custom_metrics=None` | ‚ùå Deletes all custom metrics |
| `custom_metrics=[]` | ‚ùå Deletes all custom metrics |
| Omit `custom_metrics` | ‚ùå Deletes all custom metrics |
| `custom_metrics=[...]` | ‚úÖ Replaces with provided list |

---

## Custom Metrics Design

### Business-Focused Metric Categories

#### 1. Transaction Pattern Metrics

**Purpose:** Track customer purchasing behavior changes

```python
# Average spend per transaction
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="avg_transaction_amount",
    input_columns=[":table"],  # ‚úÖ Table-level business KPI
    definition="AVG(avg_transaction_value)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# Items per basket (DERIVED - references AGGREGATE metrics)
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
    name="avg_items_per_transaction",
    input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
    definition="total_net_units / NULLIF(total_transactions, 0)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# Store traffic
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="transactions_per_store_per_day",
    input_columns=[":table"],  # ‚úÖ Table-level business KPI
    definition="AVG(transaction_count)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)
```

**Business Questions Answered:**
- Is average basket size declining?
- Are customers buying fewer items?
- Is store traffic decreasing?

#### 2. Product Performance Metrics

**Purpose:** Monitor product sales velocity and availability

```python
# Product sales velocity
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="product_velocity",
    input_columns=[":table"],  # ‚úÖ Table-level business KPI
    definition="SUM(net_units) / NULLIF(COUNT(DISTINCT store_number), 0)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# Dollar productivity per SKU
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="revenue_per_product",
    input_columns=[":table"],  # ‚úÖ Table-level business KPI
    definition="AVG(net_revenue)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# In-stock percentage
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="product_availability_rate",
    input_columns=[":table"],  # ‚úÖ Table-level business KPI
    definition="(COUNT(CASE WHEN net_units > 0 THEN 1 END) / NULLIF(COUNT(*), 0)) * 100",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)
```

#### 3. Customer Segmentation Metrics

**Purpose:** Track loyalty program and customer behavior

```python
# Loyalty member value (DERIVED - references AGGREGATE)
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
    name="loyalty_member_avg_spend",
    input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
    definition="total_net_revenue / NULLIF(total_loyalty_customers, 0)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# Member visit frequency (DERIVED - references AGGREGATE)
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
    name="loyalty_transaction_frequency",
    input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
    definition="total_loyalty_transactions / NULLIF(total_loyalty_customers, 0)",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)
```

#### 4. Promotional Effectiveness Metrics

**Purpose:** Track discount strategy and campaign ROI

```python
# Discount intensity (DERIVED - references AGGREGATE)
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
    name="discount_intensity",
    input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
    definition="(total_all_discounts / NULLIF(total_gross_revenue, 0)) * 100",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# Campaign effectiveness
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="coupon_redemption_rate",
    input_columns=[":table"],  # ‚úÖ Table-level business KPI
    definition="(COUNT(CASE WHEN coupon_discount_total > 0 THEN 1 END) / NULLIF(COUNT(*), 0)) * 100",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)
```

#### 5. Drift Metrics (Period-over-Period Comparison)

**Purpose:** Automatic anomaly detection for business KPIs

```python
# Revenue drift percentage
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DRIFT,
    name="revenue_drift_pct",
    input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
    definition="(({{current_df}}.total_net_revenue - {{base_df}}.total_net_revenue) / NULLIF({{base_df}}.total_net_revenue, 0)) * 100",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)

# Transaction volume drift
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DRIFT,
    name="transaction_drift_pct",
    input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
    definition="(({{current_df}}.total_transactions - {{base_df}}.total_transactions) / NULLIF({{base_df}}.total_transactions, 0)) * 100",
    output_data_type=T.StructField("output", T.DoubleType()).json()
)
```

### Custom Metric Limitations

**Critical Rule: No Nested Aggregations**

Databricks does NOT support aggregate functions inside other aggregate functions.

```python
# ‚ùå WRONG: Nested aggregation
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="top_10_stores_revenue_share",
    definition="(SUM(CASE WHEN net_revenue >= PERCENTILE(net_revenue, 0.9) THEN net_revenue ELSE 0 END) / NULLIF(SUM(net_revenue), 0)) * 100"
    # ‚ùå ERROR: PERCENTILE inside SUM
)

# ‚úÖ CORRECT: Two-step pattern (AGGREGATE ‚Üí DERIVED)
# Step 1: Define base aggregates
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="total_revenue",
    input_columns=[":table"],
    definition="SUM(revenue)"
),
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="p90_revenue",
    input_columns=[":table"],
    definition="PERCENTILE(revenue, 0.9)"
),

# Step 2: Define derived ratio
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
    name="top_performer_share",
    input_columns=[":table"],
    definition="p90_revenue / NULLIF(total_revenue, 0) * 100"  # ‚úÖ References aggregates
)
```

### Metric Organization Pattern

**Group metrics by business domain with clear comments:**

```python
custom_metrics=[
    # ========================================
    # TRANSACTION PATTERN METRICS
    # ========================================
    # Purpose: Track customer purchasing behavior changes
    
    MonitorMetric(...),  # avg_transaction_amount
    MonitorMetric(...),  # avg_items_per_transaction
    MonitorMetric(...),  # transactions_per_store_per_day
    
    # ========================================
    # PRODUCT PERFORMANCE METRICS
    # ========================================
    # Purpose: Monitor product sales velocity and availability
    
    MonitorMetric(...),  # product_velocity
    MonitorMetric(...),  # revenue_per_product
    MonitorMetric(...),  # product_availability_rate
    
    # ... more categories
]
```

### Documenting Custom Metrics

**Critical: Document metrics in the correct tables**

Custom metrics appear as NEW COLUMNS in monitoring output tables:
- AGGREGATE + DERIVED metrics ‚Üí `{table}_profile_metrics` table
- DRIFT metrics ‚Üí `{table}_drift_metrics` table
- **There is NO separate `custom_metrics` table!**

```python
def document_profile_metrics_table(spark: SparkSession, catalog: str, schema: str, table: str):
    """
    Document profile_metrics table including custom metric columns.
    
    ‚ö†Ô∏è KEY INSIGHT: Custom aggregate/derived metrics appear as NEW COLUMNS here.
    """
    monitoring_schema = f"{schema}_monitoring"
    profile_table = f"{catalog}.{monitoring_schema}.{table}_profile_metrics"
    
    if table == "fact_sales_daily":
        custom_metrics_descriptions = {
            # Use dual-purpose format: Business + Technical
            "avg_transaction_amount": """Average transaction basket size. 
                Business: Tracks how customer spending per visit changes over time. 
                Decline signals reduced basket size. 
                Technical: AVG(avg_transaction_value), key drift indicator.""",
            
            "product_velocity": """Average units sold per store. 
                Business: Measures product movement speed changes. 
                Technical: SUM(net_units) / COUNT(DISTINCT stores).""",
            
            # ... more metrics
        }
        
        for column_name, description in custom_metrics_descriptions.items():
            try:
                spark.sql(f"ALTER TABLE {profile_table} ALTER COLUMN {column_name} COMMENT '{description}'")
            except:
                pass  # Column may not exist yet
```

---

## Querying Metrics

### Storage Pattern by Metric Type

**Critical Rule:** The `input_columns` parameter determines WHERE metrics are stored.

| input_columns Value | Stored Where |
|---------------------|--------------|
| `["column_name"]` | `column_name = 'column_name'` row |
| `[":table"]` | `column_name = ':table'` row |
| `["col1", "col2"]` | `column_name = ':table'` (multi-column) |

### Query Pattern 1: Table-Level AGGREGATE (Direct SELECT)

**Recommended for business KPIs**

```sql
-- Python: input_columns=[":table"]

SELECT 
  window.start,
  window.end,
  -- All table-level metrics available directly
  total_gross_revenue,
  total_net_revenue,
  total_transactions,
  avg_transaction_amount
FROM fact_sales_daily_profile_metrics
WHERE log_type = 'INPUT'
  AND column_name = ':table'  -- ‚úÖ All table-level metrics here
  AND COALESCE(slice_key, 'No Slice') = :slice_key
ORDER BY window.start DESC
```

### Query Pattern 2: DERIVED Metrics (Direct SELECT)

```sql
-- Python: input_columns=[":table"]

SELECT 
  window.start,
  window.end,
  overall_return_rate,
  units_per_transaction,
  revenue_per_unit,
  discount_intensity
FROM fact_sales_daily_profile_metrics
WHERE log_type = 'INPUT'
  AND column_name = ':table'  -- All DERIVED metrics here
  AND COALESCE(slice_key, 'No Slice') = :slice_key
```

### Query Pattern 3: DRIFT Metrics (Separate Table)

```sql
-- Python: input_columns=[":table"]

SELECT 
  window.start,
  drift_type,
  revenue_drift_pct,
  transaction_drift_pct,
  unit_sales_drift_pct
FROM fact_sales_daily_drift_metrics
WHERE drift_type = 'CONSECUTIVE'  -- or 'BASELINE'
  AND column_name = ':table'
  AND COALESCE(slice_key, 'No Slice') = :slice_key
ORDER BY window.start DESC
```

### Query Pattern 4: Per-Column AGGREGATE (PIVOT Required)

**‚ö†Ô∏è Only for column-specific profiling (rare use case)**

```sql
-- Python: input_columns=["gross_revenue"]

WITH base_metrics AS (
  SELECT 
    window, granularity, slice_key, slice_value,
    column_name,
    gross_revenue_outlier_count,  -- Custom metric column
    net_revenue_outlier_count
  FROM fact_sales_daily_profile_metrics
  WHERE log_type = 'INPUT'
    AND column_name IN ('gross_revenue', 'net_revenue')  -- Input columns
)
SELECT 
  window.start,
  window.end,
  MAX(CASE WHEN column_name = 'gross_revenue' THEN gross_revenue_outlier_count END) AS gross_revenue_outliers,
  MAX(CASE WHEN column_name = 'net_revenue' THEN net_revenue_outlier_count END) AS net_revenue_outliers
FROM base_metrics
GROUP BY window.start, window.end, granularity, slice_key, slice_value
```

### AI/BI Dashboard Dataset Patterns

**Dataset Type 1: Table-Level Business KPIs (Direct SELECT)**

```sql
-- No PIVOT needed for table-level metrics!
SELECT 
  window.start AS date,
  window.end,
  COALESCE(slice_key, 'No Slice') AS slice_key_display,
  COALESCE(slice_value, 'No Slice') AS slice_value_display,
  total_net_revenue,
  total_transactions,
  avg_transaction_amount,
  overall_return_rate,
  discount_intensity
FROM fact_sales_daily_profile_metrics
WHERE log_type = 'INPUT'
  AND column_name = ':table'  -- ‚úÖ All table-level metrics in one row
  AND slice_key_display = :slice_key
  AND slice_value_display = :slice_value
ORDER BY date DESC
```

**Dataset Type 2: Slice Filters (Cascading)**

```sql
-- Slice Key Filter
SELECT 'No Slice' AS `Slice Key`
UNION ALL
SELECT DISTINCT slice_key AS `Slice Key`
FROM fact_sales_daily_profile_metrics
WHERE slice_key IS NOT NULL
ORDER BY `Slice Key`

-- Slice Value Filter (depends on Slice Key)
SELECT 'No Slice' AS `Slice Value`
UNION ALL
SELECT DISTINCT slice_value AS `Slice Value`
FROM fact_sales_daily_profile_metrics
WHERE COALESCE(slice_key, 'No Slice') = :slice_key
  AND slice_value IS NOT NULL
ORDER BY `Slice Value`
```

---

## Complete Examples

### Example 1: Sales Monitor with Comprehensive Metrics

```python
def create_sales_monitor(workspace_client: WorkspaceClient, catalog: str, schema: str):
    """Create fact_sales_daily monitor with business-focused metrics."""
    
    table_name = f"{catalog}.{schema}.fact_sales_daily"
    
    # Delete existing monitor + tables
    delete_monitor_if_exists(workspace_client, table_name, spark)
    
    # Create new monitor
    monitor = workspace_client.quality_monitors.create(
        table_name=table_name,
        assets_dir=f"/Workspace/Shared/lakehouse_monitoring/{catalog}/{schema}",
        output_schema_name=f"{catalog}.{schema}_monitoring",
        time_series=MonitorTimeSeries(
            timestamp_col="transaction_date",
            granularities=["1 day"]
        ),
        custom_metrics=[
            # ========================================
            # AGGREGATE METRICS (Base Measurements)
            # ========================================
            MonitorMetric(
                type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
                name="total_gross_revenue",
                input_columns=[":table"],  # ‚úÖ Table-level
                definition="SUM(gross_revenue)",
                output_data_type=T.StructField("output", T.DoubleType()).json()
            ),
            MonitorMetric(
                type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
                name="total_net_revenue",
                input_columns=[":table"],  # ‚úÖ Table-level
                definition="SUM(net_revenue)",
                output_data_type=T.StructField("output", T.DoubleType()).json()
            ),
            MonitorMetric(
                type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
                name="total_transactions",
                input_columns=[":table"],  # ‚úÖ Table-level
                definition="SUM(transaction_count)",
                output_data_type=T.StructField("output", T.LongType()).json()
            ),
            
            # ========================================
            # DERIVED METRICS (Business Ratios)
            # ========================================
            MonitorMetric(
                type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
                name="overall_return_rate",
                input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
                definition="(total_return_amount / NULLIF(total_gross_revenue, 0)) * 100",
                output_data_type=T.StructField("output", T.DoubleType()).json()
            ),
            MonitorMetric(
                type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
                name="avg_items_per_transaction",
                input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
                definition="total_net_units / NULLIF(total_transactions, 0)",
                output_data_type=T.StructField("output", T.DoubleType()).json()
            ),
            
            # ========================================
            # DRIFT METRICS (Period Comparison)
            # ========================================
            MonitorMetric(
                type=MonitorMetricType.CUSTOM_METRIC_TYPE_DRIFT,
                name="revenue_drift_pct",
                input_columns=[":table"],  # ‚úÖ Must match AGGREGATE
                definition="(({{current_df}}.total_net_revenue - {{base_df}}.total_net_revenue) / NULLIF({{base_df}}.total_net_revenue, 0)) * 100",
                output_data_type=T.StructField("output", T.DoubleType()).json()
            ),
        ],
        slicing_exprs=["store_number", "upc_code"],
        schedule=MonitorCronSchedule(
            quartz_cron_expression="0 0 2 * * ?",
            timezone_id="America/New_York"
        )
    )
    
    return monitor
```

### Example 2: Complete Workflow

```python
def main():
    """Setup Lakehouse monitoring with graceful error handling."""
    
    # Check SDK compatibility
    if not MONITORING_AVAILABLE:
        print("‚ö†Ô∏è  Skipping monitoring - incompatible SDK version")
        return
    
    catalog, schema = get_parameters()
    spark = SparkSession.getActiveSession()
    workspace_client = WorkspaceClient()
    
    # Tables to monitor with their types
    monitoring_config = [
        ("fact_sales_daily", "time_series"),
        ("fact_inventory_snapshot", "snapshot"),
    ]
    
    monitors_created = []
    
    for table, monitor_type in monitoring_config:
        try:
            monitor = create_table_monitor(
                workspace_client, 
                catalog, 
                schema, 
                table,
                monitor_type
            )
            if monitor:
                monitors_created.append(table)
        except Exception as e:
            print(f"‚ö†Ô∏è  Continuing after error with {table}: {e}")
            continue
    
    print("\n" + "=" * 80)
    print("‚úì Lakehouse Monitoring setup completed")
    print("=" * 80)
    print(f"\nMonitors successfully created: {len(monitors_created)}")
    for table in monitors_created:
        print(f"  ‚Ä¢ {table}")
    
    # Wait for tables to be created
    if monitors_created:
        print("\n‚è±Ô∏è  Waiting 15 minutes for monitor initialization...")
        wait_with_progress(minutes=15)
        
        # Document monitoring tables
        print("\nüìù Documenting monitoring tables...")
        for table in monitors_created:
            document_monitoring_tables(spark, catalog, schema, table)
        
        print("\n‚úÖ Monitoring setup complete with documentation!")
```

---

## Troubleshooting

### Common Mistakes

#### ‚ùå Mistake 1: Mixing `input_columns` Values

**THE MOST COMMON ERROR**

```python
# ‚ùå WRONG: AGGREGATE and DERIVED use different input_columns
MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_AGGREGATE,
    name="total_gross_revenue",
    input_columns=["gross_revenue"],  # ‚Üê Stored in 'gross_revenue' row
    definition="SUM(gross_revenue)"
)

MonitorMetric(
    type=MonitorMetricType.CUSTOM_METRIC_TYPE_DERIVED,
    name="overall_return_rate",
    input_columns=[":table"],  # ‚Üê Looks in ':table' row
    definition="(total_return_amount / NULLIF(total_gross_revenue, 0)) * 100"
    # ‚ùå Can't find total_gross_revenue!
)

# Result: overall_return_rate is NULL
```

```python
# ‚úÖ CORRECT: All use same input_columns
MonitorMetric(
    name="total_gross_revenue",
    input_columns=[":table"],  # ‚úÖ Table-level
    definition="SUM(gross_revenue)"
)

MonitorMetric(
    name="overall_return_rate",
    input_columns=[":table"],  # ‚úÖ Same location
    definition="(total_return_amount / NULLIF(total_gross_revenue, 0)) * 100"
)

# Result: Both calculate correctly
```

#### ‚ùå Mistake 2: Not Specifying Monitor Mode

```python
# ‚ùå WRONG: No mode specified
monitor = workspace_client.quality_monitors.create(
    table_name=table_name,
    # Missing: snapshot, time_series, or inference_log
)

# ‚úÖ CORRECT: Explicit mode
monitor = workspace_client.quality_monitors.create(
    table_name=table_name,
    snapshot=MonitorSnapshot(),  # ‚úÖ or time_series
    custom_metrics=[...],
)
```

#### ‚ùå Mistake 3: Querying Wrong `column_name`

```python
# Python: input_columns=[":table"]

# ‚ùå WRONG Query:
WHERE column_name IN ('gross_revenue', 'net_revenue')  # Returns NULL!

# ‚úÖ CORRECT Query:
WHERE column_name = ':table'  # Has values!
```

#### ‚ùå Mistake 4: Updating Without Full Custom Metrics

```python
# ‚ùå WRONG: Simplified config deletes all metrics
def get_all_monitor_configs(catalog, schema):
    return [
        {
            "table_name": f"{catalog}.{schema}.fact_sales_daily",
            "slicing_exprs": ["store_number"],
            # ‚ùå Missing custom_metrics ‚Üí ALL 99 METRICS DELETED!
        }
    ]

# ‚úÖ CORRECT: Always include full custom_metrics
def get_all_monitor_configs(catalog, schema):
    return [
        {
            "table_name": f"{catalog}.{schema}.fact_sales_daily",
            "custom_metrics": [... all 99 metrics ...],  # ‚úÖ Full list
            "slicing_exprs": ["store_number"],
        }
    ]
```

#### ‚ùå Mistake 5: Trying to Document `custom_metrics` Table

```python
# ‚ùå WRONG: This table doesn't exist!
custom_table = f"{catalog}.{schema}_monitoring.fact_sales_daily_custom_metrics"
spark.sql(f"ALTER TABLE {custom_table} ALTER COLUMN metric_name COMMENT '...'")
# ‚ùå ERROR: TABLE_OR_VIEW_NOT_FOUND

# ‚úÖ CORRECT: Document columns in profile_metrics and drift_metrics
profile_table = f"{catalog}.{schema}_monitoring.fact_sales_daily_profile_metrics"
spark.sql(f"ALTER TABLE {profile_table} ALTER COLUMN total_net_revenue COMMENT '...'")
```

### Verification Workflow

**Step 1: Check Python Definition**

```python
# In lakehouse_monitoring.py
MonitorMetric(
    name="total_gross_revenue",
    input_columns=[":table"],  # ‚Üê Note this!
    definition="SUM(gross_revenue)"
)
```

**Step 2: Determine Storage Location**

| input_columns | Stored Where |
|---------------|--------------|
| `[":table"]` | `column_name = ':table'` |
| `["gross_revenue"]` | `column_name = 'gross_revenue'` |

**Step 3: Test Query**

```sql
-- Debug: See where metric has values
SELECT column_name, total_gross_revenue
FROM fact_sales_daily_profile_metrics
WHERE total_gross_revenue IS NOT NULL
  AND log_type = 'INPUT'
LIMIT 10;

-- Result shows: column_name = ':table'
```

### Validation Checklist

**Setup & Configuration:**
- [ ] Import monitoring classes with try-except
- [ ] Check MONITORING_AVAILABLE before creating
- [ ] Specify ONE of: snapshot, time_series, inference_log
- [ ] Handle ResourceAlreadyExists and ResourceDoesNotExist
- [ ] Use hasattr() for MonitorInfo attributes
- [ ] Delete existing monitor + tables before recreating

**Custom Metrics:**
- [ ] **CRITICAL:** All table-level business KPIs use `input_columns=[":table"]`
- [ ] All related metrics (AGGREGATE/DERIVED/DRIFT) use same `input_columns`
- [ ] No nested aggregations
- [ ] Use DERIVED metrics for ratios
- [ ] NULLIF guards against division by zero
- [ ] All metrics have output_data_type specified
- [ ] Metrics organized by business category

**Querying:**
- [ ] Use `log_type = 'INPUT'` (not 'OUTPUT')
- [ ] Filter to correct `column_name` value
- [ ] Handle NULL slices with COALESCE
- [ ] Use PIVOT only for per-column metrics (rare)
- [ ] Direct SELECT for table-level metrics (common)

**Documentation:**
- [ ] Document metrics in `profile_metrics` (AGGREGATE + DERIVED)
- [ ] Document metrics in `drift_metrics` (DRIFT type)
- [ ] Use dual-purpose format (Business + Technical)
- [ ] Do NOT try to document `custom_metrics` table (doesn't exist)

**Workflow:**
- [ ] Wait 15+ minutes after creation before querying
- [ ] Update with FULL configuration (never omit custom_metrics)
- [ ] Use silent success pattern for idempotent operations
- [ ] Verify metric counts match before/after updates

---

## References

### Official Documentation
- [Lakehouse Monitoring Guide](https://docs.databricks.com/lakehouse-monitoring/)
- [Monitor API Reference](https://docs.databricks.com/api/workspace/qualitymonitors/create)
- [Custom Metrics](https://learn.microsoft.com/azure/databricks/lakehouse-monitoring/custom-metrics)
- [Profile Metrics Table Schema](https://docs.databricks.com/lakehouse-monitoring/monitor-output#profile-metrics-table-schema)
- [Drift Metrics Table Schema](https://docs.databricks.com/lakehouse-monitoring/monitor-output#drift-metrics-table-schema)
- [Column Schemas for Generated Tables](https://docs.databricks.com/lakehouse-monitoring/monitor-output#column-schemas-for-generated-tables) - **Critical for understanding where custom metrics appear**
- [Quality Monitors Update API](https://docs.databricks.com/api/workspace/qualitymonitors/update)
- [Time Series Monitoring Example](https://docs.databricks.com/notebooks/source/monitoring/timeseries-monitor.html)

### Project Implementation
- `src/company_gold/lakehouse_monitoring.py` - Monitor creation with 144 custom metrics
- `src/company_gold/monitor_configs.py` - Centralized configuration (pure Python file)
- `src/company_gold/document_monitoring_tables.py` - Automated documentation
- `src/company_gold/update_monitors.py` - Safe monitor updates

### Case Studies

#### input_columns Pattern for Table-Level KPIs (October 2025)
**Discovery:** For table-level business KPIs, ALL metrics must use `input_columns=[":table"]`
- Updated 83 AGGREGATE metrics (60 sales + 23 inventory)
- Queries simplified from PIVOT to direct SELECT
- Query performance improved ~40%
- See: `docs/RULE_IMPROVEMENT_INPUT_COLUMNS_PATTERN.md`

#### Custom Metrics as Table Columns (October 2025)
**Discovery:** Custom metrics appear as NEW COLUMNS, not in separate table
- No `custom_metrics` table exists (common misconception)
- AGGREGATE/DERIVED ‚Üí `profile_metrics` columns
- DRIFT ‚Üí `drift_metrics` columns
- See: `docs/MONITORING_DOCUMENTATION_ERROR_HANDLING_FIX.md`

---

## Summary

**This comprehensive guide covers:**
1. ‚úÖ Setup & configuration with error handling
2. ‚úÖ Custom metrics design for business KPIs
3. ‚úÖ Query patterns for dashboards
4. ‚úÖ Complete working examples
5. ‚úÖ Troubleshooting common issues

**Key Takeaways:**
- Always use `input_columns=[":table"]` for table-level business KPIs
- Custom metrics appear as NEW COLUMNS (no separate `custom_metrics` table)
- DERIVED metrics can only reference metrics in the same `column_name` row
- Wait 15+ minutes after creation for tables to be ready
- Always update with FULL configuration (never omit custom_metrics)
- Document metrics in `profile_metrics` and `drift_metrics` tables

**Next Steps:**
1. Read setup section for monitor creation
2. Review custom metrics section for metric design
3. Use query patterns for dashboards
4. Follow examples for implementation
5. Reference troubleshooting when issues arise
