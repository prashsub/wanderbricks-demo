---
description: Best practices for creating Databricks Lakeview AI/BI dashboards with proper system table usage and layout formatting
globs: ["**/dashboards/**/*.json", "**/*.lvdash.json"]
alwaysApply: false
---
# Databricks AI/BI Lakeview Dashboard Patterns

## üö® CRITICAL PATTERNS - Read First

This rule documents hard-won learnings from building production dashboards. The patterns below prevented 100+ deployment failures and visualization errors.

### Top 5 Causes of Dashboard Failures

| Rank | Issue | Impact | Prevention |
|------|-------|--------|------------|
| 1 | Widget-Query Column Mismatch | "no fields to visualize" | Always use explicit SQL aliases matching widget `fieldName` |
| 2 | Incorrect Number Formatting | 0% or empty values | Return raw numbers, not formatted strings |
| 3 | Missing Parameter Definitions | Unbound parameter errors | Define ALL parameters in dataset's `parameters` array |
| 4 | Monitoring Table Schema | Column not found errors | Use `CASE` pivots on `column_name`, access `window.start` |
| 5 | Pie Chart Scale Missing | Empty pie charts | Add explicit `scale` to both `color` and `angle` encodings |

---

## 1. Widget-Query Column Alignment (CRITICAL)

### The Rule
**Widget `fieldName` in encodings MUST exactly match query output column alias.**

### ‚ùå WRONG - Column name mismatch
```json
// Widget expects "total_queries"
"encodings": {
  "value": { "fieldName": "total_queries" }
}
```
```sql
-- Query returns "query_count" - MISMATCH!
SELECT COUNT(*) AS query_count FROM ...
```

### ‚úÖ CORRECT - Aligned column names
```json
// Widget expects "total_queries"
"encodings": {
  "value": { "fieldName": "total_queries" }
}
```
```sql
-- Query returns "total_queries" - MATCH!
SELECT COUNT(*) AS total_queries FROM ...
```

### Common Mismatches to Avoid

| Widget Expects | Query Often Returns | Fix |
|----------------|---------------------|-----|
| `total_queries` | `query_count` | Alias as `total_queries` |
| `warehouse_name` | `compute_type` | Alias as `warehouse_name` |
| `unique_users` | `distinct_users` | Alias as `unique_users` |
| `query_id` | `statement_id` | Alias as `query_id` |
| `failure_count` | `failed_queries` | Alias as `failure_count` |
| `total_cost` | `cost_30d` | Match widget expectation |

### Validation Pattern
```python
# Before deployment, validate each widget
def validate_widget_columns(widget, dataset):
    widget_fields = extract_field_names(widget['spec']['encodings'])
    query_columns = extract_select_aliases(dataset['query'])
    missing = widget_fields - query_columns
    if missing:
        raise ValueError(f"Widget expects {missing} but query doesn't return them")
```

---

## 2. Number Formatting Rules (CRITICAL)

### Format Behavior Reference

| Format Type | Expects | Multiplies? | Example Input ‚Üí Output |
|-------------|---------|-------------|------------------------|
| `number-plain` | Raw number | No | `1234` ‚Üí `1,234` |
| `number-percent` | 0-1 decimal | Yes (√ó100) | `0.85` ‚Üí `85%` |
| `number-currency` | Raw number | No | `1234.56` ‚Üí `$1,234.56` |

### ‚ùå WRONG - Formatted strings
```sql
-- Widget uses number-currency format
SELECT CONCAT('$', FORMAT_NUMBER(SUM(cost), 2)) AS total_cost  -- WRONG!
```
**Result:** Empty or error - widget can't parse formatted string

### ‚úÖ CORRECT - Raw numbers
```sql
-- Widget uses number-currency format
SELECT ROUND(SUM(cost), 2) AS total_cost  -- CORRECT!
```

### ‚ùå WRONG - Percentage as 85
```sql
-- Widget uses number-percent format
SELECT ROUND(success_count * 100.0 / total_count, 1) AS success_rate  -- Shows 8500%!
```

### ‚úÖ CORRECT - Percentage as 0.85
```sql
-- Widget uses number-percent format
SELECT ROUND(success_count * 1.0 / NULLIF(total_count, 0), 3) AS success_rate  -- Shows 85%
```

### Key Rules
1. **NEVER use `FORMAT_NUMBER()`** in queries for KPI widgets
2. **NEVER use `CONCAT('$', ...)` or `CONCAT(..., '%')`** for formatted widgets
3. For percentages: Return 0-1 decimal, let widget multiply by 100
4. For currency: Return raw numeric, let widget add symbol and commas

---

## 3. Parameter Configuration (CRITICAL)

### Complete Parameter Definition

Every dataset using parameters MUST define them in the `parameters` array:

```json
{
  "name": "ds_kpi_revenue",
  "query": "SELECT SUM(cost) FROM table WHERE date BETWEEN :time_range.min AND :time_range.max",
  "parameters": [
    {
      "keyword": "time_range",
      "dataType": "DATETIME_RANGE",
      "defaultSelection": {
        "range": {
          "min": { "dataType": "DATETIME", "value": "now-30d/d" },
          "max": { "dataType": "DATETIME", "value": "now/d" }
        }
      }
    }
  ]
}
```

### Time Range Parameter Access
```sql
-- Access min and max from time_range parameter
WHERE usage_date BETWEEN :time_range.min AND :time_range.max
```

### Multi-Select Parameter
```json
{
  "keyword": "param_workspace",
  "dataType": "STRING",
  "multiValuesOptions": { "enabled": true },
  "defaultSelection": {
    "values": {
      "dataType": "STRING",
      "values": [{ "value": "All" }]
    }
  }
}
```

### Filter Widget with Parameter Queries
```json
{
  "widget": {
    "name": "filter_time_range",
    "spec": {
      "widgetType": "filter-date-range-picker",
      "encodings": {
        "start": { "parameterKeyword": "time_range.min" },
        "end": { "parameterKeyword": "time_range.max" }
      }
    },
    "param_queries": [
      { "paramDatasetName": "ds_kpi_revenue", "queryName": "param_ds_kpi_revenue" },
      { "paramDatasetName": "ds_trend", "queryName": "param_ds_trend" }
    ]
  }
}
```

### Text Input Parameter (e.g., Annual Commit)
```json
{
  "widget": {
    "name": "param_annual_commit",
    "spec": {
      "widgetType": "filter-text-input",
      "encodings": {
        "fields": [{
          "displayName": "Annual Commit ($)",
          "fieldName": "annual_commit",
          "parameterKeyword": "annual_commit"
        }]
      }
    },
    "param_queries": [
      { "paramDatasetName": "ds_commit_status", "queryName": "param_commit" },
      { "paramDatasetName": "ds_variance", "queryName": "param_commit" }
    ]
  }
}
```

---

## 4. Lakehouse Monitoring Table Patterns (CRITICAL)

### Window Struct Access

Monitoring tables store time windows as a struct. Access fields correctly:

```sql
-- ‚ùå WRONG - column doesn't exist
SELECT window_start, window_end FROM monitoring_table

-- ‚úÖ CORRECT - access struct fields
SELECT window.start AS window_start, window.end AS window_end FROM monitoring_table
-- Or use directly in WHERE
WHERE window.start BETWEEN :time_range.min AND :time_range.max
```

### Generic Metric Schema

Custom metrics are stored generically. The `column_name` field identifies the metric:

| column_name | avg | count | p50 | p95 | avg_delta |
|-------------|-----|-------|-----|-----|-----------|
| success_rate | 0.95 | 1000 | 0.94 | 0.98 | 0.02 |
| total_runs | 500 | NULL | NULL | NULL | -10 |
| avg_duration | 120.5 | NULL | NULL | NULL | 5.2 |

### ‚ùå WRONG - Direct column selection
```sql
-- These columns DON'T EXIST as direct columns!
SELECT success_rate, total_runs, avg_duration FROM fact_job_run_timeline_profile_metrics
```

### ‚úÖ CORRECT - CASE pivot on column_name
```sql
SELECT 
  window.start AS window_start,
  MAX(CASE WHEN column_name = 'success_rate' THEN avg END) AS success_rate_pct,
  MAX(CASE WHEN column_name = 'total_runs' THEN count END) AS total_runs,
  MAX(CASE WHEN column_name = 'avg_duration' THEN avg END) AS avg_duration_sec
FROM ${catalog}.${gold_schema}_monitoring.fact_job_run_timeline_profile_metrics
WHERE window.start BETWEEN :time_range.min AND :time_range.max
GROUP BY window.start
ORDER BY window.start
```

### Standard Drift Metrics Pattern (for generic statistical columns)
```sql
-- Use CASE pivot for standard statistical drift (avg_delta on specific columns)
SELECT 
  window.start AS window_start,
  MAX(CASE WHEN column_name = 'column_a' THEN avg_delta END) AS column_a_drift
FROM ${catalog}.${gold_schema}_monitoring.{table}_drift_metrics
WHERE window.start BETWEEN :time_range.min AND :time_range.max
GROUP BY window.start
```

### ‚ö†Ô∏è CRITICAL: Custom Drift Metrics Pattern

**Custom drift metrics are stored as DIRECT COLUMNS, not in avg_delta!**

When you define custom drift metrics in monitoring (using `create_drift_metric`), they are stored as **named columns** in the `column_name = ':table'` row.

```sql
-- ‚úÖ CORRECT: Custom drift metrics are direct columns
SELECT 
  window.start AS window_start,
  success_rate_drift,        -- Direct column!
  cost_drift_pct,            -- Direct column!
  p95_duration_drift_pct     -- Direct column!
FROM ${catalog}.${gold_schema}_monitoring.fact_job_run_timeline_drift_metrics
WHERE column_name = ':table'
  AND drift_type = 'CONSECUTIVE'
  AND slice_key IS NULL
```

```sql
-- ‚ùå WRONG: Don't use avg_delta for custom drift metrics
SELECT COALESCE(avg_delta, 0) AS drift_pct  -- Returns 0!
FROM drift_metrics WHERE column_name = ':table'
```

**Available custom drift columns by domain:**
| Domain | Table | Drift Columns |
|--------|-------|---------------|
| Cost | `fact_usage_drift_metrics` | `cost_drift_pct`, `dbu_drift_pct`, `tag_coverage_drift` |
| Jobs | `fact_job_run_timeline_drift_metrics` | `success_rate_drift`, `duration_drift_pct`, `failure_count_drift` |
| Queries | `fact_query_history_drift_metrics` | `p95_duration_drift_pct`, `failure_rate_drift`, `query_volume_drift_pct` |
| Security | `fact_audit_logs_drift_metrics` | `event_volume_drift_pct`, `failure_rate_drift`, `sensitive_action_drift` |

### Monitoring Table Naming Convention
```
${catalog}.${gold_schema}_monitoring.{base_table}_profile_metrics
${catalog}.${gold_schema}_monitoring.{base_table}_drift_metrics
```

Examples:
- `fact_usage_profile_metrics` / `fact_usage_drift_metrics`
- `fact_query_history_profile_metrics` / `fact_query_history_drift_metrics`
- `fact_job_run_timeline_profile_metrics` / `fact_job_run_timeline_drift_metrics`

---

## 5. Pie Chart Configuration (CRITICAL)

### Required Scale Properties

Pie charts MUST have explicit `scale` properties or they won't render:

```json
{
  "spec": {
    "widgetType": "pie",
    "encodings": {
      "color": {
        "fieldName": "category",
        "displayName": "Category",
        "scale": { "type": "categorical" }  // REQUIRED!
      },
      "angle": {
        "fieldName": "value",
        "displayName": "Value",
        "scale": { "type": "quantitative" }  // REQUIRED!
      }
    }
  }
}
```

### ‚ùå WRONG - Missing scale
```json
"encodings": {
  "color": { "fieldName": "category" },
  "angle": { "fieldName": "value" }
}
```
**Result:** Empty pie chart

---

## 5.1 Bar Chart Configuration (CRITICAL)

### Required Scale Properties

Bar charts ALSO require explicit `scale` properties or they show "Select fields to visualize":

```json
{
  "spec": {
    "version": 3,
    "widgetType": "bar",
    "encodings": {
      "x": {
        "fieldName": "category_name",
        "displayName": "Category",
        "scale": { "type": "categorical" }  // REQUIRED!
      },
      "y": {
        "fieldName": "value",
        "displayName": "Value",
        "scale": { "type": "quantitative" }  // REQUIRED!
      }
    }
  }
}
```

### ‚ùå WRONG - Missing scale
```json
"encodings": {
  "x": { "fieldName": "category_name" },
  "y": { "fieldName": "value" }
}
```
**Result:** "Select fields to visualize" error

### Charts Requiring Scale Properties

| Widget Type | Encodings Requiring Scale |
|-------------|---------------------------|
| **pie** | `color.scale: categorical`, `angle.scale: quantitative` |
| **bar** | `x.scale: categorical`, `y.scale: quantitative` |
| **line** | `x.scale: temporal`, `y.scale: quantitative`, `color.scale: categorical` |
| **area** | `x.scale: temporal`, `y.scale: quantitative`, `color.scale: categorical` |

---

## 6. Table Widget Configuration

### Version 2 Structure
```json
{
  "spec": {
    "version": 2,
    "widgetType": "table",
    "encodings": {
      "columns": [
        { "fieldName": "job_name", "title": "Job Name", "type": "string" },
        { "fieldName": "total_cost", "title": "Total Cost", "type": "number" },
        { "fieldName": "runs", "title": "Runs", "type": "number" }
      ]
    },
    "frame": {
      "showTitle": true,
      "title": "Most Expensive Jobs"
    }
  }
}
```

### ‚ùå Invalid Properties (v1 only)
Remove these properties when using version 2:
```json
// REMOVE THESE:
"itemsPerPage": 50,
"condensed": true,
"withRowNumber": true
```

---

## 7. Multi-Series Line Charts

### Use UNION ALL for Multiple Series

```sql
SELECT 
  DATE(start_time) AS date,
  'Average' AS metric,
  AVG(duration_ms) / 60000 AS duration_min
FROM fact_query_history
WHERE start_time BETWEEN :time_range.min AND :time_range.max
GROUP BY 1

UNION ALL

SELECT 
  DATE(start_time) AS date,
  'P95 (Slow)' AS metric,
  PERCENTILE_APPROX(duration_ms, 0.95) / 60000 AS duration_min
FROM fact_query_history
WHERE start_time BETWEEN :time_range.min AND :time_range.max
GROUP BY 1

ORDER BY date, metric
```

### Widget Encoding for Multi-Series
```json
{
  "spec": {
    "widgetType": "line",
    "encodings": {
      "x": {
        "fieldName": "date",
        "scale": { "type": "temporal" }
      },
      "y": {
        "fieldName": "duration_min",
        "scale": { "type": "quantitative" }
      },
      "color": {
        "fieldName": "metric",
        "scale": { "type": "categorical" }
      }
    }
  }
}
```

---

## 8. Stacked Area Charts

### Configuration for Status Breakdown
```json
{
  "spec": {
    "widgetType": "area",
    "encodings": {
      "x": {
        "fieldName": "run_date",
        "scale": { "type": "temporal" }
      },
      "y": {
        "fieldName": "run_count",
        "scale": { "type": "quantitative" },
        "stack": "zero"  // CRITICAL for stacking
      },
      "color": {
        "fieldName": "status",
        "scale": {
          "type": "categorical",
          "domain": ["Success", "Failed"],
          "range": ["#00A972", "#FF3621"]  // Green, Red
        }
      }
    }
  }
}
```

### Query Pattern for Status Breakdown
```sql
SELECT 
  run_date,
  'Success' AS status,
  SUM(CASE WHEN result_state IN ('SUCCESS', 'SUCCEEDED') THEN 1 ELSE 0 END) AS run_count
FROM fact_job_run_timeline
WHERE run_date BETWEEN :time_range.min AND :time_range.max
GROUP BY run_date

UNION ALL

SELECT 
  run_date,
  'Failed' AS status,
  SUM(CASE WHEN result_state NOT IN ('SUCCESS', 'SUCCEEDED') THEN 1 ELSE 0 END) AS run_count
FROM fact_job_run_timeline
WHERE run_date BETWEEN :time_range.min AND :time_range.max
GROUP BY run_date

ORDER BY run_date, status
```

---

## 9. SQL Best Practices for Dashboards

### NULL Handling
```sql
-- Prevent empty results with COALESCE
SELECT COALESCE(SUM(cost), 0) AS total_cost

-- Prevent NULLs in grouping columns
SELECT COALESCE(sku_name, 'Unknown') AS sku_name

-- Prevent division by zero
SELECT ROUND(success / NULLIF(total, 0), 3) AS success_rate
```

### Distinct Counts
```sql
-- Use COUNT(DISTINCT) for unique users, not COUNT(*)
SELECT COUNT(DISTINCT user_identity_email) AS unique_users

-- Filter out internal/system users
WHERE user_identity_email NOT LIKE '%@databricks.com'
  AND user_identity_email NOT LIKE '%system%'
```

### Time Range Defaults

| Use Case | Default Range | Why |
|----------|---------------|-----|
| KPI Widgets | `now-7d/d` | Recent performance |
| Trend Charts | `now-30d/d` | Meaningful trends |
| Historical Analysis | `now-90d/d` | Long-term patterns |
| Cost Analysis | `now-30d/d` | Monthly budget cycles |

### ORDER BY with CASE - Avoid Mixed Types
```sql
-- ‚ùå WRONG - mixed types in CASE
ORDER BY CASE 
  WHEN severity = 'Critical' THEN 1
  WHEN severity = 'High' THEN 2
  ELSE 'Low'  -- STRING mixed with INT!
END

-- ‚úÖ CORRECT - consistent types
ORDER BY CASE 
  WHEN severity = 'Critical' THEN 1
  WHEN severity = 'High' THEN 2
  ELSE 3  -- INT
END
```

### Parentheses in OR Conditions
```sql
-- ‚ùå WRONG - OR without parentheses
WHERE workspace_id = :workspace AND action_name LIKE '%denied%' OR action_name LIKE '%error%'

-- ‚úÖ CORRECT - parenthesized OR
WHERE workspace_id = :workspace 
  AND (action_name LIKE '%denied%' OR action_name LIKE '%error%')
```

---

## 10. Schema Variable Substitution

### Standard Variables
```sql
-- Gold layer tables
FROM ${catalog}.${gold_schema}.fact_usage

-- Monitoring tables
FROM ${catalog}.${gold_schema}_monitoring.fact_usage_profile_metrics

-- ML/Feature tables
FROM ${catalog}.${feature_schema}.budget_forecaster_predictions
```

### ‚ùå NEVER Hardcode Schemas
```sql
-- WRONG
FROM prashanth_catalog.gold.fact_usage

-- CORRECT
FROM ${catalog}.${gold_schema}.fact_usage
```

---

## 11. Lineage Table Queries

### Capture Both Reads and Writes
```sql
-- fact_table_lineage has separate columns for source (read) and target (write)
SELECT 
  COALESCE(source_table_catalog, target_table_catalog) AS catalog_name,
  COUNT(DISTINCT COALESCE(source_table_full_name, target_table_full_name)) AS table_count
FROM ${catalog}.${gold_schema}.fact_table_lineage
WHERE event_date BETWEEN :time_range.min AND :time_range.max
GROUP BY 1
```

### Filter Value Queries
```sql
-- Get all catalogs (both source and target)
SELECT DISTINCT source_table_catalog AS catalog FROM fact_table_lineage WHERE source_table_catalog IS NOT NULL
UNION
SELECT DISTINCT target_table_catalog AS catalog FROM fact_table_lineage WHERE target_table_catalog IS NOT NULL
ORDER BY catalog
```

---

## 12. Composite Health Scores

### Multi-Domain Health Issues Pattern
```sql
-- Combine issues from all domains
SELECT 
  'Job' AS asset_type,
  COALESCE(j.name, CAST(f.job_id AS STRING)) AS asset_name,
  'Job Failed' AS issue_type,
  f.run_date AS issue_date
FROM ${catalog}.${gold_schema}.fact_job_run_timeline f
LEFT JOIN ${catalog}.${gold_schema}.dim_job j ON f.job_id = j.job_id
WHERE f.result_state NOT IN ('SUCCESS', 'SUCCEEDED')
  AND f.run_date BETWEEN :time_range.min AND :time_range.max

UNION ALL

SELECT 
  'Query' AS asset_type,
  statement_id AS asset_name,
  'Query Failed' AS issue_type,
  DATE(start_time) AS issue_date
FROM ${catalog}.${gold_schema}.fact_query_history
WHERE execution_status != 'FINISHED'
  AND start_time BETWEEN :time_range.min AND :time_range.max

UNION ALL

SELECT 
  'Security' AS asset_type,
  user_identity_email AS asset_name,
  'Access Denied' AS issue_type,
  DATE(event_time) AS issue_date
FROM ${catalog}.${gold_schema}.fact_audit_logs
WHERE is_failed_action = TRUE
  AND event_time BETWEEN :time_range.min AND :time_range.max

ORDER BY issue_date DESC
LIMIT 20
```

---

## 13. Global Filters

### Global Filters Page Structure
```json
{
  "name": "page_global_filters",
  "displayName": "Global Filters",
  "pageType": "PAGE_TYPE_GLOBAL_FILTERS",
  "layout": [
    {
      "widget": {
        "name": "filter_time_range",
        "spec": {
          "widgetType": "filter-date-range-picker"
        }
      },
      "position": { "x": 0, "y": 0, "width": 2, "height": 1 }
    },
    {
      "widget": {
        "name": "filter_workspace",
        "spec": {
          "widgetType": "filter-multi-select"
        }
      },
      "position": { "x": 2, "y": 0, "width": 2, "height": 1 }
    }
  ]
}
```

### Workspace Filter Dataset
```json
{
  "name": "ds_select_workspace",
  "query": "SELECT DISTINCT COALESCE(w.workspace_name, CAST(f.workspace_id AS STRING)) AS workspace_name FROM ${catalog}.${gold_schema}.fact_audit_logs f LEFT JOIN ${catalog}.${gold_schema}.dim_workspace w ON f.workspace_id = w.workspace_id WHERE f.event_time >= CURRENT_DATE() - INTERVAL 90 DAYS ORDER BY workspace_name"
}
```

---

## 14. Access Denial Detection

### Accurate Denial Query Pattern
```sql
-- Use is_failed_action and response columns for accurate detection
SELECT 
  DATE(event_time) AS event_date,
  user_identity_email AS user,
  action_name,
  response_error_message AS error_message,
  response_status_code AS status_code
FROM ${catalog}.${gold_schema}.fact_audit_logs
WHERE (is_failed_action = TRUE OR response_status_code >= 400)
  AND event_time BETWEEN :time_range.min AND :time_range.max
ORDER BY event_time DESC
```

### High-Risk Event Detection
```sql
SELECT COUNT(DISTINCT request_id) AS high_risk_events
FROM ${catalog}.${gold_schema}.fact_audit_logs
WHERE event_time BETWEEN :time_range.min AND :time_range.max
  AND (
    action_name LIKE '%delete%'
    OR action_name LIKE '%drop%'
    OR action_name LIKE '%destroy%'
    OR action_name LIKE '%terminate%'
    OR action_name LIKE '%removeUser%'
    OR action_name LIKE '%removeGroup%'
  )
```

---

## 15. Page Naming Conventions

### Intelligent Page Names

| Old Name | New Name | Why |
|----------|----------|-----|
| `üìä Lakehouse Monitoring` | `üìà Cost Metrics Overview` | Describes content |
| `Lakehouse Monitoring - fact_usage` | `üî¨ Advanced Cost Metrics` | Indicates detail level |
| `üìä Lakehouse Monitoring` | `üìà Job Metrics Overview` | Domain-specific |
| `Lakehouse Monitoring - fact_job_run_timeline` | `üî¨ Advanced Job Metrics` | Table reference |

---

## 16. Pre-Deployment SQL Validation (CRITICAL)

### Why Validate Before Deploying?

**Without validation:** Deploy ‚Üí Open dashboard ‚Üí See errors ‚Üí Fix ‚Üí Redeploy ‚Üí Repeat
- Each iteration takes **2-5 minutes** (deploy + load dashboard)
- Errors discovered one at a time

**With validation:** Validate locally ‚Üí See ALL errors ‚Üí Fix all ‚Üí Deploy once
- Validation takes **30-60 seconds**
- All errors discovered at once
- **90% reduction in dev loop time**

### The Validation Approach

Use `SELECT ... LIMIT 1` instead of `EXPLAIN` because:
- `EXPLAIN` only checks syntax, not runtime errors
- `SELECT LIMIT 1` catches:
  - Column doesn't exist (`UNRESOLVED_COLUMN`)
  - Table doesn't exist (`TABLE_OR_VIEW_NOT_FOUND`)
  - Parameter binding issues (`UNBOUND_SQL_PARAMETER`)
  - Type mismatches (`DATATYPE_MISMATCH`)
  - Invalid expressions

### SQL Validation Script Pattern

```python
# validate_dashboard_queries.py

import json
import re
from databricks.sdk import WorkspaceClient

def substitute_parameters(query: str, parameters: list) -> str:
    """Substitute dashboard parameters with test values."""
    result = query
    
    # Parameter substitution patterns
    substitutions = {
        # Time range parameters
        r':time_range\.min': "CURRENT_DATE() - INTERVAL 30 DAYS",
        r':time_range\.max': "CURRENT_DATE()",
        r':monitor_time_start': "CURRENT_DATE() - INTERVAL 30 DAYS",
        r':monitor_time_end': "CURRENT_DATE()",
        
        # Multi-select parameters (return ARRAY)
        r':param_workspace': "ARRAY('All')",
        r':param_catalog': "ARRAY('All')",
        r':param_sku': "ARRAY('All')",
        
        # Single-select parameters
        r':monitor_slice_key': "'No Slice'",
        r':monitor_slice_value': "'No Slice'",
        
        # Text input parameters
        r':annual_commit': "1000000",
    }
    
    for pattern, replacement in substitutions.items():
        result = re.sub(pattern, replacement, result)
    
    return result

def validate_query(spark, dataset_name: str, query: str, parameters: list) -> dict:
    """Validate a single query by executing SELECT LIMIT 1."""
    try:
        # Substitute parameters
        test_query = substitute_parameters(query, parameters)
        
        # Wrap in LIMIT 1 for efficiency
        validation_query = f"SELECT * FROM ({test_query}) LIMIT 1"
        
        # Execute
        spark.sql(validation_query)
        return {"dataset": dataset_name, "status": "OK", "error": None}
        
    except Exception as e:
        return {"dataset": dataset_name, "status": "ERROR", "error": str(e)}

def validate_dashboard(spark, dashboard_path: str) -> list:
    """Validate all queries in a dashboard JSON file."""
    with open(dashboard_path) as f:
        dashboard = json.load(f)
    
    results = []
    for dataset in dashboard.get("datasets", []):
        name = dataset["name"]
        query = dataset["query"]
        params = dataset.get("parameters", [])
        
        result = validate_query(spark, name, query, params)
        results.append(result)
        
        # Print progress
        status = "‚úÖ" if result["status"] == "OK" else "‚ùå"
        print(f"{status} {name}")
        if result["error"]:
            print(f"   Error: {result['error'][:200]}")
    
    return results

# Run validation
if __name__ == "__main__":
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
    
    dashboards = [
        "src/dashboards/cost.lvdash.json",
        "src/dashboards/reliability.lvdash.json",
        "src/dashboards/performance.lvdash.json",
        "src/dashboards/security.lvdash.json",
        "src/dashboards/quality.lvdash.json",
        "src/dashboards/unified.lvdash.json",
    ]
    
    all_errors = []
    for dashboard in dashboards:
        print(f"\n{'='*60}\nValidating: {dashboard}\n{'='*60}")
        results = validate_dashboard(spark, dashboard)
        errors = [r for r in results if r["status"] == "ERROR"]
        all_errors.extend(errors)
    
    # Summary
    print(f"\n{'='*60}\nSUMMARY\n{'='*60}")
    print(f"Total errors: {len(all_errors)}")
    
    if all_errors:
        for e in all_errors:
            print(f"‚ùå {e['dataset']}: {e['error'][:100]}")
        raise RuntimeError(f"Validation failed with {len(all_errors)} errors")
    else:
        print("‚úÖ All queries validated successfully!")
```

### Widget Encoding Validation Script

```python
# validate_widget_encodings.py

import json
import re

def extract_query_columns(query: str) -> set:
    """Extract column aliases from SELECT clause."""
    # Find SELECT ... FROM pattern
    select_match = re.search(r'SELECT\s+(.*?)\s+FROM', query, re.DOTALL | re.IGNORECASE)
    if not select_match:
        return set()
    
    select_clause = select_match.group(1)
    columns = set()
    
    # Match patterns: "expression AS alias" or "column_name"
    for match in re.finditer(r'(?:AS\s+)?([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:,|$)', select_clause):
        columns.add(match.group(1).lower())
    
    return columns

def extract_widget_fields(widget: dict) -> set:
    """Extract fieldName references from widget encodings."""
    fields = set()
    
    def walk(obj):
        if isinstance(obj, dict):
            if 'fieldName' in obj:
                fields.add(obj['fieldName'].lower())
            for v in obj.values():
                walk(v)
        elif isinstance(obj, list):
            for item in obj:
                walk(item)
    
    walk(widget.get('spec', {}).get('encodings', {}))
    return fields

def validate_alignment(dashboard_path: str) -> list:
    """Check widget fieldNames match dataset query aliases."""
    with open(dashboard_path) as f:
        dashboard = json.load(f)
    
    # Build dataset lookup
    datasets = {d['name']: d for d in dashboard.get('datasets', [])}
    
    issues = []
    for page in dashboard.get('pages', []):
        for layout_item in page.get('layout', []):
            widget = layout_item.get('widget', {})
            widget_name = widget.get('name', 'unknown')
            
            # Get linked datasets
            for query in widget.get('queries', []):
                dataset_name = query.get('name')
                if dataset_name not in datasets:
                    continue
                
                dataset = datasets[dataset_name]
                query_cols = extract_query_columns(dataset.get('query', ''))
                widget_fields = extract_widget_fields(widget)
                
                # Find mismatches
                missing = widget_fields - query_cols
                if missing:
                    issues.append({
                        'widget': widget_name,
                        'dataset': dataset_name,
                        'missing_columns': list(missing),
                        'available_columns': list(query_cols)
                    })
    
    return issues

# Run validation
if __name__ == "__main__":
    dashboards = ["src/dashboards/cost.lvdash.json", ...]
    
    all_issues = []
    for dashboard in dashboards:
        issues = validate_alignment(dashboard)
        all_issues.extend(issues)
    
    if all_issues:
        for issue in all_issues:
            print(f"‚ùå {issue['widget']}: Missing {issue['missing_columns']}")
            print(f"   Dataset {issue['dataset']} returns: {issue['available_columns']}")
        raise SystemExit(1)
    else:
        print("‚úÖ All widget encodings validated!")
```

### Development Workflow (Recommended)

```bash
# 1. Make changes to dashboard JSON files
vim src/dashboards/cost.lvdash.json

# 2. Run local widget encoding validation (fast, no Databricks connection)
python src/dashboards/validate_widget_encodings.py --check

# 3. Run SQL validation (requires Databricks connection)
cd src/dashboards && python validate_dashboard_queries.py

# 4. Check for regressions (compare with main branch)
git diff --stat origin/main -- src/dashboards/*.json

# 5. Deploy only after all validations pass
databricks bundle deploy -t dev --force

# 6. Run dashboard deployment job
databricks bundle run -t dev dashboard_deployment_job
```

### Anti-Regression Checks

**Before EVERY deployment, verify:**

```bash
# 1. Widget-query alignment (catches "no fields to visualize")
python src/dashboards/validate_widget_encodings.py --check

# 2. SQL syntax and runtime errors (catches column/table not found)
python src/dashboards/validate_dashboard_queries.py

# 3. Git diff to see what changed (catches accidental deletions)
git diff --name-status HEAD~1 -- src/dashboards/

# 4. If changes look wrong, review the diff
git diff HEAD~1 -- src/dashboards/cost.lvdash.json
```

### Common Validation Errors and Fixes

| Error Pattern | Cause | Fix |
|---------------|-------|-----|
| `UNRESOLVED_COLUMN: X cannot be resolved` | Column doesn't exist | Check table schema, add correct column alias |
| `UNBOUND_SQL_PARAMETER: param_name` | Missing parameter definition | Add parameter to dataset's `parameters` array |
| `TABLE_OR_VIEW_NOT_FOUND: X` | Table/view doesn't exist | Check schema path, verify table exists |
| `DATATYPE_MISMATCH` | Type conversion error | Check column types, use CAST if needed |
| `Widget expects X, query returns Y` | Column alias mismatch | Add `AS X` alias to match widget |

### Parameter Substitution Reference

| Parameter Pattern | Test Value | Notes |
|-------------------|------------|-------|
| `:time_range.min` | `CURRENT_DATE() - INTERVAL 30 DAYS` | Start of time range |
| `:time_range.max` | `CURRENT_DATE()` | End of time range |
| `:param_workspace` | `ARRAY('All')` | Multi-select with 'All' option |
| `:param_catalog` | `ARRAY('All')` | Multi-select with 'All' option |
| `:monitor_slice_key` | `'No Slice'` | Monitoring slice filter |
| `:annual_commit` | `1000000` | Numeric text input |

---

## 17. Validation Checklist

### Pre-Deployment Checklist

- [ ] **SQL Validation passed** - All queries return results without error
- [ ] **Widget Encoding Validation passed** - All fieldNames match query aliases
- [ ] **Git diff reviewed** - No accidental deletions or regressions
- [ ] **Local testing done** - Validated in dev environment

### Widget-Query Alignment

For each widget:
- [ ] Widget `fieldName` matches query output alias exactly
- [ ] Format type matches expected value type (raw numbers for percent/currency)
- [ ] All parameters are defined in dataset's `parameters` array
- [ ] Time range uses correct access pattern (`:time_range.min`, `:time_range.max`)

### Monitoring Query Validation

- [ ] Uses `window.start` not `window_start`
- [ ] Custom AGGREGATE/DERIVED metrics selected as direct columns
- [ ] Schema is `${catalog}.${gold_schema}_monitoring`
- [ ] Includes `WHERE column_name = ':table' AND slice_key IS NULL`

### Pie Chart Validation

- [ ] `color` encoding has `scale: { "type": "categorical" }`
- [ ] `angle` encoding has `scale: { "type": "quantitative" }`

### Table Widget Validation

- [ ] Uses `version: 2`
- [ ] No invalid properties (`itemsPerPage`, `condensed`, `withRowNumber`)
- [ ] Has proper `frame` object

---

## 18. Common Error Messages and Fixes

| Error Message | Cause | Fix |
|---------------|-------|-----|
| "no fields to visualize" | Widget `fieldName` doesn't match query alias | Align column aliases |
| "UNRESOLVED_COLUMN" | Column doesn't exist in table | Check schema, use correct column |
| "UNBOUND_SQL_PARAMETER" | Parameter not defined in dataset | Add to `parameters` array |
| "Unable to render visualization" | Pie chart missing scale | Add `scale` to encodings |
| Empty chart with data | Wrong number format | Return raw numbers, not strings |
| 0% or 0 in counter | Formatted string or wrong percentage | Return 0-1 decimal for percent |
| "Invalid spec version" | Wrong version for widget type | KPI=2, Chart=3, Table=2, Filter=2 |

---

## 19. Dashboard File Structure

```
src/dashboards/
‚îú‚îÄ‚îÄ deploy_dashboards.py       # Deployment script
‚îú‚îÄ‚îÄ validate_dashboard_queries.py  # SQL validation
‚îú‚îÄ‚îÄ validate_widget_encodings.py   # Widget-query alignment
‚îú‚îÄ‚îÄ cost.lvdash.json           # Cost & Commitment dashboard
‚îú‚îÄ‚îÄ performance.lvdash.json    # Performance dashboard
‚îú‚îÄ‚îÄ reliability.lvdash.json    # Reliability dashboard
‚îú‚îÄ‚îÄ security.lvdash.json       # Security dashboard
‚îú‚îÄ‚îÄ quality.lvdash.json        # Quality & Governance dashboard
‚îî‚îÄ‚îÄ unified.lvdash.json        # Executive summary

docs/dashboards/
‚îî‚îÄ‚îÄ dashboard-changelog.md     # Track all changes
```

---

## 20. Widget Version Reference

| Widget Type | Version | Notes |
|-------------|---------|-------|
| Counter (KPI) | 2 | Do NOT include `period` |
| Bar Chart | 3 | |
| Line Chart | 3 | |
| Area Chart | 3 | Use `stack: "zero"` for stacked |
| Pie Chart | 3 | MUST have `scale` on both encodings |
| Table | 2 | Remove v1 properties |
| Filter (date range) | 2 | |
| Filter (single/multi select) | 2 | |
| Filter (text input) | 2 | |

---

## 21. References

### Official Documentation
- [AI/BI Lakeview Dashboards](https://docs.databricks.com/dashboards/lakeview/)
- [System Tables Overview](https://docs.databricks.com/aws/en/admin/system-tables/)
- [Lakehouse Monitoring](https://docs.databricks.com/lakehouse-monitoring/)

### Project Documentation
- [Dashboard Changelog](../../docs/dashboards/dashboard-changelog.md)
- [AI/BI Dashboard Prompt](../../context/prompts/monitoring/10-aibi-dashboards-prompt.md)

### Related Rules
- [Lakehouse Monitoring Comprehensive](17-lakehouse-monitoring-comprehensive.mdc)
- [SQL Alerting Patterns](19-sql-alerting-patterns.mdc)

---

## 22. Dashboard Deployment Patterns (PRODUCTION)

### No Hardcoding - Always Use Variable Substitution

**Critical Rule:** NEVER hardcode catalog, schema, or warehouse values in dashboard JSON.

#### ‚úÖ CORRECT: Variable Placeholders
```sql
-- In dashboard JSON
FROM ${catalog}.${gold_schema}.fact_usage
FROM ${catalog}.${feature_schema}.ml_predictions
WHERE warehouse_id = '${warehouse_id}'
```

#### ‚ùå WRONG: Hardcoded Values
```sql
-- DON'T DO THIS!
FROM prashanth_catalog.gold.fact_usage
FROM dev_user_system_gold_ml.predictions
```

### Variable Substitution Reference

| Variable | Purpose | Example Value |
|---|---|---|
| `${catalog}` | Unity Catalog name | `health_monitor` |
| `${gold_schema}` | Gold layer schema | `system_gold` |
| `${feature_schema}` | ML/Feature schema | `system_gold_ml` |
| `${warehouse_id}` | SQL Warehouse ID | `abc123xyz` |

### Python Variable Substitution Pattern

```python
def substitute_variables(json_str: str, catalog: str, gold_schema: str, warehouse_id: str) -> str:
    """Replace variable placeholders with actual values."""
    json_str = json_str.replace('${catalog}', catalog)
    json_str = json_str.replace('${gold_schema}', gold_schema)
    json_str = json_str.replace('${warehouse_id}', warehouse_id)
    
    # ML/Feature schema - derives from gold_schema pattern
    # dev_user_system_gold -> dev_user_system_gold_ml
    feature_schema = gold_schema.replace('_system_gold', '_system_gold_ml')
    json_str = json_str.replace('${feature_schema}', feature_schema)
    
    return json_str
```

### Widget Parameters with dbutils.widgets

```python
# Always use widgets for deployment parameters
dbutils.widgets.text("catalog", "health_monitor", "Target Catalog")
dbutils.widgets.text("gold_schema", "gold", "Gold Schema")
dbutils.widgets.text("warehouse_id", "", "SQL Warehouse ID")
dbutils.widgets.text("dashboard_folder", "/Shared/health_monitor/dashboards", "Dashboard Folder")

catalog = dbutils.widgets.get("catalog")
gold_schema = dbutils.widgets.get("gold_schema")
warehouse_id = dbutils.widgets.get("warehouse_id")
dashboard_folder = dbutils.widgets.get("dashboard_folder")
```

**Benefits:**
- ‚úÖ Same dashboard JSON works in dev, staging, prod
- ‚úÖ No manual find-replace before deployment
- ‚úÖ Parameters passed from Asset Bundle job
- ‚úÖ Easy to test with different catalogs/schemas

---

## 23. UPDATE-or-CREATE Deployment Pattern (RECOMMENDED)

### Why This Pattern?

**Problem with CREATE-only:**
- Creates new dashboard each time
- Breaks URLs and permissions
- Clutters workspace with duplicates

**Problem with UPDATE-only:**
- Fails if dashboard doesn't exist
- Requires manual creation first

**Solution: Workspace Import API with overwrite=true**
- If dashboard exists ‚Üí UPDATE (preserves URL, permissions)
- If dashboard doesn't exist ‚Üí CREATE
- Single code path for both scenarios

### Implementation Pattern

```python
def deploy_dashboard_via_workspace_import(
    workspace_client, 
    dashboard_config: dict, 
    display_name: str, 
    parent_path: str, 
    source_filename: str
) -> tuple:
    """
    Deploy dashboard using Workspace Import API (Microsoft recommended).
    
    Reference: https://learn.microsoft.com/en-us/azure/databricks/dashboards/tutorials/workspace-dashboard-api
    
    Returns: (dashboard_id, "CREATED" or "UPDATED")
    """
    import base64
    
    # Set display name
    dashboard_config['displayName'] = display_name
    
    # Serialize and encode
    serialized_dashboard = json.dumps(dashboard_config)
    content_base64 = base64.b64encode(serialized_dashboard.encode('utf-8')).decode('utf-8')
    
    # Use source filename directly (clean paths)
    file_path = f"{parent_path}/{source_filename}"  # e.g., cost.lvdash.json
    
    # Check if exists
    existing = False
    try:
        result = workspace_client.api_client.do(
            method="GET",
            path="/api/2.0/workspace/get-status",
            query={"path": file_path}
        )
        existing = result.get("object_type") == "DASHBOARD"
    except:
        pass
    
    # Import with overwrite (UPDATE if exists, CREATE if new)
    workspace_client.api_client.do(
        method="POST",
        path="/api/2.0/workspace/import",
        body={
            "path": file_path,
            "content": content_base64,
            "format": "AUTO",
            "overwrite": True  # ‚úÖ KEY: Enables UPDATE-or-CREATE
        }
    )
    
    action = "UPDATED" if existing else "CREATED"
    return file_path, action
```

### Cleanup Pattern - Prevent Duplicates

```python
def cleanup_old_dashboards(workspace_client, folder_path: str, keep_filenames: list):
    """
    Delete old dashboards NOT in keep_filenames list.
    Prevents duplicates when changing naming conventions.
    """
    items = workspace_client.workspace.list(folder_path)
    
    for item in items:
        item_name = item.path.split('/')[-1]
        
        if item_name.endswith('.lvdash.json') and item_name not in keep_filenames:
            workspace_client.workspace.delete(item.path)
            print(f"  üóëÔ∏è Deleted old dashboard: {item_name}")
```

---

## 24. Professional Naming Conventions

### File Naming Pattern

**Use simple, descriptive names - NOT branded prefixes**

| ‚úÖ GOOD | ‚ùå BAD |
|---------|--------|
| `cost.lvdash.json` | `Health_Monitor_Cost_and_Commitment.lvdash.json` |
| `performance.lvdash.json` | `DBX_Performance_Dashboard_v2.lvdash.json` |
| `reliability.lvdash.json` | `[Health Monitor] Reliability 2026.lvdash.json` |

**Why simple names:**
- Clean workspace paths: `/Shared/dashboards/cost.lvdash.json`
- Easy to reference in code: `dashboard_dir / "cost.lvdash.json"`
- Git-friendly (no spaces, no special characters)
- Professional appearance in file listings

### Display Name Pattern

**Use branded, user-friendly display names**

```python
DASHBOARDS = [
    ("cost.lvdash.json", "Databricks Cost Intelligence"),
    ("performance.lvdash.json", "Databricks Performance Analytics"),
    ("reliability.lvdash.json", "Databricks Reliability Monitor"),
    ("security.lvdash.json", "Databricks Security & Compliance"),
    ("quality.lvdash.json", "Databricks Data Quality Hub"),
    ("unified.lvdash.json", "Databricks Health Monitor"),
]
```

**Pattern:**
- `Databricks {Domain} {Type}`
- Examples: "Databricks Cost Intelligence", "Databricks Security & Compliance"
- Consistent branding across all dashboards
- Domain-specific keywords for discoverability

---

## 25. Pre-Deployment Validation Strategy (90% Time Reduction)

### The Problem with Deploy-First

**Without validation:**
1. Make change in JSON
2. Deploy (2-5 minutes)
3. Open dashboard
4. See error
5. Fix error
6. Redeploy (2-5 minutes)
7. Repeat 10+ times

**Total time:** 20-50 minutes per development cycle

### The Solution: Validate First

**With validation:**
1. Make changes in JSON
2. Run validator (30-60 seconds)
3. See ALL errors at once
4. Fix all errors
5. Deploy once (2-5 minutes)

**Total time:** 3-7 minutes per development cycle
**Savings:** 90% reduction in dev loop time

### SELECT LIMIT 1 vs EXPLAIN

**Why SELECT LIMIT 1?**

| Error Type | EXPLAIN Catches? | SELECT LIMIT 1 Catches? |
|---|---|---|
| Syntax errors | ‚úÖ Yes | ‚úÖ Yes |
| Column doesn't exist | ‚ùå Sometimes | ‚úÖ Always |
| Table doesn't exist | ‚úÖ Yes | ‚úÖ Yes |
| Type mismatches | ‚ùå No | ‚úÖ Yes |
| Ambiguous references | ‚ùå No | ‚úÖ Yes |
| Runtime errors | ‚ùå No | ‚úÖ Yes |

**Pattern:**
```python
def validate_query(spark, query: str) -> bool:
    """Validate query by executing with LIMIT 1."""
    try:
        # Wrap query in LIMIT 1 for efficiency
        validation_query = f"SELECT * FROM ({query}) LIMIT 1"
        spark.sql(validation_query).collect()
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False
```

### Error Categorization Pattern

```python
def categorize_error(error_str: str) -> dict:
    """Extract structured info from SQL errors."""
    result = {'error_type': 'OTHER'}
    
    if 'UNRESOLVED_COLUMN' in error_str:
        result['error_type'] = 'COLUMN_NOT_FOUND'
        # Extract column name
        match = re.search(r"name `([^`]+)`", error_str)
        if match:
            result['column'] = match.group(1)
        # Extract suggestions
        match = re.search(r"Did you mean one of the following\? \[([^\]]+)\]", error_str)
        if match:
            result['suggestions'] = match.group(1)
    
    elif 'AMBIGUOUS_REFERENCE' in error_str:
        result['error_type'] = 'AMBIGUOUS_COLUMN'
        match = re.search(r"Reference `([^`]+)`", error_str)
        if match:
            result['column'] = match.group(1)
    
    elif 'TABLE_OR_VIEW_NOT_FOUND' in error_str:
        result['error_type'] = 'TABLE_NOT_FOUND'
        match = re.search(r"table or view `([^`]+)`", error_str)
        if match:
            result['table'] = match.group(1)
    
    return result
```

---

## 26. Avoiding Whack-a-Mole: Root Cause Analysis Pattern

### Today's Example: workspace_owner Error

**What Happened:**
- Dashboard showed: `UNRESOLVED_COLUMN: w.workspace_owner cannot be resolved`
- Error suggested: `Did you mean one of [w.workspace_name, w.workspace_id, ...]`

**‚ùå WRONG Response (Whack-a-Mole):**
```python
# Just change the query blindly
COALESCE(w.workspace_owner, 'Unknown') AS asset_owner
# becomes
COALESCE(w.workspace_name, 'Unknown') AS asset_owner  # May not be correct!
```

**‚úÖ CORRECT Response (Root Cause Analysis):**
1. **Check actual schema:** What columns DOES `dim_workspace` have?
2. **Understand intent:** What was `workspace_owner` supposed to represent?
3. **Targeted fix:** Remove non-existent column reference
4. **Verify no side effects:** Check if column used elsewhere

**Solution:**
```python
# After checking schema, dim_workspace has:
# - workspace_id
# - workspace_name
# - workspace_url
# - create_time
# (NO workspace_owner column)

# Fixed: Remove reference to non-existent column
'Unknown' AS asset_owner  # Cost issues don't have a single owner
COALESCE(w.workspace_name, c.workspace_id) AS workspace_name  # This is valid
```

### Root Cause Checklist

Before fixing ANY dashboard error:
- [ ] Reproduce the error (don't rely on screenshot alone)
- [ ] Check actual table schema (DESCRIBE TABLE or Gold YAML)
- [ ] Understand what the query intends to do
- [ ] Identify the root cause (column doesn't exist vs wrong alias vs wrong table)
- [ ] Fix ONLY the root cause
- [ ] Verify the fix doesn't break other widgets/queries
- [ ] Run full validation to catch any new issues

---

## 27. Production Dashboard Organization

### File Structure (Implemented)

```
src/dashboards/
‚îú‚îÄ‚îÄ deploy_dashboards.py              # Deployment script (UPDATE-or-CREATE)
‚îú‚îÄ‚îÄ validate_dashboard_queries.py     # SQL validation (SELECT LIMIT 1)
‚îú‚îÄ‚îÄ validate_widget_encodings.py      # Widget-query alignment
‚îÇ
‚îú‚îÄ‚îÄ cost.lvdash.json                  # 253 KB, 7175 lines
‚îú‚îÄ‚îÄ performance.lvdash.json           # 320 KB, 8576 lines
‚îú‚îÄ‚îÄ reliability.lvdash.json           # 234 KB, 6596 lines
‚îú‚îÄ‚îÄ security.lvdash.json              # 146 KB, 4153 lines
‚îú‚îÄ‚îÄ quality.lvdash.json               # 94 KB, 2855 lines
‚îî‚îÄ‚îÄ unified.lvdash.json               # 212 KB, 6151 lines
```

### Dashboard Inventory

| Dashboard | File Size | Datasets | Pages | Purpose |
|---|---|---|---|---|
| Cost Intelligence | 253 KB | ~30 | 5 | Cost tracking, commit tracking, DBU usage |
| Performance Analytics | 320 KB | ~40 | 6 | Query performance, warehouse utilization |
| Reliability Monitor | 234 KB | ~35 | 5 | Job success rates, failure analysis |
| Security & Compliance | 146 KB | ~25 | 4 | Audit logs, access denials, compliance |
| Data Quality Hub | 94 KB | ~20 | 3 | Lineage, data quality metrics |
| Health Monitor (Unified) | 212 KB | ~35 | 7 | Executive summary across all domains |

---

## Version History

- **v4.0** (January 2026) - Production deployment patterns from actual implementation
  - **NEW Section 22:** No hardcoding - variable substitution patterns
  - **NEW Section 23:** UPDATE-or-CREATE deployment pattern
  - **NEW Section 24:** Professional naming conventions
  - **NEW Section 25:** Pre-deployment validation strategy (90% time reduction)
  - **NEW Section 26:** Avoiding whack-a-mole with root cause analysis
  - **NEW Section 27:** Production dashboard organization
  - Added actual deployment script patterns (Workspace Import API)
  - Added cleanup patterns for duplicate dashboards
  - Added SELECT LIMIT 1 validation rationale
  - Added error categorization patterns
  - **Key insight:** Variable substitution + validation + UPDATE-or-CREATE = production-grade deployment
  - **Real example:** workspace_owner error fix demonstrates root cause analysis over blind fixes

- **v3.1** (January 2026) - Added pre-deployment SQL validation patterns
  - **NEW Section 16:** Comprehensive SQL validation guide
  - Added `SELECT LIMIT 1` vs `EXPLAIN` comparison
  - Added parameter substitution reference table
  - Added widget encoding validation script pattern
  - Added development workflow (validation ‚Üí diff ‚Üí deploy)
  - Added anti-regression checks guidance
  - Expanded sections from 20 to 21 total
  - **Key insight:** Pre-deployment validation reduces dev loop time by 90%

- **v3.0** (January 2026) - Major enhancement based on 100+ production fixes
  - Added 20 critical pattern sections
  - Documented monitoring table CASE pivot pattern
  - Added pie chart scale requirements
  - Added multi-series chart patterns
  - Added stacked area chart patterns
  - Added widget-query validation checklist
  - Added error message troubleshooting table
  - Added page naming conventions
  - Reorganized for faster reference

- **v2.0** (December 2025) - Initial production patterns
  - Basic grid layout (6-column)
  - Widget version specifications
  - Parameter configuration

- **v1.0** (October 2025) - Initial rule creation
