---
description: MLflow and ML Model patterns for Databricks - experiment creation, model training, batch inference, and Unity Catalog integration
globs: src/wanderbricks_ml/**/*.py, src/ml/**/*.py, resources/ml/*.yml
alwaysApply: false
---
# MLflow and ML Model Patterns for Databricks

## Pattern Origin

**Date:** December 2025 (Updated: January 14, 2026 - v4.0)  
**Source:** Production ML pipeline implementation with 25 models across 5 domains (Cost, Security, Performance, Reliability, Quality)  
**Impact:** 96% inference success rate (23/24 models), 93% reduction in debugging time  
**Key Artifacts:**
- Feature Registry pattern for dynamic schema querying
- Standardized training utilities (`training_base.py`)
- Custom inference pattern for TF-IDF models (`tag_recommender`)
- NaN/Inf handling at feature table source

**Reference:** [Post-Mortem Documentation](../../docs/reference/rule-improvement-mlflow-mlmodels-patterns.md)

---

## Quick Reference: 15 Non-Negotiable Rules

| # | Rule | Pattern | Why It Fails Otherwise |
|---|------|---------|----------------------|
| 1 | **Experiment Path** | `/Shared/{project}_ml_{model_name}` | `/Users/...` fails silently if subfolder doesn't exist |
| 2 | **Dataset Logging** | Inside `mlflow.start_run()` context | Won't associate with run, invisible in UI |
| 3 | **Exit Signal** | `dbutils.notebook.exit("SUCCESS")` | Job status unclear, may show SUCCESS on failure |
| 4 | **UC Model Logging** | Use `fe.log_model()` with `output_schema` | Unity Catalog rejects models without output spec |
| 5 | **Feature Store** | Use `FeatureEngineeringClient` for feature tables | Manual joins lose lineage, inference breaks |
| 6 | **Pin MLflow Version** | `mlflow==3.7.0` (exact version) | Version mismatch warnings during inference |
| 7 | **NaN Handling** | Clean NaN/Inf at **feature table creation** | sklearn GradientBoosting fails; XGBoost handles NaN but sklearn doesn't |
| 8 | **Label Binarization** | Convert 0-1 rates to binary for classifiers | `XGBoostError: base_score must be in (0,1)` |
| 9 | **Single-Class Check** | Verify label distribution before training | Classifier can't train on all-same labels |
| 10 | **Exclude Labels** | Use `exclude_columns=[LABEL_COLUMN]` | Label included as feature causes inference failure |
| 11 | **Feature Registry** | Query feature table schemas dynamically | Hardcoded feature lists drift out of sync |
| 12 | **Standardized Utils** | Import from `training_base.py` | Custom implementations have inconsistent patterns |
| 13 | **Double Type Casting** | Cast ALL numeric features to DOUBLE in feature tables | MLflow signatures reject DecimalType |
| 14 | **Custom Inference** | Separate task for TF-IDF/NLP models | `fe.score_batch()` can't compute runtime features |
| 15 | **Bundle Path Setup** | Use `sys.path.insert(0, _bundle_root)` pattern | Module imports fail in serverless |

---

## ⚠️ Critical Rules (Non-Negotiable)

### 0. Pin Package Versions Across All ML Pipelines

```yaml
# ✅ CORRECT: Pin MLflow to SAME version in training AND inference
# resources/ml/ml_training_pipeline.yml
environments:
  - environment_key: default
    spec:
      environment_version: "4"
      dependencies:
        - "mlflow==3.7.0"        # Pin exact version
        - "scikit-learn>=1.3.0"
        - "xgboost>=2.0.0"

# resources/ml/ml_inference_pipeline.yml
environments:
  - environment_key: ml_inference
    spec:
      environment_version: "4"
      dependencies:
        - "mlflow==3.7.0"        # MUST match training version
        - "scikit-learn>=1.3.0"
        - "xgboost>=2.0.0"       # Required for XGBoost models
        - "databricks-feature-engineering"
```

```yaml
# ❌ WRONG: Loose version constraints cause mismatches
dependencies:
  - "mlflow>=3.0.0"  # Training may get 3.7.0, inference may get 3.8.1
```

**Warnings you'll see if versions mismatch:**
```
WARNING mlflow.utils.requirements_utils: Detected one or more mismatches 
between the model's dependencies and the current Python environment:
 - mlflow (current: 3.8.1, required: mlflow==3.7.0)

WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager="local"` 
does not recreate the same environment that was used during training
```

**Why pinning matters:**
- Models are serialized with specific MLflow version metadata
- Inference deserialization must match training serialization
- `fe.score_batch()` uses `spark_udf()` internally, which checks versions
- Serverless environments can't use `env_manager="conda"` (too slow/unsupported)

### 1. Experiment Path: ALWAYS Use `/Shared/`

```python
# ✅ CORRECT: /Shared/ path always works
experiment_name = f"/Shared/wanderbricks_ml_{model_name}"
mlflow.set_experiment(experiment_name)

# ❌ WRONG: User path requires parent folder to exist (will fail silently)
experiment_name = f"/Users/{user}/wanderbricks_ml/{model_name}"

# ❌ WRONG: Using spark to get username (may fail in serverless)
user = spark.sql("SELECT current_user()").collect()[0][0]

# ❌ WRONG: Asset Bundle experiment definitions create duplicates with [dev username] prefix
# Never define experiments in resources/ml/ml_experiments.yml
```

**Root Cause:** `/Users/{user}/subfolder/` fails silently if `subfolder` doesn't exist. Falls back to notebook's default "train" experiment without warning. Asset Bundles add `[dev username]` prefix to experiments in dev mode.

**What we tried (all failed):**
1. Dynamic user path → Parent directory doesn't exist
2. Asset Bundle experiments → Duplicates with dev prefix
3. UC Volume artifact_location → Volume created but experiments failed
4. `/Users/{user}/...` → Silent failure, falls back to "train"

**What worked:** `/Shared/wanderbricks_ml_{model_name}` - always exists, no parent folder issues.

### 2. Dataset Logging: MUST Be Inside Run Context

```python
# ✅ CORRECT: Inside mlflow.start_run()
with mlflow.start_run(run_name=run_name) as run:
    # Dataset logging MUST be here
    training_df = spark.table(f"{catalog}.{schema}.{table_name}")
    dataset = mlflow.data.from_spark(
        df=training_df, 
        table_name=f"{catalog}.{schema}.{table_name}",
        version="latest"
    )
    mlflow.log_input(dataset, context="training")
    print(f"✓ Dataset logged: {table_name}")
    
    # Then log params, metrics, model...
    mlflow.log_params({...})
    mlflow.log_metrics({...})
    mlflow.xgboost.log_model(...)

# ❌ WRONG: Outside run context - won't associate with run
dataset = mlflow.data.from_spark(...)
mlflow.log_input(dataset)  # Dataset won't appear in UI!
with mlflow.start_run(run_name=run_name) as run:
    # Dataset logged above won't show in this run!
```

**Root Cause:** MLflow associates artifacts with the *active* run. If called outside `start_run()`, there's no run to associate with.

### 3. Helper Functions: ALWAYS Inline

```python
# ✅ CORRECT: Inline helpers in each notebook
def setup_mlflow_experiment(model_name: str) -> str:
    """
    Inlined helper - module imports don't work in Asset Bundle notebooks.
    
    CRITICAL: Do not move this to a shared module!
    """
    experiment_name = f"/Shared/wanderbricks_ml_{model_name}"
    try:
        experiment = mlflow.set_experiment(experiment_name)
        print(f"✓ Experiment: {experiment_name}")
        return experiment_name
    except Exception as e:
        print(f"❌ Experiment setup failed: {e}")
        return None

# ❌ WRONG: Import from shared module - causes ModuleNotFoundError
from wanderbricks_ml.utils.mlflow_setup import setup_mlflow_experiment

# ❌ WRONG: Import from relative path
from ..utils.mlflow_helpers import setup_mlflow_experiment

# ❌ WRONG: Use %run (doesn't work in deployed notebooks)
%run ./utils/mlflow_setup
```

**Root Cause:** Asset Bundle notebooks can't reliably import from local modules due to path resolution issues in serverless environments. Even pure Python files fail to import in this context.

**What we tried:**
1. Created `src/wanderbricks_ml/utils/mlflow_setup.py` → `ModuleNotFoundError`
2. Created `src/wanderbricks_ml/utils/mlflow_helpers.py` → Same error
3. Tried relative imports → Failed
4. Tried sys.path manipulation → Failed

**What worked:** Copy helper functions into each training script (code duplication is acceptable here).

### 4. Notebook Exit: ALWAYS Signal Success

```python
def main():
    try:
        # ... training code ...
        
        print("✓ Training completed successfully!")
        
    except Exception as e:
        print(f"❌ Error: {str(e)}")
        import traceback
        print(traceback.format_exc())
        # ✅ REQUIRED: Signal failure with message
        dbutils.notebook.exit(f"FAILED: {str(e)}")
    
    # ✅ REQUIRED: Signal success to Databricks
    dbutils.notebook.exit("SUCCESS")

# ❌ WRONG: No exit signal - job may show SUCCESS even on failure
def main():
    # ... training code ...
    print("Done!")  # Job status unclear - no dbutils.notebook.exit()
    
# ❌ WRONG: spark.stop() prevents exit signal
def main():
    try:
        # ... training code ...
    finally:
        spark.stop()  # ❌ This will prevent dbutils.notebook.exit() from working!
```

**Root Cause:** Databricks jobs need explicit exit signals. Without `dbutils.notebook.exit()`, the job may show SUCCESS even when code throws exceptions or fails silently.

### 5. Unity Catalog Model Logging with Feature Engineering Client

**⚠️ CRITICAL: When using Feature Store, use `fe.log_model()` NOT direct MLflow logging**

The `fe.log_model()` method embeds feature lookup metadata for automatic inference. Unity Catalog requires BOTH input and output schema specifications.

#### Pattern by Model Type

```python
from databricks.feature_engineering import FeatureEngineeringClient
from mlflow.types import ColSpec, DataType, Schema
import mlflow

fe = FeatureEngineeringClient()
mlflow.set_registry_uri("databricks-uc")

# =============================================================================
# PATTERN 1: REGRESSION MODELS (with labels, continuous output)
# =============================================================================
# Example: Budget Forecaster, Chargeback Attribution, Revenue Predictor
# Output: double (floating point prediction)

output_schema = Schema([ColSpec(DataType.double)])

fe.log_model(
    model=model,
    artifact_path="model",
    flavor=mlflow.sklearn,  # REQUIRED: specify the ML flavor
    training_set=training_set,
    registered_model_name=f"{catalog}.{schema}.{model_name}",
    infer_input_example=True,  # Automatically infers input schema
    output_schema=output_schema  # Explicitly define output type
)

# =============================================================================
# PATTERN 2: CLASSIFICATION MODELS (with labels, discrete output)
# =============================================================================
# Example: Failure Predictor, Commitment Recommender, Threat Detector
# Output: long (integer class label: 0, 1, 2, etc.)

output_schema = Schema([ColSpec(DataType.long)])

fe.log_model(
    model=model,
    artifact_path="model",
    flavor=mlflow.sklearn,
    training_set=training_set,
    registered_model_name=f"{catalog}.{schema}.{model_name}",
    infer_input_example=True,
    output_schema=output_schema
)

# =============================================================================
# PATTERN 3: ANOMALY DETECTION MODELS (NO labels - label=None in training_set)
# =============================================================================
# Example: Isolation Forest, One-Class SVM, Drift Detector
# Output: long (anomaly indicator: -1 for anomaly, 1 for normal)
# 
# ⚠️ CRITICAL: Models trained WITHOUT labels MUST have output_schema!
# The infer_input_example=True alone is NOT sufficient for unsupervised models.

output_schema = Schema([ColSpec(DataType.long)])  # -1 or 1 for Isolation Forest

fe.log_model(
    model=model,
    artifact_path="model",
    flavor=mlflow.sklearn,
    training_set=training_set,  # This has label=None for anomaly detection
    registered_model_name=f"{catalog}.{schema}.{model_name}",
    infer_input_example=True,
    output_schema=output_schema  # REQUIRED for unsupervised models!
)
```

#### ❌ Common Mistakes That Cause Signature Errors

```python
# =============================================================================
# ERROR: "Model passed for registration contained a signature that includes only inputs"
# =============================================================================

# ❌ MISTAKE 1: Using mlflow.sklearn.log_model instead of fe.log_model
# This loses feature lineage and may have signature issues
mlflow.sklearn.log_model(
    model,
    artifact_path="model",
    registered_model_name=registered_name
)

# ❌ MISTAKE 2: Using signature parameter instead of output_schema
# Per official docs, signature is NOT recommended for fe.log_model
fe.log_model(
    model=model,
    artifact_path="model",
    flavor=mlflow.sklearn,
    training_set=training_set,
    registered_model_name=registered_name,
    signature=infer_signature(X_train, y_pred)  # NOT recommended!
)

# ❌ MISTAKE 3: Missing output_schema for unsupervised models
# When training_set has label=None, output_schema is REQUIRED
fe.log_model(
    model=model,
    artifact_path="model",
    flavor=mlflow.sklearn,
    training_set=training_set,  # label=None for anomaly detection
    registered_model_name=registered_name,
    infer_input_example=True
    # Missing output_schema! Will fail with signature error
)

# ❌ MISTAKE 4: Missing flavor parameter
fe.log_model(
    model=model,
    artifact_path="model",
    training_set=training_set,
    registered_model_name=registered_name
    # Missing flavor=mlflow.sklearn! Will fail with "missing required argument"
)

# ❌ MISTAKE 5: Using pd.DataFrame() constructor instead of .head()
# This can cause type issues
input_example = pd.DataFrame(X_train[:5])  # May lose dtypes
predictions = model.predict(X_train[:5])   # Different object!

# ✅ CORRECT: Use .head() consistently
input_example = X_train.head(5).astype('float64')
predictions = model.predict(input_example)  # Same object
```

#### Alternative: Using infer_signature (Works but Less Reliable)

```python
# This pattern ALSO works but is less recommended per official docs
# Use when you need compatibility with non-feature-store models

input_example = X_train.head(5).astype('float64')
predictions = model.predict(input_example)
signature = infer_signature(input_example, predictions)

# Must disable autolog to avoid conflicts
mlflow.autolog(disable=True)

with mlflow.start_run(run_name=run_name) as run:
    fe.log_model(
        model=model,
        artifact_path="model",
        flavor=mlflow.sklearn,
        training_set=training_set,
        registered_model_name=registered_name,
        input_example=input_example,
        signature=signature  # Both input AND output must be present
    )
```

#### DataType Reference for output_schema

| Model Type | Python Output | MLflow DataType | Example |
|---|---|---|---|
| Regression | float | `DataType.double` | Price prediction |
| Binary Classification | int (0,1) | `DataType.long` | Churn prediction |
| Multi-class Classification | int (0,1,2...) | `DataType.long` | Category prediction |
| Anomaly Detection | int (-1,1) | `DataType.long` | Isolation Forest |
| Probability | float [0,1] | `DataType.double` | predict_proba |

#### Official Documentation Reference

- [FeatureEngineeringClient.log_model API](https://api-docs.databricks.com/python/feature-engineering/latest/feature_engineering.client.html)
- Key Parameters:
  - `flavor`: REQUIRED - the MLflow model flavor (e.g., `mlflow.sklearn`)
  - `training_set`: TrainingSet object from `create_training_set()`
  - `output_schema`: REQUIRED for models without labels (unsupervised)
  - `infer_input_example`: RECOMMENDED over manual `signature` parameter
  - `registered_model_name`: 3-level UC name: `catalog.schema.model_name`

### 6. NaN/Inf Handling at Feature Table Source (CRITICAL)

**⚠️ ROOT CAUSE OF 3 INFERENCE FAILURES (Jan 2026):** sklearn models fail at inference when feature tables contain NaN.

#### The Problem

| Stage | Training | Inference |
|-------|----------|-----------|
| **Data Source** | Feature Table | Feature Table |
| **Processing** | `prepare_training_data()` → `fillna(0)` | `fe.score_batch()` → **No NaN handling** |
| **Result** | Clean data → Model trains ✅ | NaN in data → Crash ❌ |

**Algorithm-specific behavior:**
- **XGBoost**: Handles NaN natively ✅
- **sklearn GradientBoostingRegressor**: Crashes with `ValueError: Input X contains NaN` ❌
- **sklearn GradientBoostingClassifier**: Same issue ❌

**Error message:**
```
ValueError: Input X contains NaN.
GradientBoostingRegressor does not accept missing values encoded as NaN natively.
For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier
and Regressor which accept missing values encoded as NaNs natively.
```

#### The Solution: Clean at Feature Table Creation

```python
# In create_feature_table() - clean ALL numeric columns at source
def clean_numeric(col_name):
    """Replace NaN, NULL, and Inf with 0.0 for sklearn compatibility."""
    return F.when(
        F.col(col_name).isNull() | F.isnan(F.col(col_name)) |
        (F.col(col_name) == float('inf')) | (F.col(col_name) == float('-inf')),
        F.lit(0.0)
    ).otherwise(F.col(col_name))

# Apply to ALL DoubleType columns (after cast to DOUBLE)
from pyspark.sql.types import DoubleType

for field in df.schema.fields:
    if isinstance(field.dataType, DoubleType):
        df = df.withColumn(field.name, clean_numeric(field.name))
```

#### ❌ WRONG: Clean only at training time

```python
# This is in prepare_training_data() - but ONLY affects training!
X = X.fillna(0).replace([np.inf, -np.inf], 0)

# Inference via fe.score_batch() bypasses this → NaN reaches model → Crash
```

#### ✅ CORRECT: Clean at feature table source

```python
# In feature table creation script
def create_feature_table(spark, fe, df, table_name, primary_keys, ...):
    """Create feature table with NaN/Inf cleaned for sklearn compatibility."""
    
    # Cast all numeric to DOUBLE AND clean NaN/Inf
    numeric_types = (IntegerType, LongType, FloatType, DecimalType, ShortType, ByteType)
    double_cols = []
    
    for field in df.schema.fields:
        if isinstance(field.dataType, numeric_types):
            # Cast and clean in one step
            df = df.withColumn(
                field.name,
                F.coalesce(F.col(field.name).cast("double"), F.lit(0.0))
            )
            double_cols.append(field.name)
    
    # Handle existing DOUBLE columns that may have NaN
    for field in df.schema.fields:
        if isinstance(field.dataType, DoubleType) and field.name not in double_cols:
            df = df.withColumn(field.name, clean_numeric(field.name))
    
    print(f"  Cleaned NaN/Inf→0.0 for {len(double_cols)} columns (sklearn compatibility)")
    
    # Write feature table...
```

**Impact of this fix:**
- Before: 3 models failing (`query_performance_forecaster`, `warehouse_optimizer`, `cluster_capacity_planner`)
- After: All 24 models passing inference ✅

### 7. Label Binarization for XGBoost Classifiers

**⚠️ XGBClassifier requires binary labels (0 or 1), NOT continuous rates (0.0 to 1.0)**

#### The Problem

Many business metrics are rates/ratios (0-1) but XGBClassifier expects discrete classes.

**Error:**
```
XGBoostError: base_score must be in (0,1) for the logistic loss
```

#### Standard Binarization Thresholds

| Use Case | Label Column | Threshold | Binarized Label |
|----------|--------------|-----------|-----------------|
| Failure prediction | `failure_rate` | > 0.2 | `high_failure_rate` |
| Success prediction | `success_rate` | > 0.7 | `high_success_rate` |
| SLA breach | `sla_breach_rate` | > 0.1 | `needs_optimization` |
| Serverless adoption | `serverless_adoption_ratio` | > 0.5 | `high_serverless_adoption` |
| Cache efficiency | `spill_rate` | > 0.3 | `high_spill_rate` |

#### Pattern: Binarize Before Training

```python
from pyspark.sql.functions import col, when

# Original continuous label (0-1 rate)
LABEL_COLUMN = "failure_rate"

# Binarized label name
BINARIZED_LABEL_COLUMN = "high_failure_rate"

# Threshold for binarization
THRESHOLD = 0.2

def main():
    # ... load training data ...
    
    # ✅ Binarize the continuous label
    training_df = training_df.withColumn(
        BINARIZED_LABEL_COLUMN,
        when(col(LABEL_COLUMN) > THRESHOLD, 1).otherwise(0)
    )
    
    # Use BINARIZED label for classifier
    training_set = fe.create_training_set(
        df=training_df,
        feature_lookups=feature_lookups,
        label=BINARIZED_LABEL_COLUMN,  # ✅ Use binarized column
        exclude_columns=[LABEL_COLUMN]  # Exclude original continuous column
    )
    
    # XGBClassifier now receives 0/1 labels
    model = XGBClassifier(...)
    model.fit(X_train, y_train)
```

#### ❌ WRONG: Pass continuous rate to classifier

```python
# This causes "base_score must be in (0,1)" error
training_set = fe.create_training_set(
    df=training_df,
    feature_lookups=feature_lookups,
    label="failure_rate"  # ❌ Continuous 0-1 rate
)
```

### 8. Single-Class Data Detection

**⚠️ Classifiers CANNOT train when all samples have the same label**

#### The Problem

When your label column has only one unique value (e.g., all 0s or all 1s), the classifier cannot learn any decision boundary.

**Error:**
```
ValueError: The least populated class in y has only 0 members, which is too few.
```

#### Pattern: Check Label Distribution Before Training

```python
import json

def check_label_distribution(training_df, label_column):
    """
    Check if label has multiple classes. Skip training if single-class.
    
    Returns: (is_valid, distribution_info)
    """
    label_distribution = training_df.groupBy(label_column).count().collect()
    
    if len(label_distribution) < 2:
        # Single-class data - cannot train classifier
        single_class = label_distribution[0][label_column]
        sample_count = label_distribution[0]["count"]
        
        msg = f"Single-class data: all {sample_count} samples have label={single_class}. Cannot train classifier."
        print(f"⚠️ {msg}")
        
        return False, msg
    
    print(f"  Label distribution: {label_distribution}")
    return True, label_distribution

# In main():
is_valid, distribution = check_label_distribution(training_df, BINARIZED_LABEL_COLUMN)

if not is_valid:
    # Exit gracefully - don't fail the job
    dbutils.notebook.exit(json.dumps({
        "status": "SKIPPED",
        "reason": distribution
    }))
    
# Continue with training...
```

#### Root Cause Analysis

**Why single-class data happens:**
1. **Data source limitation**: System tables may not track historical changes (e.g., `schema_changes_7d` always 0)
2. **Threshold too extreme**: All values fall on one side of the threshold
3. **Data quality issue**: Missing or incomplete source data
4. **Time window too short**: Not enough variance in the data

**Solution options:**
1. **Skip model gracefully** (recommended if data is sparse)
2. **Adjust threshold** to create more balanced classes
3. **Expand time window** to capture more variance
4. **Switch to regression** if rate prediction is needed

### 9. Feature Column Exclusion During Training

**⚠️ Label columns MUST be excluded from feature set**

#### The Problem

If the label column is included as a feature during training, inference will fail because the label is not available at prediction time.

**Error:**
```
ValueError: Input contains NaN.
# or
KeyError: 'label_column_name'
```

#### Pattern: Explicit Exclusion

```python
from src.ml.config.feature_registry import FeatureRegistry

registry = FeatureRegistry()

# ✅ CORRECT: Exclude label from features
feature_names = registry.get_feature_columns(
    FEATURE_TABLE, 
    exclude_columns=[LABEL_COLUMN]
)

# ❌ WRONG: Get all columns including label
feature_names = registry.get_feature_columns(FEATURE_TABLE)  # Label may be included!
```

#### FeatureRegistry Pattern

```python
class FeatureRegistry:
    """Registry of feature tables and their schemas."""
    
    def get_feature_columns(self, table_name: str, exclude_columns: list = None) -> list:
        """Get feature columns, optionally excluding specific columns."""
        
        config = self.tables.get(table_name)
        if not config:
            raise ValueError(f"Unknown feature table: {table_name}")
        
        columns = config["columns"]
        
        # Always exclude primary keys
        pk_cols = config.get("primary_keys", [])
        
        # Add user-specified exclusions
        exclude = set(pk_cols)
        if exclude_columns:
            exclude.update(exclude_columns)
        
        return [c for c in columns if c not in exclude]
```

---

## Databricks Feature Store Patterns

### Feature Table Creation

```python
from databricks.feature_engineering import FeatureEngineeringClient

fe = FeatureEngineeringClient()

# ✅ CORRECT: Primary keys without timestamp_keys
fe.create_table(
    name=f"{catalog}.{feature_schema}.property_features",
    primary_keys=["property_id"],  # Single key
    df=property_features_df,
    description="Property-level features for ML models"
)

# ✅ CORRECT: Composite primary key for time-series
fe.create_table(
    name=f"{catalog}.{feature_schema}.engagement_features",
    primary_keys=["property_id", "engagement_date"],  # Composite key
    df=engagement_features_df,
    description="Daily engagement features"
)

# ❌ WRONG: timestamp_keys was removed in newer versions
fe.create_table(
    name=feature_table_name,
    primary_keys=["property_id"],
    timestamp_keys=["feature_date"],  # ❌ Not supported anymore
    df=features_df
)
```

### Feature Lookup Pattern

```python
from databricks.feature_engineering import FeatureLookup

# Define feature lookups
feature_lookups = [
    FeatureLookup(
        table_name=f"{catalog}.{feature_schema}.property_features",
        feature_names=["bedrooms", "base_price", "bookings_30d"],
        lookup_key="property_id"
    ),
    FeatureLookup(
        table_name=f"{catalog}.{feature_schema}.engagement_features",
        feature_names=["view_count", "click_count", "conversion_rate"],
        lookup_key=["property_id", "engagement_date"]  # Composite key
    )
]

# Create training set with feature lookups
training_set = fe.create_training_set(
    df=labels_df,
    feature_lookups=feature_lookups,
    label="target_column"
)
```

### Feature Store Error: Column Conflict

```python
# ❌ WRONG: DataFrame already has columns that FeatureLookup will add
labels_df = (
    base_df
    .withColumn("day_of_week", dayofweek("date"))  # ❌ This exists in engagement_features!
    .withColumn("is_weekend", ...)
)

# ✅ CORRECT: Let Feature Store provide the columns
labels_df = base_df.select("property_id", "engagement_date", "target")
# day_of_week, is_weekend will come from engagement_features via FeatureLookup
```

**Error:** `DataFrame contains column names that match feature output names`

---

## Schema Verification (MANDATORY Before Coding)

### Gold Layer Column Verification Pattern

```python
# ALWAYS verify columns against YAML before coding
# Location: gold_layer_design/yaml/{domain}/{table}.yaml

# Step 1: Read the YAML file
# Step 2: List all columns with types
# Step 3: Check if table is SCD2 (has is_current?)

# Common column name mistakes from this project:
# ❌ destination        → ✅ destination_id       (in dim_property)
# ❌ nights_stayed      → ✅ avg_nights_booked    (in fact_booking_daily)
# ❌ occupancy_rate     → ✅ avg_booking_value    (occupancy_rate doesn't exist)
# ❌ booking_date       → ✅ created_at           (in fact_booking_detail)
# ❌ user_id            → ✅ (doesn't exist)      (in fact_property_engagement)
```

### SCD2 vs Regular Dimension Tables

```python
# =============================================================================
# SCD2 Tables (have is_current column) - FILTER IS REQUIRED
# =============================================================================
# - dim_property
# - dim_user
# - dim_host

# ✅ CORRECT: Filter for current records
dim_property = (
    spark.table(f"{catalog}.{gold_schema}.dim_property")
    .filter(col("is_current") == True)
)

# =============================================================================
# Regular Dimension Tables (NO is_current) - DO NOT FILTER
# =============================================================================
# - dim_destination
# - dim_date
# - dim_weather_location

# ✅ CORRECT: No filter needed
dim_destination = spark.table(f"{catalog}.{gold_schema}.dim_destination")

# ❌ WRONG: Assume all dimensions have is_current
dim_destination.filter(col("is_current"))  # Column doesn't exist!
```

**Error if wrong:** `UNRESOLVED_COLUMN.WITH_SUGGESTION: A column with name 'is_current' cannot be resolved`

### Ambiguous Column Resolution

```python
# When joining tables with same column names, be explicit

# ❌ WRONG: Ambiguous destination_id after join
training_df = (
    fact_booking_daily
    .join(dim_property, "property_id")
    .join(dim_destination, "destination_id")  # Which destination_id?
)

# ✅ CORRECT: Drop ambiguous column before join
fact_for_join = fact_booking_daily.drop("destination_id")
training_df = (
    fact_for_join
    .join(dim_property.select("property_id", "destination_id"), "property_id")
    .join(dim_destination, "destination_id")
)
```

---

## Model Training Patterns

### Standard Training Pipeline Structure

```python
# Databricks notebook source
"""
{Model Name} Training Pipeline

Trains a {model_type} model for {use_case}.
Uses {algorithm} with MLflow 3.1+ best practices.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup
import mlflow
from mlflow.models import infer_signature
import pandas as pd
import numpy as np
from xgboost import XGBRegressor  # or XGBClassifier
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime

# =============================================================================
# MLflow Configuration (at module level)
# =============================================================================
mlflow.set_registry_uri("databricks-uc")

# =============================================================================
# INLINE HELPER FUNCTIONS (do not import - copy to each script)
# =============================================================================
def setup_mlflow_experiment(model_name: str) -> str:
    """Set up MLflow experiment using /Shared/ path."""
    print("\n" + "="*80)
    print(f"Setting up MLflow Experiment: {model_name}")
    print("="*80)
    
    experiment_name = f"/Shared/wanderbricks_ml_{model_name}"
    
    try:
        experiment = mlflow.set_experiment(experiment_name)
        print(f"✓ Experiment set: {experiment_name}")
        print(f"  Experiment ID: {experiment.experiment_id}")
        return experiment_name
    except Exception as e:
        print(f"❌ Experiment setup failed: {e}")
        return None


def log_training_dataset(spark, catalog: str, schema: str, table_name: str) -> bool:
    """Log training dataset for MLflow lineage. MUST be inside mlflow.start_run()."""
    full_table_name = f"{catalog}.{schema}.{table_name}"
    try:
        print(f"  Logging dataset: {full_table_name}")
        training_df = spark.table(full_table_name)
        dataset = mlflow.data.from_spark(
            df=training_df, 
            table_name=full_table_name,
            version="latest"
        )
        mlflow.log_input(dataset, context="training")
        print(f"✓ Dataset logged: {full_table_name}")
        return True
    except Exception as e:
        print(f"⚠️  Dataset logging failed: {e}")
        return False


def get_run_name(model_name: str, algorithm: str, version: str = "v1") -> str:
    """Generate descriptive run name for MLflow tracking."""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    return f"{model_name}_{algorithm}_{version}_{timestamp}"


def get_standard_tags(model_name: str, domain: str, model_type: str, 
                      algorithm: str, use_case: str, training_table: str,
                      feature_store_enabled: bool = False) -> dict:
    """Get standard MLflow run tags for consistent organization."""
    return {
        "project": "wanderbricks",
        "domain": domain,
        "model_name": model_name,
        "model_type": model_type,  # "regression" or "classification"
        "algorithm": algorithm,     # "xgboost", "gradient_boosting"
        "layer": "ml",
        "team": "data_science",
        "use_case": use_case,
        "feature_store_enabled": str(feature_store_enabled).lower(),
        "training_data": training_table
    }


def get_parameters():
    """Get job parameters from dbutils widgets (NEVER use argparse)."""
    catalog = dbutils.widgets.get("catalog")
    gold_schema = dbutils.widgets.get("gold_schema")
    feature_schema = dbutils.widgets.get("feature_schema")
    model_name = dbutils.widgets.get("model_name")
    
    print(f"Catalog: {catalog}")
    print(f"Gold Schema: {gold_schema}")
    print(f"Feature Schema: {feature_schema}")
    print(f"Model Name: {model_name}")
    
    return catalog, gold_schema, feature_schema, model_name


# =============================================================================
# DATA PREPROCESSING (Critical: Replicate exactly in batch inference)
# =============================================================================
def preprocess_features(df):
    """
    Preprocess features for model training.
    
    CRITICAL: Document all transformations - batch inference MUST replicate exactly.
    """
    pdf = df.toPandas()
    
    # 1. Convert Spark DECIMAL to float (REQUIRED - XGBoost can't handle Decimal)
    decimal_cols = ['revenue', 'price', 'booking_value', 'base_price']
    for col in decimal_cols:
        if col in pdf.columns:
            pdf[col] = pd.to_numeric(pdf[col], errors='coerce')
    
    # 2. Encode categorical columns (must match inference)
    categorical_cols = ['property_type', 'destination', 'user_type', 'country']
    for col in categorical_cols:
        if col in pdf.columns and pdf[col].dtype == 'object':
            pdf[col] = pd.Categorical(pdf[col]).codes
    
    # 3. Handle datetime columns
    if 'check_in_date' in pdf.columns:
        pdf['check_in_date'] = pd.to_datetime(pdf['check_in_date'])
    
    # 4. Fill NaN values
    pdf = pdf.fillna(0)
    
    return pdf


# =============================================================================
# MODEL LOGGING (with all required components for UC)
# =============================================================================
def log_model_with_mlflow(
    model,
    X_train: pd.DataFrame,
    metrics: dict,
    model_name: str,
    catalog: str,
    feature_schema: str,
    gold_schema: str,
    experiment_name: str,
    spark
):
    """Log model with MLflow best practices."""
    
    # 3-level name for Unity Catalog
    registered_model_name = f"{catalog}.{feature_schema}.{model_name}"
    
    # Descriptive run name
    run_name = get_run_name(model_name, "xgboost", "v2")
    
    with mlflow.start_run(run_name=run_name) as run:
        
        # 1. Set tags (before other logging)
        mlflow.set_tags(get_standard_tags(
            model_name=model_name,
            domain="your_domain",
            model_type="regression",
            algorithm="xgboost",
            use_case="demand_forecasting",
            training_table=f"{catalog}.{gold_schema}.fact_booking_daily",
            feature_store_enabled=False
        ))
        
        # 2. Log dataset (INSIDE run context!)
        log_training_dataset(spark, catalog, gold_schema, "fact_booking_daily")
        
        # 3. Log hyperparameters
        mlflow.log_params({
            "max_depth": model.max_depth,
            "learning_rate": model.learning_rate,
            "n_estimators": model.n_estimators,
            "objective": "reg:squarederror"
        })
        
        # 4. Log metrics
        mlflow.log_metrics(metrics)
        
        # 5. Create signature with BOTH input and output (REQUIRED for UC)
        sample_input = X_train.head(5)
        sample_output = model.predict(sample_input)
        signature = infer_signature(sample_input, sample_output)
        
        # 6. Log and register model
        mlflow.xgboost.log_model(
            model,
            artifact_path="model",
            signature=signature,
            input_example=sample_input,
            registered_model_name=registered_model_name
        )
        
        return {
            "run_id": run.info.run_id,
            "model_uri": f"runs:/{run.info.run_id}/model",
            "registered_model_name": registered_model_name,
            "experiment_name": experiment_name
        }


# =============================================================================
# MAIN ENTRY POINT
# =============================================================================
def main():
    """Main training pipeline."""
    
    catalog, gold_schema, feature_schema, model_name = get_parameters()
    
    spark = SparkSession.builder.appName(f"{model_name} Training").getOrCreate()
    
    # Disable autolog to control what's logged
    mlflow.autolog(disable=True)
    
    # Setup experiment
    experiment_name = setup_mlflow_experiment(model_name)
    
    try:
        # Your training logic here...
        
        print("✓ Training completed successfully!")
        
    except Exception as e:
        import traceback
        print(f"❌ Error: {str(e)}")
        print(traceback.format_exc())
        dbutils.notebook.exit(f"FAILED: {str(e)}")
    
    # Signal success (REQUIRED)
    dbutils.notebook.exit("SUCCESS")


if __name__ == "__main__":
    main()
```

---

## Batch Inference Patterns

### Signature-Driven Preprocessing (CRITICAL)

The batch inference script must replicate EXACT preprocessing from training. Type mismatches are the #1 cause of inference failures.

```python
def load_model_and_signature(catalog: str, feature_schema: str, model_name: str, model_version: str):
    """Load model and extract signature to know exact expected schema."""
    
    full_model_name = f"{catalog}.{feature_schema}.{model_name}"
    
    if model_version == "latest":
        # Get latest version
        client = mlflow.tracking.MlflowClient()
        versions = client.search_model_versions(f"name='{full_model_name}'")
        if versions:
            latest_version = max(versions, key=lambda v: int(v.version))
            model_uri = f"models:/{full_model_name}/{latest_version.version}"
        else:
            raise Exception(f"No model versions found for {full_model_name}")
    else:
        model_uri = f"models:/{full_model_name}/{model_version}"
    
    # Load model to get signature
    model = mlflow.pyfunc.load_model(model_uri)
    signature = model.metadata.signature
    
    print(f"Model Signature:")
    print(f"  Input columns: {[inp.name for inp in signature.inputs]}")
    print(f"  Input types: {[(inp.name, str(inp.type)) for inp in signature.inputs]}")
    
    return model, model_uri, signature


def prepare_features_for_signature(pdf: pd.DataFrame, signature):
    """
    Prepare features to EXACTLY match model's expected signature.
    
    CRITICAL: Type mismatches cause inference failures.
    """
    expected_cols = {inp.name: str(inp.type) for inp in signature.inputs}
    
    for col_name, col_type in expected_cols.items():
        if col_name not in pdf.columns:
            print(f"⚠️  Missing column: {col_name}")
            # Add with appropriate default
            if 'double' in col_type or 'float' in col_type:
                pdf[col_name] = 0.0
            elif 'long' in col_type or 'integer' in col_type:
                pdf[col_name] = 0
            elif 'boolean' in col_type:
                pdf[col_name] = False
            continue
        
        # Type conversions
        if 'float32' in col_type:
            pdf[col_name] = pdf[col_name].astype('float32')
        elif 'float64' in col_type or 'double' in col_type:
            pdf[col_name] = pd.to_numeric(pdf[col_name], errors='coerce').fillna(0.0).astype('float64')
        elif 'bool' in col_type:
            if pdf[col_name].dtype in ['int64', 'float64']:
                pdf[col_name] = pdf[col_name].astype(bool)
        elif 'long' in col_type:
            pdf[col_name] = pd.to_numeric(pdf[col_name], errors='coerce').fillna(0).astype('int64')
        elif 'integer' in col_type:
            pdf[col_name] = pd.to_numeric(pdf[col_name], errors='coerce').fillna(0).astype('int32')
    
    # Select ONLY columns in signature, in signature order
    signature_col_names = [inp.name for inp in signature.inputs]
    pdf_final = pdf[signature_col_names]
    
    return pdf_final
```

### Common Type Conversion Issues (from Production)

```python
# =============================================================================
# All type conversion issues discovered in batch inference
# =============================================================================

# Issue 1: Spark DECIMAL → Python float (affects all models)
# Error: unsupported operand type(s) for -: 'decimal.Decimal' and 'float'
pdf['revenue'] = pd.to_numeric(pdf['revenue'], errors='coerce')

# Issue 2: Boolean → float64 (Customer LTV model expects this)
# Error: Cannot safely convert bool to float64
pdf['is_business'] = pdf['is_business'].astype('float64')
pdf['is_repeat'] = pdf['is_repeat'].astype('float64')

# Issue 3: int64 → float64 (Customer LTV model)
# Error: Cannot safely convert int64 to float64
pdf['total_bookings'] = pdf['total_bookings'].astype('float64')

# Issue 4: float64 → float32 (Demand Predictor trained with float32)
# Error: Cannot safely convert float64 to float32
pdf['base_price'] = pdf['base_price'].astype('float32')

# Issue 5: float64 → bool (Conversion Predictor)
# Error: Cannot safely convert float64 to bool
pdf['is_weekend'] = pdf['is_weekend'].astype(bool)

# Issue 6: Categorical encoding must match training
# Error: Incompatible input types for column property_type
pdf['property_type'] = pd.Categorical(pdf['property_type']).codes
```

---

## Asset Bundle Configuration

### Training Job Pattern

```yaml
# resources/ml/ml_training_orchestrator_job.yml
resources:
  jobs:
    ml_training_orchestrator_job:
      name: "[${bundle.target}] ML Training Orchestrator"
      description: "Trains all ML models with MLflow 3.1+ LoggedModel tracking"
      
      # Shared environment for all tasks
      environments:
        - environment_key: default
          spec:
            environment_version: "4"
            dependencies:
              - "mlflow>=3.1"
              - "databricks-feature-engineering>=0.6.0"
              - "xgboost==2.0.3"
              - "scikit-learn==1.3.2"
              - "pandas==2.1.4"
              - "numpy==1.26.2"
      
      # Tasks run in PARALLEL (no depends_on)
      tasks:
        - task_key: train_demand_predictor
          environment_key: default
          notebook_task:
            notebook_path: ../../src/wanderbricks_ml/models/demand_predictor/train.py
            base_parameters:  # NOT parameters with --flags!
              catalog: ${var.catalog}
              gold_schema: ${var.gold_schema}
              feature_schema: ${var.ml_schema}
              model_name: demand_predictor
          timeout_seconds: 3600
        
        - task_key: train_conversion_predictor
          environment_key: default
          notebook_task:
            notebook_path: ../../src/wanderbricks_ml/models/conversion_predictor/train.py
            base_parameters:
              catalog: ${var.catalog}
              gold_schema: ${var.gold_schema}
              feature_schema: ${var.ml_schema}
              model_name: conversion_predictor
          timeout_seconds: 3600
      
      # Schedule: Weekly retraining
      schedule:
        quartz_cron_expression: "0 0 2 ? * SUN"
        timezone_id: "America/Los_Angeles"
        pause_status: PAUSED  # Enable in production
      
      timeout_seconds: 14400  # 4 hours total
      
      # ❌ DO NOT define experiments in Asset Bundle - creates duplicates!
      # experiments:
      #   my_experiment:
      #     name: /Shared/my_experiment  # Don't do this!
      
      tags:
        environment: ${bundle.target}
        project: wanderbricks
        layer: ml
        job_type: training
```

### Batch Inference Job Pattern

```yaml
resources:
  jobs:
    ml_batch_inference_job:
      name: "[${bundle.target}] ML Batch Inference"
      
      environments:
        - environment_key: default
          spec:
            environment_version: "4"
            dependencies:
              - "mlflow>=3.1"
              - "databricks-feature-engineering>=0.6.0"
              - "xgboost==2.0.3"
              - "pandas==2.1.4"
      
      tasks:
        - task_key: score_demand_predictor
          environment_key: default
          notebook_task:
            notebook_path: ../../src/wanderbricks_ml/inference/batch_inference_fixed.py
            base_parameters:
              catalog: ${var.catalog}
              model_name: demand_predictor
              input_table: ${var.catalog}.${var.gold_schema}.fact_booking_daily
              output_table: ${var.catalog}.${var.ml_schema}.demand_predictions
              model_version: latest
              feature_schema: ${var.ml_schema}
```

---

## Unity Catalog Integration

### Model Registration Pattern

```python
# Set registry URI (at module level, before any MLflow calls)
mlflow.set_registry_uri("databricks-uc")

# 3-level naming convention (REQUIRED for UC)
registered_model_name = f"{catalog}.{schema}.{model_name}"

# Log with signature (REQUIRED for UC)
mlflow.xgboost.log_model(
    model,
    artifact_path="model",
    signature=infer_signature(X_train.head(5), model.predict(X_train.head(5))),
    input_example=X_train.head(5),
    registered_model_name=registered_model_name  # Auto-registers to UC
)
```

### Loading Models from UC

```python
# Load specific version
model_uri = f"models:/{catalog}.{schema}.{model_name}/1"
model = mlflow.pyfunc.load_model(model_uri)

# Load by alias (if set)
model_uri = f"models:/{catalog}.{schema}.{model_name}@production"

# Load latest version (requires additional lookup)
client = mlflow.tracking.MlflowClient()
versions = client.search_model_versions(f"name='{catalog}.{schema}.{model_name}'")
latest = max(versions, key=lambda v: int(v.version))
model_uri = f"models:/{catalog}.{schema}.{model_name}/{latest.version}"
```

---

## Validation Checklists

### Pre-Development Checklist
- [ ] Verify ALL column names against Gold layer YAML schemas
- [ ] Check if dimension tables are SCD2 (has `is_current`?)
- [ ] Confirm data types (DECIMAL, STRING, BOOLEAN, etc.)
- [ ] Identify categorical columns needing encoding
- [ ] Review Feature Store tables if using feature lookups
- [ ] Document all preprocessing transformations

### MLflow Setup Checklist
- [ ] Use `/Shared/wanderbricks_ml_{model_name}` experiment path
- [ ] Do NOT define experiments in Asset Bundle
- [ ] Inline ALL helper functions (no module imports)
- [ ] Add `mlflow.autolog(disable=True)` before custom logging
- [ ] Set `mlflow.set_registry_uri("databricks-uc")` at module level

### Training Pipeline Checklist
- [ ] Convert DECIMAL columns: `pd.to_numeric(col, errors='coerce')`
- [ ] Encode categorical columns: `pd.Categorical(col).codes`
- [ ] Log dataset INSIDE `mlflow.start_run()` context
- [ ] Use `fe.log_model()` NOT `mlflow.sklearn.log_model()` when using Feature Store
- [ ] Include `flavor=mlflow.sklearn` in `fe.log_model()` call
- [ ] Include `output_schema` for ALL models (regression: `DataType.double`, classification: `DataType.long`)
- [ ] Use `infer_input_example=True` (preferred over manual `signature` parameter)
- [ ] For anomaly detection (label=None): `output_schema` is REQUIRED
- [ ] Register model with 3-level name: `catalog.schema.model_name`
- [ ] Add `dbutils.notebook.exit("SUCCESS")` at end
- [ ] Do NOT call `spark.stop()` (prevents exit signal)
- [ ] Add `mlflow.autolog(disable=True)` before custom logging
- [ ] **Label Binarization**: If using XGBClassifier with 0-1 rate labels, binarize first
- [ ] **Single-Class Check**: Verify label has multiple classes before training classifier
- [ ] **Exclude Labels**: Use `exclude_columns=[LABEL_COLUMN]` when getting feature columns

### Feature Table Creation Checklist (CRITICAL for Inference)
- [ ] Cast ALL numeric columns to DOUBLE
- [ ] Clean NaN/Inf with `F.coalesce()` at source (sklearn compatibility)
- [ ] Filter NULL primary key rows before writing
- [ ] Add PK constraint after table creation
- [ ] Verify row count after filtering

### Batch Inference Checklist
- [ ] Load model signature from registered model
- [ ] Replicate EXACT preprocessing from training
- [ ] Explicit type conversions for each column
- [ ] Handle missing columns gracefully
- [ ] Verify column order matches signature
- [ ] Test with small batch before full inference
- [ ] **Verify feature table has no NaN**: Check `df.filter(isnan(col)).count() == 0` for numeric columns
- [ ] **Algorithm compatibility**: sklearn models require clean data; XGBoost handles NaN
- [ ] **Error propagation**: Ensure partial failures don't mask as success

### Job Configuration Checklist
- [ ] Use `notebook_task` with `base_parameters` (not argparse)
- [ ] Use `dbutils.widgets.get()` for parameters (not argparse)
- [ ] Include all required dependencies in environment
- [ ] Set appropriate timeout
- [ ] DO NOT define experiments in Asset Bundle

---

## Common Errors and Solutions

### Signature and Model Registration Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| `Model passed for registration contained a signature that includes only inputs` | Missing output schema for Unity Catalog | Add `output_schema=Schema([ColSpec(DataType.xxx)])` to `fe.log_model()` |
| `Model passed for registration did not contain any signature metadata` | Neither signature nor output_schema provided | Add `output_schema` AND `infer_input_example=True` |
| `Model signature must contain both input and output` | `infer_signature()` called without predictions | Use `infer_signature(input_example, model.predict(input_example))` |
| `FeatureEngineeringClient.log_model() missing 1 required keyword-only argument: 'flavor'` | Missing flavor parameter | Add `flavor=mlflow.sklearn` (or appropriate flavor) |

### Experiment and Logging Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| Experiment shows as "train" | `/Users/` path failed silently | Use `/Shared/{project}_ml_{model_name}` |
| Dataset not in LoggedModel view | `log_input()` outside run context | Move inside `mlflow.start_run()` |
| Duplicate experiments with `[dev username]` prefix | Asset Bundle experiment definitions | Remove experiments from Asset Bundle YAML |
| `Parent directory does not exist` | User experiment path missing folder | Use `/Shared/` path instead |

### Module and Import Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| `ModuleNotFoundError: No module named 'src'` | Serverless path resolution | Add `sys.path` setup block to notebook |
| `ModuleNotFoundError` for helpers | Import from local module | Inline all helper functions in each script |
| `NameError: name 'X' is not defined` | Variable scope issue after refactoring | Ensure all variables are defined before use |

### Feature Store Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| `Unable to find feature '{column}'` | Feature name mismatch | Check actual column names in feature table with `DESCRIBE TABLE` |
| `DataFrame contains column names that match feature output names` | Column conflict in FeatureLookup | Remove conflicting columns from input DataFrame before lookup |
| `create_training_set() missing required argument` | API signature changed | Verify correct parameters: `df`, `feature_lookups`, `label` |

### Data Type Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| `unsupported operand type(s) for -: 'decimal.Decimal' and 'float'` | Spark DECIMAL not converted | Use `pd.to_numeric(col, errors='coerce')` |
| `Incompatible input types` | Type mismatch between training and inference | Use signature-driven preprocessing |
| `Cannot safely convert bool to float64` | Boolean needs explicit conversion | `df[col].astype('float64')` |
| `Cannot safely convert int64 to float64` | Integer needs explicit conversion | `df[col].astype('float64')` |

### Inference Errors (Training Succeeds but Inference Fails)

| Error | Root Cause | Solution |
|-------|------------|----------|
| `ValueError: Input X contains NaN` | Feature table has NaN, sklearn doesn't handle it | Clean NaN/Inf at **feature table creation**, not just training |
| `GradientBoostingRegressor does not accept missing values encoded as NaN` | sklearn vs XGBoost NaN handling difference | Fill NaN with 0.0 in `create_feature_table()` using `F.coalesce()` |
| `DataFrame is missing required columns` | Model has stale feature metadata | Delete model from UC and retrain with correct feature table |

### Training Errors (XGBoost Specific)

| Error | Root Cause | Solution |
|-------|------------|----------|
| `XGBoostError: base_score must be in (0,1) for the logistic loss` | XGBClassifier received continuous label (0-1 rate) | Binarize label: `when(col("rate") > threshold, 1).otherwise(0)` |
| `ValueError: The least populated class in y has only 0 members` | Single-class data, classifier can't learn | Check label distribution, skip if single-class |
| `ValueError: y should be a 1d array, got an array of shape...` | Label column shape issue | Ensure label is 1D: `y_train.values.ravel()` |

### Schema and Column Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| `UNRESOLVED_COLUMN: Column 'X' cannot be resolved` | Column name doesn't exist | Verify against Gold YAML schema before coding |
| `Column 'is_current' cannot be resolved` | Assumed SCD2 table but isn't | Check if table actually has `is_current` column |
| `Label column 'X' was not found in DataFrame` | Label column mismatch | Verify label column exists in training data |
| `KeyError` during inference | Label column included as feature during training | Use `exclude_columns=[LABEL_COLUMN]` in `get_feature_columns()` |

### Job Execution Errors

| Error | Root Cause | Solution |
|-------|------------|----------|
| Job SUCCESS but actually failed | No exit signal | Add `dbutils.notebook.exit("SUCCESS")` |
| Job timeout | Long-running training | Increase `timeout_seconds` in job YAML |
| `TypeError: get_standard_tags() missing argument` | Function signature changed | Update all calls to match new signature |

---

## Model-Specific Notes

### Demand Predictor
- Uses XGBoost Regressor
- Target: `booking_count`
- Special: Trigonometric seasonal features (sin/cos month, week)
- Training with `TimeSeriesSplit` for temporal validation

### Conversion Predictor
- Uses XGBoost Classifier
- Target: `is_converted` (binary)
- Handle single-class edge case in `predict_proba` and `confusion_matrix`

### Pricing Optimizer
- Uses Gradient Boosting Regressor
- Target: `optimal_price`
- Handle division by zero in MAPE calculation

### Customer LTV
- Uses XGBoost Regressor
- Target: `predicted_ltv_12m`
- Boolean columns (`is_business`, `is_repeat`) need float64 conversion

### Revenue Forecaster (Prophet)
- Currently excluded due to dependency issues
- Requires `pystan`, `cmdstanpy` dependencies
- `stan_backend` attribute error needs resolution

### Customer Segmentation (Hybrid ML + Rules)
- Uses XGBoost Classifier for churn prediction + business rules for segment assignment
- **Two-Stage Pipeline**:
  1. Stage 1: Churn Prediction (XGBoost) → outputs `churn_score`
  2. Stage 2: Segment Assignment (Rules) → outputs `primary_segment`, `secondary_segment`
- **Key Lessons**:
  - Use dynamic churn threshold (median-based) instead of fixed 90-day cutoff
  - Handle PySpark function shadowing: `from pyspark.sql import functions as F` and `import builtins`
  - Use `builtins.max()`, `builtins.min()`, `builtins.sum()` for Python builtins
  - Use `F.max()`, `F.count()` etc. for PySpark aggregations
  - Add safety checks for single-class data before stratified split
  - Handle LTV predictions table join gracefully (may not have `user_id`)
- **Output Table**: `customer_segments` (saved directly during training)
- **Segments**: high_value, at_risk, repeat_travelers, price_sensitive, new_prospects

---

## Version History

### v3.0 (January 4, 2026) - NaN Handling, Label Binarization, and Inference Patterns

**Trigger:** 5 models failing at inference despite successful training. 3 performance models (`query_performance_forecaster`, `warehouse_optimizer`, `cluster_capacity_planner`) with `ValueError: Input X contains NaN`. 1 quality model (`schema_change_predictor`) with single-class data.

**Root Cause Analysis:**

1. **NaN Handling Asymmetry (3 model failures):**
   - **Training flow**: Feature table → `prepare_training_data()` → `fillna(0)` → Model trains ✅
   - **Inference flow**: Feature table → `fe.score_batch()` → **No NaN handling** → Crash ❌
   - **Algorithm difference**: XGBoost handles NaN natively; sklearn `GradientBoostingRegressor` does NOT
   - **Solution**: Clean NaN/Inf at **feature table creation**, not just training time

2. **Label Binarization (6 model failures):**
   - XGBClassifier received continuous labels (0-1 rates) instead of binary (0/1)
   - Error: `XGBoostError: base_score must be in (0,1) for the logistic loss`
   - **Solution**: `when(col("rate") > threshold, 1).otherwise(0)` before training

3. **Single-Class Data (1 model):**
   - `schema_change_predictor` had all samples with label=0 (system.information_schema doesn't track changes)
   - Classifier can't train on single-class data
   - **Solution**: Check label distribution, skip gracefully if single-class

4. **Label as Feature (3 models):**
   - Models trained with label column included in feature set
   - At inference, label not available → feature count mismatch
   - **Solution**: Use `exclude_columns=[LABEL_COLUMN]` in `get_feature_columns()`

**New Patterns Documented:**

| Pattern | Section | Purpose |
|---------|---------|---------|
| NaN/Inf cleaning at source | Rule 6 | Prevent sklearn inference failures |
| Label binarization | Rule 7 | XGBClassifier requires 0/1 labels |
| Single-class detection | Rule 8 | Skip gracefully when data is inadequate |
| Feature column exclusion | Rule 9 | Prevent label leakage into features |

**Scripts Modified:**
- `src/ml/features/create_feature_tables.py` - Added NaN/Inf cleaning in `create_feature_table()`
- `src/ml/cost/train_commitment_recommender.py` - Added label binarization
- `src/ml/performance/train_cache_hit_predictor.py` - Added label binarization
- `src/ml/reliability/train_*_predictor.py` (4 files) - Added label binarization
- `src/ml/quality/train_schema_change_predictor.py` - Added single-class check, skip logic
- `src/ml/performance/train_warehouse_optimizer.py` - Added `exclude_columns`
- `src/ml/performance/train_cluster_capacity_planner.py` - Added `exclude_columns`
- `src/ml/inference/batch_inference_all_models.py` - Added proper error propagation

**Impact:**
- Before: 19/25 models (76%) inference success
- After: 23/24 models (96%) inference success (1 model skipped due to single-class data)
- Training time reduced: Error debugging from ~2 hours to 0
- Documented systematic debugging methodology

**Key Insight:** Training success ≠ Inference success. Always verify end-to-end pipeline, not just individual steps. sklearn and XGBoost have fundamentally different NaN handling - clean at source for consistency.

### v2.0 (January 2026) - fe.log_model and output_schema Patterns

**Trigger:** Persistent `MlflowException: Model passed for registration contained a signature that includes only inputs` errors across 25 training scripts despite multiple fix attempts.

**Learnings Documented:**

1. **`output_schema` is REQUIRED for all Unity Catalog models:**
   - Initial attempts with `infer_input_example=True` alone failed
   - `signature` parameter is NOT recommended per official docs
   - `output_schema=Schema([ColSpec(DataType.xxx)])` is the reliable solution

2. **Model type to DataType mapping:**
   - Regression → `DataType.double`
   - Classification → `DataType.long`
   - Anomaly Detection (Isolation Forest) → `DataType.long` (-1 or 1)

3. **`flavor` parameter is REQUIRED:**
   - Missing `flavor=mlflow.sklearn` causes immediate failure
   - Must match the model type being logged

4. **Anomaly detection models (label=None) have stricter requirements:**
   - `infer_input_example=True` is NOT sufficient
   - MUST explicitly provide `output_schema`
   - Supervised models (with labels) are more forgiving but `output_schema` is still recommended

**Scripts Fixed:** 25 training scripts across 5 domains (cost, performance, quality, reliability, security)

**Impact:** 
- Reduced signature errors from 10+ per run to 0
- All 25 models now train successfully
- Pattern documented prevents future errors

**Key Reference:** [FeatureEngineeringClient.log_model API Documentation](https://api-docs.databricks.com/python/feature-engineering/latest/feature_engineering.client.html)

### v1.0 (December 2025) - Initial Rule

- Initial patterns from 5 model implementation
- Experiment path, dataset logging, helper functions, exit signals documented
- 20+ errors documented with solutions

---

## References

### Official Documentation
- [FeatureEngineeringClient.log_model API](https://api-docs.databricks.com/python/feature-engineering/latest/feature_engineering.client.html) - **CRITICAL: output_schema requirements**
- [MLflow Experiments - Databricks](https://learn.microsoft.com/en-us/azure/databricks/mlflow/experiments)
- [MLflow 3.1 LoggedModel](https://docs.databricks.com/aws/en/mlflow/logged-model)
- [Unity Catalog Model Registry](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/manage-model-lifecycle/)
- [Databricks Feature Store](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/feature-store/)
- [Feature Engineering Client API](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/feature-store/uc/feature-tables-uc)
- [MLflow Model Signatures](https://mlflow.org/docs/latest/model/signatures.html) - Input/output specification requirements

### Internal Documentation
- [Post-Mortem: ML Pipeline Implementation](../../docs/reference/rule-improvement-mlflow-mlmodels-patterns.md)

### Related Cursor Rules
- [databricks-python-imports.mdc](../common/09-databricks-python-imports.mdc) - sys.path setup for Asset Bundles
- [mlflow-genai-patterns.mdc](./28-mlflow-genai-patterns.mdc) - GenAI/LLM patterns
